{"date": 2014, "text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415?1425, October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics Using Structured Events to Predict Stock Price Movement: An Empirical Investigation Xiao Ding ?? , Yue Zhang ? , Ting Liu ? , Junwen Duan ? ? Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China {xding, tliu, jwduan}@ir.hit.edu.cn ? Singapore University of Technology and Design yue zhang@sutd.edu.sg Abstract It has been shown that news events influence the trends of stock price movements. However, previous work on news-driven stock market prediction rely on shallow features (such as bags-of-words, named entities and noun phrases), which do not capture structured entity-relation information, and hence cannot represent complete and exact events. Recent advances in Open Information Extraction (Open IE) techniques enable the extraction of structured events from web-scale data. We propose to adapt Open IE technology for event-based stock price movement prediction, extracting structured events from large-scale public news without manual efforts. Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market. Largescale experiments show that the accuracy of S&P 500 index prediction is 60%, and that of individual stock prediction can be over 70%. Our event-based system outperforms bags-of-words-based baselines, and previously reported systems trained on S&P 500 stock historical data. 1 Introduction Predicting stock price movements is of clear interest to investors, public companies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock ? This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CEO Steve Jobs passed away. Google?s stock fell after grim earnings came out. Accurate extraction of events from financial news may play an important role in stock market prediction. However, previous work represents news documents mainly using simple features, such as bags-of-words, noun phrases, and named entities (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d?Aspremont, 2012; Schumaker and Chen, 2009). With these unstructured features, it is difficult to capture key events embedded in financial news, and even more difficult to model the impact of events on stock market prediction. For example, representing the event ?Apple has sued Samsung Electronics for copying ?the look and feel? 1415 of its iPad tablet and iPhone smartphone.? using term-level features {?Apple?, ?sued?, ?Samsung?, ?Electronics?, ?copying?, ...} alone, it can be difficult to accurately predict the stock price movements of Apple Inc. and Samsung Inc., respectively, as the unstructured terms cannot indicate the actor and object of the event. In this paper, we propose using structured information to represent events, and develop a prediction model to analyze the relationship between events and the stock market. The problem is important because it provides insights into understanding the underlying mechanisms of the influence of events on the stock market. There are two main challenges to this method. On the one hand, how to obtain structured event information from large-scale news streams is a challenging problem. We propose to apply Open Information Extraction techniques (Open IE; Banko et al. (2007); Etzioni et al. (2011); Fader et al. (2011)), which do not require predefined event types or manually labeled corpora. Subsequently, two ontologies (i.e. VerbNet and WordNet) are used to generalize structured event features in order to reduce their sparseness. On the other hand, the problem of accurately predicting stock price movement using structured events is challenging, since events and the stock market can have complex relations, which can be influenced by hidden factors. In addition to the commonly used linear models, we build a deep neural network model, which takes structured events as input and learn the potential relationships between events and the stock market. Experiments on large-scale financial news datasets from Reuters 1 (106,521 documents) and Bloomberg 2 (447,145 documents) show that events are better features for stock market prediction than bags-of-words. In addition, deep neural networks achieve better performance than linear models. The accuracy of S&P 500 index prediction by our approach outperforms previous systems, and the accuracy of individual stock prediction can be over 70% on the large-scale data. Our system can be regarded as one step towards building an expert system that exploits rich knowledge for stock market prediction. Our results are helpful for automatically mining stock price related news events, and for improving the accuracy of algorithm trading systems. 1 http://www.reuters.com/ 2 http://www.bloomberg.com/ 2 Method 2.1 Event Representation We follow the work of Kim (1993) and design a structured representation scheme that allows us to extract events and generalize them. Kim defines an event as a tuple (O i , P , T ), where O i ? O is a set of objects, P is a relation over the objects and T is a time interval. We propose a representation that further structures the event to have roles in addition to relations. Each event is composed of an action P , an actor O 1 that conducted the action, and an object O 2 on which the action was performed. Formally, an event is represented as E = (O 1 , P, O 2 , T ), where P is the action, O 1 is the actor,O 2 is the object and T is the timestamp (T is mainly used for aligning stock data with news data). For example, the event ?Sep 3, 2013 - Microsoft agrees to buy Nokia?s mobile phone business for $7.2 billion.? is modeled as: (Actor = Microsoft, Action = buy, Object = Nokia?s mobile phone business, Time = Sep 3, 2013). Previous work on stock market prediction represents events as a set of individual terms (Fung et al., 2002; Fung et al., 2003; Hayo and Kutan, 2004; Feldman et al., 2011). For example, ?Microsoft agrees to buy Nokia?s mobile phone business for $7.2 billion.? can be represented by {?Microsoft?, ?agrees?, ?buy?, ?Nokia?s?, ?mobile?, ...} and ?Oracle has filed suit against Google over its ever-more-popular mobile operating system, Android.? can be represented by {?Oracle?, ?has?, ?filed?, ?suit?, ?against?, ?Google?, ...}. However, terms alone might fail to accurately predict the stock price movement ofMicrosoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literature to empirically investigate structured event representations for stock market prediction. 2.2 Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words P v , such that P v starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: ? Syntactic constraint: every multi-word event phrase must begin with a verb, end with a preposition, and be a contiguous sequence of words in the sentence. ? Lexical constraint: an event phrase should appear with at least a minimal number of distinct argument pairs in a large corpus. 2. Argument Extraction. For each event phrase P v identified in the step above, we find the nearest noun phrase O 1 to the left of P v in the sentence, and O 1 should contain the subject of the sentence (if it does not contain the subject of P v , we find the second nearest noun phrase). Analogously, we find the nearest noun phrase O 2 to the right of P v in the sentence, and O 2 should contain the object of the sentence (if it does not contain the object of P v , we find the second nearest noun phrase). An example of the extraction algorithm is as follows. Consider the sentence, Instant view: Private sector adds 114,000 jobs in July: ADP. The predicate verb is identified as ?adds?, and its subject and object ?sector? and ?jobs?, respectively. The structured event is extracted as (Private sector, adds, 114,000 jobs). 2.3 Event Generalization Our goal is to train a model that is able to make predictions based on various expressions of the same event. For example, ?Microsoft swallows Nokia?s phone business for $7.2 billion? and ?Microsoft purchases Nokia?s phone business? report the same event. To improve the accuracy of our prediction model, we should endow the event extraction algorithm with generalization capacity. To this end, we leverage knowledge from two well-known ontologies, WordNet (Miller, 1995) and VerbNet (Kipper et al., 2006). The process of event generalization consists of two steps. First, we construct a morphological analysis tool based on the WordNet stemmer to extract lemma forms of inflected words. For example, in ?Instant view: Private sector adds 114,000 jobs in July.?, the words ?adds? and ?jobs? are transformed to ?add? and ?job?, respectively. Second, we generalize each verb to its class name in VerbNet. For example, ?add? belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event generalization have been investigated in Open IE based event causal prediction (Radinsky and Horvitz, 2013). 2.4 Prediction Models 1. Linear model. Most previous work uses linear models to predict the stock market (Fung et al., 2002; Luss and d?Aspremont, 2012; Schumaker and Chen, 2009; Kogan et al., 2009; Das and Chen, 2007; Xie et al., 2013). To make direct comparisons, this paper constructs a linear prediction model by using Support Vector Machines (SVMs), a state-of-the-art classification model. Given a training set (d 1 , y 1 ), (d 2 , y 2 ), ..., (d N , y N ), where n ? [1, N ], d n is a news document and y i ? {+1, ?1} is the output class. d n can be news titles, news contents or both. The output Class +1 represents that the stock price will increase the next day/week/month, and the output Class -1 represents that the stock price will decrease the next day/week/month. The features can be bag-of-words features or structured event features. By SVMs, y = argmax{Class + 1, Class ? 1} is determined by the linear function w ??(d n , y n ), where w is the feature weight vector, and ?(d n , y n ) is a function that maps d n into a M-dimensional feature space. Feature templates will be discussed in the next subsection. 2. Nonlinear model. Intuitively, the relationship between events and the stock market may be more complex than linear, due to hidden and indirect 1417 ? News documents ?1 Class +1 The polarity of the stock price movement is positive Class -1 The polarity of the stock price movement is negative Input Layer Output Layer Hidden Layers ? ? ?2 ?3 ?M Figure 2: Structure of the deep neural network model relationships. We exploit a deep neural network model, the hidden layers of which is useful for learning such hidden relationships. The structure of the model with two hidden layers is illustrated in Figure 2. In all layers, the sigmoid activation function ? is used. Let the values of the neurons of the output layer be y cls (cls ? {+1,?1}), its input be net cls , and y 2 be the value vector of the neurons of the last hidden layer; then: y cls = f(net cls ) = ?(wcls ? y2) (1) where wcls is the weight vector between the neuron cls of the output layer and the neurons of the last hidden layer. In addition, y 2k = ?(w 2k ? y1) (k ? [1, |y2|]) y 1j = ?(w 1j ??(dn)) (j ? [1, |y1|]) (2) Here y 1 is the value vector of the neurons of the first hidden layer, w 2k = (w 2k1 , w 2k2 , ..., w 2k|y 1 | ), k ? [1, |y 2 |] and w 1j = (w1j1, w1j2, ..., w1jM ), j ? [1, |y1|]. w 2kj is the weight between the kth neuron of the last hidden layer and the jth neuron of the first hidden layer; w 1jm is the weight between the jth neuron of the first hidden layer and the mth neuron of the input layer m ? [1, M ]; d n is a news document and ?(d n ) maps d n into a M-dimensional features space. News documents and features used in the nonlinear model are the same as those in the linear model, which will be introduced in details in the next subsection. The standard back-propagation algorithm (Rumelhart et al., 1985) is used for supervised training of the neural network. train dev test number of instances 1425 178 179 number of events 54776 6457 6593 time interval 02/10/2006 18/16/2012 19/06/2012 21/02/2013 22/02/2013 21/11/2013 Table 1: Dataset splitting 2.5 Feature Representation In this paper, we use the same features (i.e. document representations) in the linear and nonlinear prediction models, including bags-of-words and structured events. (1) Bag-of-words features. We use the classic ?TFIDF? score for bag-of-words features. Let L be the vocabulary size derived from the training data (introduced in the next section), and freq(t l ) denote the number of occurrences of the lth word in the vocabulary in document d. TF l = 1 |d| freq(t l ), ?l ? [1 , L], where |d| is the number of words in the document d (stop words are removed). TFIDF l = 1 |d| freq(t l ) ? log( N |{d :freq(t l )>0}| ), where N is the number of documents in the training set. The feature vector ? can be represented as? = (? 1 , ? 2 , ..., ? M ) = (TFIDF 1 , TFIDF 2 , ..., TFIDF M ). The TFIDF feature representation has been used by most previous studies on stock market prediction (Kogan et al., 2009; Luss and d?Aspremont, 2012). (2) Event features. We represent an event tuple (O 1 , P, O 2 , T ) by the combination of elements (except for T) (O 1 , P , O 2 , O 1 + P , P + O 2 , O 1 + P + O 2 ). For example, the event tuple (Microsoft, buy, Nokia?s mobile phone business) can be represented as (#arg1=Microsoft, #action=get class, #arg2=Nokia?s mobile phone business, #arg1 action=Microsoft get class, #action arg2=get class Nokia?s mobile phone business, #arg1 action arg2=Microsoft get class Nokia?s mobile phone business). Structured events are more sparse than words, and we reduce sparseness by two means. First, verb classes (Section 2.3) are used instead of verbs for P. For example, ?get class? is used instead of the verb ?buy?. Second, we use back-off features, such as O 1 + P (?Microsoft get class?) and P + O 2 (?get class Nokia?s mobile phone business?), to address the sparseness ofO 1 andO 2 . Note that the order of O 1 and O 2 is important for our task since they indicate the actor and object, respectively. 1418 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.6 1day 1week 1month Ac cu rac y Time span bow+svmbow+deep neural network event+svm event+deep neural network (a) Accuarcy 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 1day 1week 1month MC C Time span bow+svmbow+deep neural network event+svm event+deep neural network (b) MCC Figure 3: Overall development experiment results 3 Experiments Our experiments are carried out on three different time intervals: short term (1 day), medium term (1 week) and long term (1 month). We test the influence of events on predicting the polarity of stock change for each time interval, comparing the event-based news representation with bag-ofwords-based news representations, and the deep neural network model with the SVM model. 3.1 Data Description We use publicly available financial news from Reuters and Bloomberg over the period from October 2006 to November 2013. This time span witnesses a severe economic downturn in 20072010, followed by a modest recovery in 20112013. There are 106,521 documents in total from Reuters News and 447,145 from Bloomberg News. News titles and contents are extracted from HTML. The timestamps of the news are also extracted, for alignment with stock price information. The data size is larger than most previous work in the literature. We mainly focus on predicting the change of the Standard & Poor?s 500 stock (S&P 500) index 3 , obtaining indices and stock price data from Yahoo Finance. To justify the effectiveness of our prediction model, we also predict price movements of fifteen individual shares from different sectors in S&P 500. We automatically align 1,782 instances of daily trading data with news titles and contents from the previous day/the day a week before the stock price data/the day a month before the stock price data, 4/5 of which are used as the training 3 Standard & Poor?s 500 is a stock market index based on the market capitalizations of 500 large companies having common stock listed on the NYSE or NASDAQ. data, 1/10 for development testing and 1/10 for testing. As shown in Table 1, the training, development and test set are split temporally, with the data from 02/10/2006 to 18/16/2012 for training, the data from 19/06/2012 to 21/02/2013 for development testing, and the data from 22/02/2013 to 21/11/2013 for testing. There are about 54,776 events in the training set, 6,457 events in the development set and 6,593 events in the test set. 3.2 Evaluation Metrics We use two assessment metrics. First, a standard and intuitive approach to measuring the performance of classifiers is accuracy. However, this measure is very sensitive to data skew: when a class has an overwhelmingly high frequency, the accuracy can be high using a classifier that makes prediction on the majority class. Previous work (Xie et al., 2013) uses an additional evaluation metric, which relies on the Matthews Correlation Cofficient (MCC) to avoid bias due to data skew (our data are rather large and not severely skewed, but we also use MCC for comparison with previous work). MCC is a single summary value that incorporates all 4 cells of a 2*2 confusion matrix (True Positive, False Positive, True Negative and False Negative, respectively). GivenTP ,TN , FP and FN : MCC = TP ?TN?FP ?FN ? (TP+FP)(TP+FN )(TN +FP)(TN +FN ) (3) 3.3 Overall Development Results We evaluate our four prediction methods (i.e. SVM with bag-of-word features (bow), deep neural network with bag-of-word features (bow), 1419 1 day 1 week 1 month 1 layer Accuracy 58.94% 57.73% 55.76% MCC 0.1249 0.0916 0.0731 2 layers Accuracy 59.60% 57.73% 56.19% MCC 0.1683 0.1215 0.0875 Table 2: Different numbers of hidden layers title content content + title bloomberg title + title Acc 59.60% 54.65% 56.83% 59.64% MCC 0.1683 0.0627 0.0852 0.1758 Table 3: Different amounts of data SVM with event features and deep neural network with event features) on three time intervals (i.e. 1 day, 1 week and 1 month, respectively) on the development dataset, and show the results in Figure 3. We find that: (1) Structured event is a better choice for representing news documents. Given the same prediction model (SVM or deep neural network), the event-based method achieves consistently better performance than the bag-of-words-based method over all three time intervals. This is likely due to the following two reasons. First, being an extraction of predicate-argument structures, events carry the most essential information of the document. In contrast, bag-of-words can contain more irrelevant information. Second, structured events can directly give the actor and object of the action, which is important for predicting stock market. (2) The deep neural network model achieves better performance than the SVM model, partly by learning hidden relationships between structured events and stock prices. We give analysis to these relationships in the next section. (3) Event information is a good indicator for short-term volatility of stock prices. As shown in Figure 3, the performance of daily prediction is better than weekly and monthly prediction. Our experimental results confirm the conclusion of Tetlock, Saar-Tsechansky, and Macskassy (2008) that there is a one-day delay between the price response and the information embedded in the news. In addition, we find that some events may cause immediate changes of stock prices. For example, former Microsoft CEO Steve Ballmer announced he would step down within 12 months on 23/08/2013. Within an hour, Microsoft shares jumped as much as 9 percent. This fact indicates that it may be possible to predict stock price movement on a shorter time interval than one day. HowGoogle Inc. Company News Sector News All News Acc MCC Acc MCC Acc MCC 67.86% 0.4642 61.17% 0.2301 55.70% 0.1135 Boeing Company Company News Sector News All News Acc MCC Acc MCC Acc MCC 68.75% 0.4339 57.14% 0.1585 56.04% 0.1605 Wal-Mart Stores Company News Sector News All News Acc MCC Acc MCC Acc MCC 70.45% 0.4679 62.03% 0.2703 56.04% 0.1605 Table 4: Individual stock prediction results ever, we cannot access fine-grained stock price historical data, and this investigation will be left as future work. 3.4 Experiments with Different Numbers of Hidden Layers of the Deep Neural Network Model Cybenko (1989) states that when every processing element utilizes the sigmoid activation function, one hidden layer is enough to solve any discriminant classification problem, and two hidden layers are capable to parse arbitrary output functions of input pattern. Here we conduct a development experiment by different number of hidden layers for the deep neural network model. As shown in Table 2, the performance of two hidden layers is better than one hidden layer, which is consistent with the experimental results of Sharda and Delen (2006) on the task of movie box-office prediction. It indicates that more hidden layers can explain more complex relations (Bengio, 2009). Intuitively, three or more hidden layers may achieve better performance. However, three hidden layers mean that we construct a five-layer deep neural network, which is difficult to train (Bengio et al., 1994). We did not obtain improved accuracies using three hidden layers, due to diminishing gradients. A deep investigation of this problem is out of the scope of this paper. 3.5 Experiments with Different Amounts of Data We conduct a development experiment by extracting news titles and contents from Reuters and Bloomberg, respectively. While titles can give the central information about the news, contents may provide some background knowledge or details. Radinsky et al. (2012) argued that news titles are more helpful for prediction compared to news con1420 0.5 0.55 0.6 0.65 0.7 0.75 0  100  200  300  400  500 Ac cu rac y Company Ranking Wal-Mart GoogleBoeing Nike Qualcomm Apache Starbucks Avon Visa Symantec Hershey Mattel Actavis GannettSanDisk individual stock (a) Accuarcy 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0  100  200  300  400  500 MC C Company Ranking Wal-MartGoogle Boeing Nike Qualcomm Apache Starbucks Avon Visa Symantec Hershey Mattel Actavis Gannett SanDisk individual stock (b) MCC Figure 4: Individual stock prediction experiment results tents, and this paper mainly uses titles. Here we design a comparative experiment to analyze the effectiveness of news titles and contents. First, we use Reuters news to compare the effectiveness of news titles and contents, and then add Bloomberg news titles to investigate whether the amounts of data matters. Table 3 shows that using only news titles achieves the best performance. A likely reason is that we may extract some irrelevant events from news contents. With the additional Bloomberg data, the results are not dramatically improved. This is intuitively because most events are reported by both Reuters news and Bloomberg news. We randomly select about 9,000 pieces of news documents from Reuters and Bloomberg and check the daily overlap manually, finding that about 60% of the news are reported by both Reuters and Bloomberg. The overlap of important news (news related to S&P 500 companies) is 80% and the overlap of unimportant news is 40%. 3.6 Individual Stock Prediction In addition to predicting the S&P 500 index, we also investigate the effectiveness of our approach on the problem of individual stock prediction using the development dataset. We select three wellknown companies, Google Inc., Boeing Company and Wal-Mart Stores from three different sectors (i.e. Information Technology, Industrials and Consumer Staples, respectively) classified by the Global Industry Classification Standard (GICS). We use company news, sector news and all news to predict individual stock price movement, respectively. The experimental results are listed in Table 4. The result of individual stock prediction by using only company news dramatically outperforms the result of S&P 500 index prediction. The main reason is that company-related events can directly affect the volatility of company shares. There is a strong correlation between company events and company shares. Table 4 also shows that the result of individual stock prediction by using sector news or all news does not achieve a good performance, probably because there are many irrelevant events in all news, which would reduce the performance of our prediction model. The fact that the accuracy of these well-known stocks are higher than the index may be because there is relatively more news events dedicated to the relevant companies. To gain a better understanding of the behavior of the model on more individual stocks, we randomly select 15 companies (i.e. Google Inc., Boeing Company, Wal-Mart Stores, Nike Inc., QUALCOMM Inc., Apache Corporation, Starbucks Corp., Avon Products, Visa Inc., Symantec Corp., The Hershey Company, Mattel Inc., Actavis plc, Gannett Co. and SanDisk Corporation) from S&P 500 companies. More specifically, according to the Fortune ranking of S&P 500 companies 4 , we divide the ranked list into five parts, and randomly select three companies from each part. The experimental results are shown in Figure 4. We find that: (1) All 15 individual stocks can be predicted with accuracies above 50%, while 60% of the stocks can be predicted with accuracies above 60%. It shows that the amount of company-related events has strong relationship with the volatility of 4 http://money.cnn.com/magazines/fortune/fortune500/. The amount of company-related news is correlated to the fortune ranking of companies. However, we find that the trade volume does not have such a correlation with the ranking. 1421 S&P 500 Index Prediction Individual Stock Prediction Google Inc. Boeing Company Wal-Mart Stores Accuracy MCC Accuracy MCC Accuracy MCC Accuracy MCC dev 59.60% 0.1683 67.86% 0.4642 68.75% 0.4339 70.45% 0.4679 test 58.94% 0.1649 66.97% 0.4435 68.03% 0.4018 69.87% 0.4456 Table 5: Final experimental results on the test dataset company shares. (2) With decreasing company fortune rankings, the accuracy and MCC decrease. This is mainly because there is not as much daily news about lowranking companies, and hence one cannot extract enough structured events to predict the volatility of these individual stocks. 3.7 Final Results The final experimental results on the test dataset are shown in Table 5 (as space is limited, we show the results on the time interval of one day only). The experimental results on the development and test datasets are consistent, which indicate that our approach has good robustness. The following conclusions obtained from development experiments also hold on the test dataset: (1) Structured events are more useful representations compared to bags-of-words for the task of stock market prediction. (2) A deep neural network model can be more accurate on predicting the stock market compared to the linear model. (3) Our approach can achieve stable experiment results on S&P 500 index prediction and individual stock prediction over a large amount of data (eight years of stock prices and more than 550,000 pieces of news). (4) The quality of information is more important than the quantity of information on the task of stock market prediction. That is to say that the most relevant information (i.e. news title vs news content, individual company news vs all news) is better than more, but less relevant information. 3.8 Analysis and Discussion We use Figure 5 to demonstrate our analysis to the development experimental result of Google Inc. stock prediction, which directly shows the relationship between structured events and the stock market. The links between each layer show the magnitudes of feature weights in the model learned using the training set. Three events, (Google, says bought stake in, China?s XunLei), (Google, reveals stake in, Chi?1 ?2 ?3 ?4 ?5 ?6 ?7 ?8 ?M ?\u00031: (Google, says bought stake in, China?s XunLei) ?\u00034: (Google, reveals stake in, Chinese social website) ?\u00036: (Capgemini, partners, Google apps software)  ?\u00032: (Oracle, sues, Google) ?\u00035: (Google map, break, privacy law) ?\u00038: (Google, may pull out of, China) ? ? Figure 5: Prediction of Google Inc. (we only show structured event features since backoff features are less informative) nese social website) and (Capgemini, partners, Google apps software), have the highest link weights to the first hidden node (from the left). These three events indicate that Google constantly makes new partners and expands its business area. The first hidden node has high-weight links to Class +1, showing that Google?s positive cooperation can lead to the rise of its stock price. Three other events, (Oracle, sues, Google), (Google map, break, privacy law) and (Google, may pull out of, China), have high-weight links to the second hidden node. These three events show that Google was suffering questions and challenges, which could affect its reputation and further pull down its earnings. Correspondingly, the second hidden node has high-weight links to Class -1. These suggest that our method can automatically and directly reveal complex relationships between structured events and the stock market, which is very useful for investors, and can facilitate the research of stock market prediction. Note that the event features used in our prediction model are generalized based on the algorithm introduced in Section 2.5. Therefore, though a specific event in the development test set might have never happened, its generalized form can be found in the training set. For example, ?Google acquired social marketing company Wildfire In1422 teractive? is not in the training data, but ?Google get class? (?get? is the class name of ?acquire? and ?buy? in VerbNet) can indeed be found in the training set, such as ?Google bought stake in XunLei? on 04/01/2007. Hence although the full specific event feature does not fire, its back-offs fire for a correct prediction. For simplicity of showing the event, we did not include back-off features in Figure 5. 4 Related Work Stock market prediction has attracted a great deal of attention across the fields of finance, computer science and other research communities in the past. The literature of stock market prediction was initiated by economists (Keynes, 1937). Subsequently, the influential theory of Efficient Market Hypothesis (EMH) (Fama, 1965) was established, which states that the price of a security reflects all of the information available and that everyone has a certain degree of access to the information. EMH had a significant impact on security investment, and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis technology has been widely used across numerous disciplines (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d?Aspremont, 2012). These studies primarily use bags-of-words to represent financial news documents. However, as Schumaker and Chen (2009) and Xie et al. (2013) point out, bagof-words features are not the best choice for predicting stock prices. Schumaker and Chen (2009) extract noun phrases and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distribution of features. These can be regarded as extensions to the bag-of-word method. The drawback of these approaches, as discussed in the introduction, is that they do not directly model events, which have structured information. There has been efforts to model events more directly (Fung et al., 2002; Hayo and Kutan, 2005; Feldman et al., 2011). Fung, Yu, and Lam (2002) use a normalized word vector-space to model event. Feldman et al. (2011) extract 9 predefined categories of events based on heuristic rules. There are two main problems with these efforts. First, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in all Wall Street Joural (WSJ) and Dow Jones News Services (DJNS) stories about individual S&P500 firms from 1980 to 2004. Bollen and Zeng (2011) study whether the large-scale collective emotion on Twitter is correlated with the volatility of Dow Jones Industrial Average (DJIA). From the experimental results, they find that changes of the public mood match shifts in the DJIA values that occur 3 to 4 days later. Sentiment-analysis-based stock market prediction focuses on investigating the influence of subjective emotion. However, this paper puts emphasis on the relationship between objective events and the stock price movement, and is orthogonal to the study of subjectivity. As a result, our model can be combined with the sentimentanalysis-based method. 5 Conclusion In this paper, we have presented a framework for event-based stock price movement prediction. We extracted structured events from large-scale news based on Open IE technology and employed both linear and nonlinear models to empirically investigate the complex relationships between events and the stock market. Experimental results showed that events-based document representations are 1423 better than bags-of-words-based methods, and deep neural networks can model the hidden and indirected relationship between events and the stock market. For further comparisons, we freely release our data at http://ir.hit.edu.cn/?xding/data. Acknowledgments We thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the National Basic Research Program (973 Program) of China via Grant 2014CB340503, the National Natural Science Foundation of China (NSFC) via Grant 61133012 and 61202277, the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design. We are very grateful to Ji Ma for providing an implementation of the neural network algorithm. References Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction for the web. In IJCAI, volume 7, pages 2670?2676. Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157?166. Yoshua Bengio. 2009. Learning deep architectures for ai. Foundations and trends R ? in Machine Learning, 2(1):1?127. Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1?8. Werner FM Bondt and Richard Thaler. 1985. Does the stock market overreact? The Journal of finance, 40(3):793?805. Wesley S Chan. 2003. Stock price reaction to news and no-news: Drift and reversal after headlines. Journal of Financial Economics, 70(2):223?260. David M Cutler, James M Poterba, and Lawrence H Summers. 1998. What moves stock prices? Bernstein, Peter L. and Frank L. Fabozzi, pages 56?63. George Cybenko. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303?314. Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for amazon: Sentiment extraction from small talk on the web. Management Science, 53(9):1375?1388. Xiao Ding, Bing Qin, and Ting Liu. 2013. Building chinese event type paradigm based on trigger clustering. In Proc. of IJCNLP, pages 311?319, October. Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open information extraction: The second generation. In Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume One, pages 3?10. AAAI Press. Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535?1545. Association for Computational Linguistics. Eugene F Fama. 1965. The behavior of stock-market prices. The journal of Business, 38(1):34?105. Ronen Feldman, Benjamin Rosenfeld, Roy Bar-Haim, and Moshe Fresko. 2011. The stock sonarsentiment analysis of stocks based on a hybrid approach. In Twenty-Third IAAI Conference. Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai Lam. 2002. News sensitive stock trend prediction. In Advances in Knowledge Discovery and Data Mining, pages 481?493. Springer. Bernd Hayo and Ali M Kutan. 2005. The impact of news, oil prices, and global market developments on russian financial markets1. Economics of Transition, 13(2):373?393. Narasimhan Jegadeesh and Sheridan Titman. 1993. Returns to buying winners and selling losers: Implications for stock market efficiency. The Journal of Finance, 48(1):65?91. Narasimhan Jegadeesh. 1990. Evidence of predictable behavior of security returns. The Journal of Finance, 45(3):881?898. Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In ACL, pages 254?262. John Maynard Keynes. 1937. The general theory of employment. The Quarterly Journal of Economics, 51(2):209?223. Jaegwon Kim. 1993. Supervenience and mind: Selected philosophical essays. Cambridge University Press. Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending verbnet with novel verb classes. In Proceedings of LREC, volume 2006, page 1. Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Jacob S. Sagi, and Noah A. Smith. 2009. Predicting risk from financial reports with regression. In Proc. NAACL, pages 272?280, Boulder, Colorado, June. Association for Computational Linguistics. 1424 Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul Ogilvie, David Jensen, and James Allan. 2000. Mining of concurrent text and time series. In KDD2000 Workshop on Text Mining, pages 37?44. Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proc. of ACL (Volume 1: Long Papers), pages 73?82, August. Andrew W Lo and Archie Craig MacKinlay. 1990. When are contrarian profits due to stock market overreaction? Review of Financial studies, 3(2):175?205. Ronny Luss and Alexandre d?Aspremont. 2012. Predicting abnormal returns from news using text classification. Quantitative Finance, pp.1?14, doi:10.1080/14697688.2012.672762. Burton G. Malkiel. 1973. A Random Walk Down Wall Street. W. W. Norton, New York. George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41. Kira Radinsky and Eric Horvitz. 2013. Mining the web to predict future events. In Proceedings of the sixth ACM international conference on Web search and data mining, pages 255?264. ACM. Kira Radinsky, Sagie Davidovich, and Shaul Markovitch. 2012. Learning causality for news events prediction. In Proc. of WWW, pages 909?918. ACM. David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1985. Learning internal representations by error propagation. Technical report, DTIC Document. Robert P Schumaker and Hsinchun Chen. 2009. Textual analysis of stock market prediction using breaking financial news: The azfin text system. ACM Transactions on Information Systems (TOIS), 27(2):12. Ramesh Sharda and Dursun Delen. 2006. Predicting box-office success of motion pictures with neural networks. Expert Systems with Applications, 30(2):243?254. Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng. 2013. Exploiting topic based twitter sentiment for stock prediction. In Proc. of ACL (Volume 2: Short Papers), pages 24? 29, Sofia, Bulgaria, August. Association for Computational Linguistics. Paul C Tetlock, Maytal Saar-Tsechansky, and Sofus Macskassy. 2008. More than words: Quantifying language to measure firms? fundamentals. The Journal of Finance, 63(3):1437?1467. Paul C Tetlock. 2007. Giving content to investor sentiment: The role of media in the stock market. The Journal of Finance, 62(3):1139?1168. William Yang Wang and Zhenhao Hua. 2014. A semiparametric gaussian copula regression model for predicting financial risks from earnings calls. In Proc. of ACL, June. Boyi Xie, Rebecca J. Passonneau, Leon Wu, and Germ?an G. Creamer. 2013. Semantic frames to predict stock price movement. In Proc. of ACL (Volume 1: Long Papers), pages 873?883, August. Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. Textrunner: open information extraction on the web. In Proc. of NAACL: Demonstrations, pages 25?26. Association for Computational Linguistics. Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105?151. 1425 "}
{"date": 2001, "text": "Towards Automatic Classification of Discourse Elements in Essays  Jill Burstein ETS Technologies MS 18E Princeton, NJ 08541 USA Jburstein@ etstechnologies.com Daniel Marcu ISI/USC 4676 Admiralty Way Marina del Rey, CA, USA Marcu@isi.edu Slava Andreyev ETS Technologies MS 18E Princeton, NJ 08541 USA sandreyev@ etstechnologies.com Martin Chodorow Hunter College, The City University of New York New York, NY USA Martin.chodorow@ hunter.cuny.edu   Abstract Educators are interested in essay evaluation systems that include feedback about writing features that can facilitate the essay revision process. For instance, if the thesis statement of a student?s essay could be automatically identified, the student could then use this information to reflect on the thesis statement with regard to its quality, and its relationship to other discourse elements in the essay. Using a relatively small corpus of manually annotated data, we use Bayesian classification to identify thesis statements.  This method yields results that are much closer to human performance than the results produced by two baseline systems.  1 Introduction  Automated essay scoring technology can achieve agreement with a single human judge that is comparable to agreement between two single human judges (Burstein, et al1998; Foltz, et al1998; Larkey, 1998; and Page and Peterson, 1995). Unfortunately, providing students with just a score (grade) is insufficient for instruction. To help students improve their writing skills, writing evaluation systems need to provide feedback that is specific to each individual?s writing and that is applicable to essay revision. The factors that contribute to improvement of student writing include refined sentence structure, variety of appropriate word usage, and organizational structure. The improvement of organizational structure is believed to be critical in the essay revision process toward overall improvement of essay quality.  Therefore, it would be desirable to have a system that could indicate as feedback to students, the discourse elements in their essays. Such a system could present to students a guided list of questions to consider about the quality of the discourse. For instance, it has been suggested by writing experts that if the thesis statement1 of a student?s essay could be automatically provided, the student could then use this information to reflect on the thesis statement and its quality. In addition, such an instructional application could utilize the thesis statement to discuss other types of discourse elements in the essay, such as the relationship between the thesis statement and the conclusion, and the connection between the thesis statement and the main points in the essay.  In the teaching of writing, in order to facilitate the revision process, students are often presented with ?Revision Checklists.? A revision checklist is a list of questions posed to the student to help the student reflect on the quality of his or her writing. Such a list might pose questions such as: a) Is the intention of my thesis statement clear?  1 A thesis statement is generally defined as the sentence that explicitly identifies the purpose of the paper or previews its main ideas. See the Literacy Education On-line (LEO) site at http://leo.stcloudstate.edu. (Annotator 1) ?In my opinion student should do what they want to do because they feel everything and they can't have anythig they feel because they probably feel to do just because other people do it not they want it. (Annotator 2) I think doing what students want is good for them. I sure they want to achieve in the highest place but most of the student give up. They they don?t get what they want. To get what they want, they have to be so strong and take the lesson from their parents Even take a risk, go to the library, and study hard by doing different thing. Some student they do not get what they want because of their family. Their family might be careless about their children so this kind of student who does not get support, loving from their family might not get what he wants. He just going to do what he feels right away. So student need a support from their family they has to learn from them and from their background. I learn from my background I will be the first generation who is going to gradguate from university that is what I want.?  Figure 1: Sample student essay with human annotations of thesis statements.  b) Does my thesis statement respond directly to the essay question? c) Are the main points in my essay clearly stated? d) Do the main points in my essay relate to my original thesis statement? If these questions are expressed in general terms, they are of little help; to be useful, they need to be grounded and need to refer explicitly to the essays students write (Scardamalia and Bereiter, 1985; White 1994). The ability to automatically identify and present to students the discourse elements in their essays can help them focus and reflect on the critical discourse structure of the essays. In addition, the ability for the application to indicate to the student that a discourse element could not be located, perhaps due to the ?lack of clarity? of this element, could also be helpful. Assuming that such a capability was reliable, this would force the writer to think about the clarity of an intended discourse element, such as a thesis statement. Using a relatively small corpus of essay data where thesis statements have been manually annotated, we built a Bayesian classifier using the following features: sentence position; words commonly used in thesis statements; and discourse features, based on Rhetorical Structure Theory (RST) parses (Mann and Thompson, 1988 and Marcu, 2000).  Our results indicate that this classification technique may be used toward automatic identification of thesis statements in essays.  Furthermore, we show that this method generalizes across essay topics.  2 What Are Thesis Statements?  A thesis statement is defined as the sentence that explicitly identifies the purpose of the paper or previews its main ideas (see footnote 1). This definition seems straightforward enough, and would lead one to believe that even for people to identify the thesis statement in an essay would be clear-cut.  However, the essay in Figure 1 is a common example of the kind of first-draft writing that our system has to handle. Figure 1 shows a student response to the essay question: Often in life we experience a conflict in choosing between something we \"want\" to do and something we feel we \"should\" do.  In your opinion, are there any circumstances in which it is better for people to do what they  \"want\" to do rather than what they feel they \"should\" do? Support your position with evidence from your own experience or your observations of other people. The writing in Figure 1 illustrates one kind of challenge in automatic identification of discourse elements, such as thesis statements.  In this case, the two human annotators independently chose different text as the thesis statement (the two texts highlighted in bold and italics in Figure 1).  In this kind of first-draft writing, it is not uncommon for writers to repeat ideas, or express more than one general opinion about the topic, resulting in text that seems to contain multiple thesis statements. Before building a system that automatically identifies thesis statements in essays, we wanted to determine whether the task was well-defined. In collaboration with two writing experts, a simple discourse-based annotation protocol was developed to manually annotate discourse elements in essays for a single essay topic. This was the initial attempt to annotate essay data using discourse elements generally associated with essay structure, such as thesis statement, concluding statement, and topic sentences of the essay?s main ideas. The writing experts defined the characteristics of the discourse labels.  These experts then annotated 100 essay responses to one English Proficiency Test (EPT) question, called Topic B, using a PC-based interface implemented in Java. We computed the agreement between the two human annotators using the kappa coefficient (Siegel and Castellan, 1988), a statistic used extensively in previous empirical studies of discourse.  The kappa statistic measures pairwise agreement among a set of coders who make categorial judgments, correcting for chance expected agreement. The kappa agreement between the two annotators with respect to the thesis statement labels was 0.733 (N=2391, where 2391 represents the total number of sentences across all annotated essay responses).  This shows high agreement based on research in content analysis (Krippendorff, 1980) that suggests that values of kappa higher than 0.8 reflect very high agreement and values higher than 0.6 reflect good agreement.  The corresponding z statistic was 27.1, which reflects a confidence level that is much higher than 0.01, for which the corresponding z value is 2.32 (Siegel and Castellan, 1988). In the early stages of our project, it was suggested to us that thesis statements reflect the most important sentences in essays.  In terms of summarization, these sentences would represent indicative, generic summaries (Mani and Maybury, 1999; Marcu, 2000). To test this hypothesis (and estimate the adequacy of using summarization technology for identifying thesis statements), we carried out an additional experiment. The same annotation tool was used with two different human judges, who were asked this time to identify the most important sentence of each essay. The agreement between human judges on the task of identifying summary sentences was significantly lower: the kappa was 0.603 (N=2391). Tables 1a and 1b summarize the results of the annotation experiments. Table 1a shows the degree of agreement between human judges on the task of identifying thesis statements and generic summary sentences. The agreement figures are given using the kappa statistic and the relative precision (P), recall (R), and F-values (F), which reflect the ability of one judge to identify the sentences labeled as thesis statements or summary sentences by the other judge. The results in Table 1a show that the task of thesis statement identification is much better defined than the task of identifying important summary sentences. In addition, Table 1b indicates that there is very little overlap between thesis and generic summary sentences: just 6% of the summary sentences were labeled by human judges as thesis statement sentences. This strongly suggests that there are critical differences between thesis statements and summary sentences, at least in first-draft essay writing. It is possible that thesis statements reflect an intentional facet (Grosz and Sidner, 1986) of language, while summary sentences reflect a semantic one (Martin, 1992). More detailed experiments need to be carried out though before proper conclusions can be derived. Table 1a: Agreement between human judges on thesis and summary sentence identification. Metric Thesis Statements Summary Sentences Kappa 0.733 0.603 P (1 vs. 2) 0.73 0.44 R (1 vs. 2) 0.69 0.60 F (1 vs. 2) 0.71 0.51  Table 1b: Percent overlap between human labeled thesis statements and summary sentences. Thesis statements  vs. Summary sentences Percent Overlap 0.06  The results in Table 1a provide an estimate for an upper bound of a thesis statement identification algorithm. If one can build an automatic classifier that identifies thesis statements at recall and precision levels as high as 70%, the performance of such a classifier will be indistinguishable from the performance of humans.  3 A Bayesian Classifier for Identifying Thesis Statements  3.1 Description of the Approach  We initially built a Bayesian classifier for thesis statements using essay responses to one English Proficiency Test (EPT) test question: Topic B. McCallum and Nigam (1998) discuss two probabilistic models for text classification that can be used to train Bayesian independence classifiers. They describe the multinominal model as being the more traditional approach for statistical language modeling (especially in speech recognition applications), where a document is represented by a set of word occurrences, and where probability estimates reflect the number of word occurrences in a document. In using the alternative, multivariate Bernoulli model, a document is represented by both the absence and presence of features. On a text classification task, McCallum and Nigam (1998) show that the multivariate Bernoulli model performs well with small vocabularies, as opposed to the multinominal model which performs better when larger vocabularies are involved. Larkey (1998) uses the multivariate Bernoulli approach for an essay scoring task, and her results are consistent with the results of McCallum and Nigam (1998) (see also Larkey and Croft (1996) for descriptions of additional applications). In Larkey (1998), sets of essays used for training scoring models typically contain fewer than 300 documents. Furthermore, the vocabulary used across these documents tends to be restricted. Based on the success of Larkey?s experiments, and McCallum and Nigam?s findings that the multivariate Bernoulli model performs better on texts with small vocabularies, this approach would seem to be the likely choice when dealing with data sets of essay responses. Therefore, we have adopted this approach in order to build a thesis statement classifier that can select from an essay the sentence that is the most likely candidate to be labeled as thesis statement.2  2 In our research, we trained classifiers using a classical Bayes approach too, where two classifiers were built: a thesis classifier and a non-thesis In our experiments, we used three general feature types to build the classifier: sentence position; words commonly occurring in thesis statements; and RST labels from outputs generated by an existing rhetorical structure parser (Marcu, 2000). We trained the classifier to predict thesis statements in an essay. Using the multivariate Bernoulli formula, below, this gives us the log probability that a sentence (S) in an essay belongs to the class (T) of sentences that are thesis statements.  We found that it helped performance to use a Laplace estimator to deal with cases where the probability estimates were equal to zero.  i i i ii log(P(T | S)) = log(P(T)) + log(P(A | T) /P(A)), log(P(A | T) /P(A )), i i if S contains A if S does not contain A ????? ?   In this formula, P(T) is the prior probability that a sentence is in class T, P(Ai|T) is the conditional probability of a sentence having feature Ai , given that the sentence is in T, and P(Ai) is the prior probability that a sentence contains feature Ai, P( iA |T) is the conditional probability that a sentence does not have feature Ai, given that it is in T, and P( iA ) is the prior probability that a sentence does not contain feature Ai.  3.2 Features Used to Classify Thesis Statements 3.2.1 Positional Feature We found that the likelihood of a thesis statement occurring at the beginning of essays was quite high in the human annotated data. To account for this, we used one feature that reflected the position of each sentence in an essay.  classifier. In the classical Bayes implementation, each classifier was trained only on positive feature evidence, in contrast to the multivariate Bernoulli approach that trains classifiers both on the absence and presence of features. Since the performance of the classical Bayes classifiers was lower than the performance of the Bernoulli classifier, we report here only the performance of the latter. 3.2.2 Lexical Features All words from human annotated thesis statements were used to build the Bayesian classifier. We will refer to these words as the thesis word list.  From the training data, a vocabulary list was created that included one occurrence of each word used in all resolved human annotations of thesis statements.  All words in this list were used as independent lexical features. We found that the use of various lists of stop words decreased the performance of our classifier, so we did not use them. 3.2.3 Rhetorical Structure Theory Features According to RST (Mann and Thompson, 1988), one can associate a rhetorical structure tree to any text. The leaves of the tree correspond to elementary discourse units and the internal nodes correspond to contiguous text spans. Each node in a tree is characterized by a status (nucleus or satellite) and a rhetorical relation, which is a relation that holds between two non-overlapping text spans.  The distinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer?s intention than the satellite; and that the nucleus of a rhetorical relation is comprehensible independent of the satellite, but not vice versa.  When spans are equally important, the relation is multinuclear. Rhetorical relations reflect semantic, intentional, and textual relations that hold between text spans as is illustrated in Figure 2. For example, one text span may elaborate on another text span; the information in two text spans may be in contrast; and the information in one text span may provide background for the information presented in another text span. Figure 2 displays in the style of Mann and Thompson (1988) the rhetorical structure tree of a text fragment. In Figure 2, nuclei are represented using straight lines; satellites using arcs. Internal nodes are labeled with rhetorical relation names. We built RST trees automatically for each essay using the cue-phrase-based discourse parser of Marcu (2000). We then associated with each sentence in an essay a feature that reflected the status of its parent node (nucleus or satellite), and another feature that reflected its rhetorical relation. For example, for the last sentence in Figure 2 we associated the status satellite and the relation elaboration because that sentence is the satellite of an elaboration relation.  For sentence 2, we associated the status nucleus and the relation elaboration because that sentence is the nucleus of an elaboration relation. We found that some rhetorical relations occurred more frequently in sentences annotated as thesis statements. Therefore, the conditional probabilities for such relations were higher and provided evidence that certain sentences were thesis statements.  The Contrast relation shown in Figure 2, for example, was a rhetorical relation that occurred more often in thesis statements. Arguably, there may be some overlap between words in thesis statements, and rhetorical relations used to build the classifier. The RST relations, however, capture long distance relations between text spans, which are not accounted by the words in our thesis word list.  3.3 Evaluation of the Bayesian classifier  We estimated the performance of our system using a six-fold cross validation procedure. We partitioned the 93 essays that were labeled by both human annotators with a thesis statement into six groups. (The judges agreed that 7 of the 100 essays they annotated had no thesis statement.) We trained six times on 5/6 of the labeled data and evaluated the performance on the other 1/6 of the data. The evaluation results in Table 2 show the average performance of our classifier with respect to the resolved annotation (Alg. wrt. Resolved), using traditional recall (R), precision (P), and F-value (F) metrics. For purposes of comparison, Table 2 also shows the performance of two baselines: the random baseline    classifies    the     thesis statements Figure 2:  Example of RST tree. randomly; while the position baseline assumes that the thesis statement is given by the first sentence in each essay. Table 2: Performance of the thesis statement classifier. System vs. system P R F Random baseline wrt. Resolved 0.06 0.05 0.06 Position baseline wrt. Resolved 0.26 0.22 0.24 Alg. wrt. Resolved 0.55 0.46 0.50 1 wrt. 2 0.73 0.69 0.71 1 wrt. Resolved 0.77 0.78 0.78 2 wrt. Resolved 0.68 0.74 0.71  4 Generality of the Thesis Statement Identifier In commercial settings, it is crucial that a classifier such as the one discussed in Section 3 generalizes across different test questions. New test questions are introduced on a regular basis; so it is important that a classifier that works well for a given data set works well for other data sets as well, without requiring additional annotations and training. For the thesis statement classifier it was important to determine whether the positional, lexical, and RST-specific features are topic independent, and thus generalizable to new test questions.  If so, this would indicate that we could annotate thesis statements across a number of topics, and re-use the algorithm on additional topics, without further annotation. We asked a writing expert to manually annotate the thesis statement in approximately 45 essays for 4 additional test questions: Topics A, C, D and E. The annotator completed this task using the same interface that was used by the two annotators in Experiment 1. To test generalizability for each of the five EPT questions, the thesis sentences selected by a writing expert were used for building the classifier.  Five combinations of 4 prompts were used to build the classifier in each case, and the resulting classifier was then cross-validated on the fifth topic, which was treated as test data. To evaluate the performance of each of the classifiers, agreement was calculated for each ?cross-validation? sample (single topic) by comparing the algorithm selection to our writing expert?s thesis statement selections.  For example, we trained on Topics A, C, D, and E, using the thesis statements selected manually. This classifier was then used to select, automatically, thesis statements for Topic B.  In the evaluation, the algorithm?s selection was compared to the manually selected set of thesis statements for Topic B, and agreement was calculated. Table 3 illustrates that in all but one case, agreement exceeds both baselines from Table 2.  In this set of manual annotations, the human judge almost always selected one sentence as the thesis statement.  This is why Precision, Recall, and the F-value are often equal in Table 3. Table 3: Cross-topic generalizability of the thesis statement classifier. Training Topics CV Topic P R  F ABCD   E 0.36 0.36 0.36 ABCE   D 0.49 0.49 0.49 ABDE   C 0.45 0.45 0.45 ACDE   B 0.60 0.59 0.59 BCDE   A 0.25 0.24 0.25 Mean  0.43 0.43 0.43 5 Discussion and Conclusions  The results of our experimental work indicate that the task of identifying thesis statements in essays is well defined. The empirical evaluation of our algorithm indicates that with a relatively small corpus of manually annotated essay data, one can build a Bayes classifier that identifies thesis statements with good accuracy. The evaluations also provide evidence that this method for automated thesis selection in essays is generalizable.  That is, once trained on a few human annotated prompts, it can be applied to other prompts given a similar population of writers, in this case, writers at the college freshman level.  The larger implication is that we begin to see that there are underlying discourse elements in essays that can be identified, independent of the topic of the test question. For essay evaluation applications this is critical since new test questions are continuously being introduced into on-line essay evaluation applications. Our results compare favorably with results reported by Teufel and Moens (1999) who also use Bayes classification techniques to identify rhetorical arguments such as aim and background in scientific texts, although the texts we are working with are extremely noisy. Because EPT essays are often produced for high-stake exams, under severe time constraints, they are often ungrammatical, repetitive, and poorly organized at the discourse level. Current investigations indicate that this technique can be used to reliably identify other essay-specific discourse elements, such as, concluding statements, main points of arguments, and supporting ideas.  In addition, we are exploring how we can use estimated probabilities as confidence measures of the decisions made by the system. If the confidence level associated with the identification of a thesis statement is low, the system would instruct the student that no explicit thesis statement has been found in the essay.  Acknowledgements  We would like to thank our annotation experts, Marisa Farnum, Hilary Persky, Todd Farley, and Andrea King.  References  Burstein, J., Kukich, K. Wolff, S. Lu, C. Chodorow, M, Braden-Harder, L. and Harris M.D. (1998). Automated Scoring Using A Hybrid Feature Identification Technique. Proceedings of ACL, 206-210. Foltz, P. W., Kintsch, W., and Landauer, T.. (1998). The Measurement of Textual Coherence with Latent Semantic Analysis. Discourse Processes, 25(2&3), 285-307. Grosz B. and Sidner, C. (1986). Attention, Intention, and the Structure of Discourse. Computational Linguistics, 12 (3), 175-204. Krippendorff K. (1980). Content Analysis: An Introduction to Its Methodology. Sage Publ. Larkey, L. and Croft, W. B. (1996). Combining Classifiers in Text Categorization. Proceedings of  SIGIR,  289-298. Larkey, L. (1998). Automatic Essay Grading Using Text Categorization Techniques. Proceedings of SIGIR, pages 90-95. Mani, I. and Maybury, M. (1999). Advances in Automatic Text Summarization. The MIT Press. Mann, W.C. and Thompson, S.A.(1988). Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text 8(3), 243?281. Martin, J. (1992). English Text. System and Structure. John Benjamin Publishers. Marcu, D. (2000). The Theory and Practice of Discourse Parsing and Summarization. The MIT Press. McCallum, A. and Nigam, K. (1998). A Comparison of Event Models for Naive Bayes Text Classification. The AAAI-98 Workshop on \"Learning for Text Categorization\". Page, E.B. and Peterson, N. (1995). The computer moves into essay grading: updating the ancient test. Phi Delta Kappa, March, 561565. Scardamalia, M. and Bereiter, C. (1985). Development of Dialectical Processes in Composition.  In Olson, D. R., Torrance, N. and Hildyard, A. (eds), Literacy, Language, and Learning: The nature of consequences of reading and writing.  Cambridge University Press. Siegel S. and Castellan, N.J. (1988). Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill. Teufel , S. and Moens, M. (1999). Discourselevel argumentation in scientific articles. Proceedings of the ACL99 Workshop on Standards and Tools for Discourse Tagging. White E.M. (1994). Teaching and Assessing Writing. Jossey-Bass Publishers, 103-108. "}
{"date": 2014, "text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 290?300, October 25-29, 2014, Doha, Qatar. c ?2014 Association for Computational Linguistics Parsing low-resource languages using Gibbs sampling for PCFGs with latent annotations Liang Sun 1 Jason Mielens 2 1 Department of Mechanical Engineering 2 Department of Linguistics The University of Texas at Austin The University of Texas at Austin sally722@utexas.edu {jmielens,jbaldrid}@utexas.edu Jason Baldridge 2 Abstract PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing. We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations. For PCFG-LA, we present an additional Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols. We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach. 1 Introduction Despite great progress over the past two decades on parsing, relatively little work has considered the problem of creating accurate parsers for low-resource languages. Existing work in this area focuses primarily on approaches that use some form of cross-lingual bootstrapping to improve performance. For instance, Hwa et al. (2005) use a parallel Chinese/English corpus and an English dependency grammar to induce an annotated Chinese corpus in order to train a Chinese dependency grammar. Kuhn (2004b) also considers the benefits of using multiple languages to induce a monolingual grammar, making use of a measure for data reliability in order to weight training data based on confidence of annotation. Bootstrapping approaches such as these achieve markedly improved results, but they are dependent on the existence of a parallel bilingual corpus. Very few such corpora are readily available, particularly for low-resource languages, and creating such corpora obviously presents a challenge for many practical applications. Kuhn (2004a) shows some of the difficulty in handling low-resource languages by examining various tasks using Q?anjob?al as an example. Another approach is that of Bender et al. (2002), who take a more linguistically-motivated approach by making use of linguistic universals to seed newly developed grammars. This substantially reduces the effort by making it unnecessary to learn the basic parameters of a language, but it lacks the robustness of grammars learned from data. Recent work on Probabilistic Context-Free Grammars with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov et al., 2006) have shown them to be effective models for syntactic parsing, especially when less training material is available (Liang et al., 2009; Shindo et al., 2012). The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFGLA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda. Previous PCFG-LA work focuses on the problem of parameter estimation, including expectationmaximization (EM) (Matsuzaki et al., 2005; Petrov et al., 2006), spectral learning (Cohen et al., 2012; Cohen et al., 2013), and variational inference (Liang et al., 2009; Wang and Blunsom, 2013). Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse under a new sentence-specific PCFG obtained from an approximation of the original grammar (Matsuzaki et al., 2005). Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sampling algorithm of Johnson et al. (2007), which learns rule probabilities in an unsupervised PCFG. We use a Gibbs sampler to collect sampled trees theoretically distributed from the true posterior distribution in order to parse. Priors in a Bayesian model can control the sparsity of grammars (which the insideoutside algorithm fails to do), while naturally incorporating smoothing into the model (Johnson et al., 2007; Liang et al., 2009). We also build a Bayesian model for parsing with a treebank, and incorporate information from training data as a prior. Moreover, we extend the Gibbs sampler to learn and parse PCFGs with latent annotations. Learning the latent annotations is a compute-intensive process. We show how a small amount of training data can be used to bootstrap: after running a large number of sampling iterations on a small set, the resulting parameters are used to seed a smaller number of iterations on the full training data. 290 This allows us to employ more latent annotations while maintaining reasonable training times and still making full use of the available training data. To determine the cross-linguistic applicability of these methods, we evaluate on a wide variety of languages with varying amounts of available training data. We use English and Chinese as examples of languages with high data availability, while Italian, Malagasy, and Kinyarwanda provide examples of languages with little available data. We find that our technique comes near state of the art results on large datasets, such as those for Chinese and English, and it provides excellent results on limited datasets ? both artificially limited in the case of English, and naturally limited in the case of Italian, Malagasy, and Kinyarwanda. This, combined with its ability to run off-the-shelf on new languages without any supporting materials such as parallel corpora, make it a valuable technique for the parsing of low-resource languages. 2 Gibbs sampling for PCFGs Our starting point is a Gibbs Sampling algorithm for vanilla PCFGs introduced by Johnson et al. (2007) for estimating rule probabilities in an unsupervised PCFG. We focus instead on using this algorithm for parsing new sentences and then extending it to learn PCFGs with latent annotations. We begin by summarizing the Bayesian PCFG and Gibbs sampler defined by Johnson et al. (2007). Bayesian PCFG For a grammarG, each rule r in the set of rules R has an associated probability ? r . The probabilities for all the rules that expand the same nonterminal A must sum to one: ? A???R ? A?? = 1. Given an input corpusw=(w (1) , ? ? ? ,w (n) ), we introduce a latent variable t=(t (1) , ? ? ? , t (n) ) for trees generated by G for each sentence. The joint posterior distribution of t and ? conditioned on w is: p(t, ? | w) ? p(?)p(w | t)p(t | ?) = p(?)( ? n i=1 p(w (i) | t (i) )p(t (i) | ?)) = p(?)( ? n i=1 p(w (i) | t (i) ) ? r?R ? f r (t (i) r )) (1) Here f r (t) is the number of occurrences of rule r in the derivation of t; p(w (i) | t (i) ) = 1 if the yield of t (i) is the sequence w (i) , and 0 otherwise. We use a Dirichlet distribution parametrized by ? A : Dir(? A ) as the prior of the probability distribution for all rules expanding non-terminal A (p(? A )). The prior for all ?, p(?), is the product of all Dirichlet distributions over all non-terminals A ? N : p(? | ?) = ? A?N p(? A | ? A ). Since the Dirichlet distribution is conjugate to the Multinomial distribution, which we use to model the likelihood of trees, the conditional posterior of ? A can be updated as follows: p G (? | t, ?) ? p G (t | ?)p(? | ?) ? ( ? r?R ? f r (t) r )( ? r?R ? ? r ?1 r ) = ? r?R ? f r (t)+? r ?1 r (2) which is still a Dirichlet distribution with updated parameter f r (t) + ? r for each rule r ? R. Gibbs sampler The parameters of the PCFG model can be learned from an annotated corpus by simply counting rules. However, parsing cannot be done directly with standard CKY as with standard PCFGs, so we use the Gibbs sampling algorithm presented in Johnson et al. (2007). An additional motivation for using this algorithm is that Johnson et al. use it for learning without annotated structures, and in future work we seek to learn from fewer, and at times partial, annotations. An advantage of using Gibbs sampling for Bayesian inference, as opposed to other approximation algorithms such as Variational Bayesian inference (VB) and Collapsed Variational Bayesian inference (CVB), is that Markov Chain Monte Carlo (MCMC) algorithms are guaranteed to converge to a sample from the true posterior under appropriate conditions (Taddy, 2011). Both VB and CVB converge to inaccurate and locally optimal solutions, like EM. In some models, CVB can achieve more accurate results due to weaker assumptions (Wang and Blunsom, 2013). Another advantage of Gibbs sampling is that the sampler allows for parallel computation by allowing each sentence to be sampled entirely independently of the others. After each parallel sampling stage, all model parameters are updated in a single step, and the process then repeats (see ?2). To sample the joint posterior p(t, ? | w), we sample production probabilities ? and then trees t from these conditional distributions: p(t | ?,w, ?) = ? n i=1 p(t i | w i , ?) (3) p(? | t,w, ?) = ? A?N Dir(? A | f A (t) + ? A ) (4) Step 1: Sample Rule Probabilities. Given trees t and prior ?, the production probabilities ? A for each nonterminal A?N are sampled from a Dirichlet distribution with parameters f A (t) + ? A . f A (t) is a vector, and each component of f A (t), is the number of occurrences of one rule expanding nonterminal A. Step 2: Sample Tree Structures. To sample trees from p(t i | w i , ?), we use the efficient sampling scheme used in previous work (Goodman, 1998; Finkel et al., 2006; Johnson et al., 2007). There are two parts to this algorithm. The first constructs an inside table as in the Inside-Outside algorithm for PCFGs (Lary and Young, 1990). The second selects the tree by recursively sampling productions from top to bottom. 291 Require: A is parent node of binary rule; w i,k is a span of words: i+ 1 < k function TREESAMPLER(A, i, k) for i < j < k and pair of child nodes of A:B,C do P (j, B,C) = ? A?BC ?p B,i,j ?p C,j,k ? p A,i,k end for Sample j?, B?, C? from multinomial distribution for (j, B,C) with probabilities calculated above return j?, B?, C? end function Algorithm 1: Sampling split position and rule to expand parent node Consider a sentence w, with sub-spans w i,k = (w i+1 , ? ? ? , w k ). Given ?, we construct the inside table with entries p A,i,k for each nonterminal and each word span w i,k : 0 ? i < k ? l, where p A,i,k = P G A (w i,k |?) is the probability that words i through k were produced by the non-terminal A. The table is computed recursively by p A,k?1,k = ? A?w k (5) p A,i,k = ? A?BC?R ? i<j<k ? A?BC ? p B,i,j ? p C,j,k (6) for all A,B,C ? N and 0 ? i < j < k ? l. The resulting inside probabilities are then used to generate trees from the distribution of all valid trees of the sentence. The tree is generated from top to bottom recursively with the function TreeSampler defined in Algorithm 1. In unsupervised PCFG learning, the rule probabilities can be resampled using the sampled trees, then used to reparse the corpus, and so on. We use this property to refine latent annotations for the PCFG-LA model described in the next section. 3 PCFG with latent annotations When labeled trees are available, rule frequencies can be directly extracted and used as priors for a PCFG. However, when learning PCFG-LAs, we must learn the fine-grained rules from the coarse trees, so we extend the Gibbs sampler to assign latent annotations to unannotated trees. The resulting learned PCFG-LA parser outputs samples of annotated trees so that we can obtain unannotated trees after marginalizing. 3.1 Model With the PCFG-LA model (Matsuzaki et al., 2005; Petrov et al., 2006) fine-grained CFG rules are automatically induced from training, effectively providing a form of feature engineering without human intervention. GivenH = {1, ? ? ? ,K}, a set of latent annotation symbols, and x ? H: ? ? A[x]?U is the probability of rule A[x] ? U , where U ? N ?N ? T . The probabilities for all rules that expand the same annotated non-terminal must sum to one. ? ? A[x],B,C?y,z is the probability of assigning latent annotation y, z to child nodes B,C of A[x]. ? y,z?H?H ? A[x],B,C?y,z = 1. The inputs to the PCFG-LA are a CFG G with finite number of latent annotations for each non-terminal, an initial guess of probabilities of grammar rule ? 0 , and a prior ? ? is learned from training. The joint posterior distribution of t and ?, ? conditioned on w is: p(t, ?, ? | w) ? p(?, ?)p(w | t)p(t | ?, ?) = p(?)p(?)( ? n i=1 p(w i | t i )p(t i | ?, ?)) (7) We assume that ? and ? are independent to get P (?, ?) = P (?)P (?). To learn parameters ?, ?, we use a Dirichlet distribution as a prior for both ? and ?. The distribution for all rules expanding A[x] is: P (? | ? ? ) = ? A?N,x?H P (? A[x] | ? ? A[x] ) (8) The distribution for latent annotations associated with child nodes of A[x]? BC is: P (? | ? ? ) = ? y,z?H?H P (? A[x],B,C | ? ? A[x],B,C ). (9) With this setting, the conditional posterior of ? A[x] and ? A[x],B,C can be updated, as in ?2. For all unary and binary rules r expanding A[x]: ? A[x] | t, ? ? ? Dir(f r (t) + ? ? r ) (10) Here, f r (t) is the number of occurrence of annotated rule r in t. Also, for combination of latent annotations y, z ? H ?H assigned to B,C in rule A[x]? B,C: ? A[x],B,C | t, ? ? ? Dir(f d (t) + ? ? d ) (11) Here, f d (t) is the number of occurrences of combination d in t. 3.2 Learning PCFG-LAs from raw text To learn from raw text, we extend the sampler in ?2 to PCFG-LA. Given priors ? ? , ? ? and raw text, the algorithm alternates between two steps. The first samples trees for the entire corpus; the second samples ? and ? from Dirichlet distributions with updated parameters, combining priors and counts from sampled trees. The algorithm then alternates between these steps until convergence. The outputs are samples of ?, ? and annotated trees. The parsing process is specified in Algorithm 2. The first step assigns a tree to a sentence, say w 0,l . We first 292 Require: w 1 , ? ? ? , w n are raw sentences; ? 0 , ? 0 are initial values; ? ? , ? ? are priors; M is the number of iterations function PARSE(w 1 , .., w n , ? 0 , ? 0 , ? ? , ? ? ,M ) for iteration i = 1 to M do for sentence s = 1 to n do Calculate Inside Table Sample tree nodes and associated latent annotations, get tree structure t (i) s end for Sample ? (i) , ? (i) end for for sentence s = 1 to n do Marginalize the latent annotations to get unannotated trees T (1) s , ? ? ? , T (M) s Find the mode of T (1) s , ? ? ? , T (M) s : T s end for return T 1 , ? ? ? , T n end function Algorithm 2: Parsing new sentences construct an inside table (see ?2). Each entry in the table stores the probability that a word span is produced by a given annotated nonterminal. For root node S, with ?, ? and inside table p A[x],i,k , we sample one annotation based on all p S[x],0,l , x ? H . Assume that we sampled x for S, we further sample a rule to expand S[x] and possible splits of the span w 0,l jointly. Assume that we sampled nonterminals B,C to expand S[x], where B is responsible for w 0,j and C is responsible for w j,l . We further sample annotations for B,C together, say y, z. Then we sample rules and split positions to expand B[y] and C[z], and continue until reaching the terminals. This algorithm alone could be used for unsupervised learning of PCFG-LA if we input a non-informed or weakly-informed prior ? ? and ? ? . With access to unannotated trees for training, we only need to assign latent annotations to them and then use the frequencies of these annotated rules as the prior when parsing. The details of training when trees are available are illustrated in ?3.3. Once we have trees (with latent annotations), the step of sampling ? and ? from a Dirichlet distribution is direct. We need to count the number of occurrences f r (t) for each rule r like A[x] ? U,U ? N ?N ? T in updated annotated trees t, and draw ? A[x] from the updated Dirichlet distribution Dir(f A[x] (t) + ? ? A[x] ). We also need to count the number of occurrences of f d (t) for each combination of yz ? H?H assigned to B,C givenA[x]? B,C in t, and draw ? A[x],B,C from the updated Dirichlet distribution Dir(f A[x],B,C (t) + ? ? A[x],B,C ) similarly. To parse a sentence, we first calculate the inside table and then sample the tree. Calculate the inside table. Given ?,? and a string w=w 0,l , we construct a table with entries p A[x],i,k for each A?N , x ? H and 0 ? i < k ? l, where p A[x],i,k = P G A[x] (w i,k |?, ?) is the probability that words i through k were produced by the annotated nonterminal A[x]. The table can be computed recursively, for all A ? N , x ? H , by p A[x],k?1,k = ? A[x]?w k (12) p A[x],i,k = ? A[x]?BC:BC?N?N ? j:i<j<k ? yz?H?H ? A[x]?BC ? A[x]BC?yz p B[y],i,j p C[z],j,k (13) Sample the tree, top to bottom. First, from start symbol S, sample latent annotation from multinomial with probability pi S[x] p S[x],0,l for each x ? H . Next, given annotated non-terminal A[x] and i, k, sample possible child nodes and split positions from multinomial with probability: p(B,C, j) = 1 p A[x],i,k ? ? y,z?H ? A[x]?BC ? A[x]BC?yz p B[y],i,j p C[z],j,k (14) Here the probability is calculated by marginalizing all possible latent annotations for B,C, and ? A[x]?BC ? A[x]BC?yz is the probability of choosing B[y], C[z] to expandA[x], and p B[y],i,j p C[z],j,k are the probabilities for B[y] and C[z] to be responsible for word span w i,j and w j,k respectively. And p A[x],i,k is the normalizing term. Third, given A[x], B,C, i, j, k, sample annotations for B,C from multinomial with probability: p(y, z) = ? A[x]BC?yz p B[y],i,j p C[z],j,k ? y,z ? A[x]BC?yz p B[y],i,j p C[z],j,k (15) A crucial aspect of this procedure is that all trees can be sampled independently. This parallel process produces a substantial speed gain that is important particularly when using more latent annotations. After all trees have been sampled (independently), the counts from each individual tree are combined prior to the next sampling iteration. 3.3 Learning from coarse training trees In training, we need to learn the probabilities of finegrained rules given coarsely-labeled trees. We perform Gibbs sampling on the training data by first iteratively sampling probabilities and then assigning annotations to tree nodes. We use the average counts of annotated production rules from sampled trees to produce the prior ? ? and ? ? incorporated into parsing raw sentences. We first index the non-terminal nodes of each tree T by 1, 2, ? ? ? from top to bottom, and left to right. Then the sampler iterates between two steps. The first samples ?, ? given annotated trees (as in ?3.2). The second samples latent annotations for nonterminal nodes 293 Require: T 1 , ? ? ? , T n are fully parsed trees; ? 0 , ? 0 are initial values; ? ? 0 , ? ? 0 are priors; M is the number of iterations function ANNO(T 1 , ? ? ? , T n , ? 0 , ? 0 , ? ? 0 , ? ? 0 ,M ) for iteration i = 1 to M do for sentence s = 1 to n do Calculate inside probability Sample latent annotations for each node in the tree, get tree with latent annotations t (i) s end for Sample ? (i) , ? (i) end for return Mean of number of occurrences of production rules and associated latent annotations from all sampled annotated trees end function Algorithm 3: Learning prior from training in parsed trees, which also takes two steps. The first step is to, for each node in the tree, calculate and store the probability that the node is annotated by x. The second step is to jointly sample latent annotations for child nodes of root nodes, and then continue this process from top to bottom until reaching the pre-terminal nodes. Step one: inside probabilities. Given tree T , compute b i T [x] for each non-terminal i recursively: 1. If node N i is a pre-terminal node above terminal symbol w, then for x?H b i T [x] = ? N i [x]?w (16) 2. Otherwise, let j, k be two child nodes of i, then for x ? H b i T [x] = ? y,z?H ? N i [x]?N j N k ? N i [x]N j N k ?y,z b j T [y]b k T [z] (17) Step two: outside sampling. Given inside probability b i T [x] for every non-terminal i and all latent annotations x?H , we sample the latent annotations from top to bottom: 1. If node i is the root node (i = 1), then sample x ? H from a multinomial distribution with f i T [x] = pi(N i [x]). 2. For a parent node with sampled latent annotation N i [x] with childrenN j , N k , sample latent annotations for these two nodes from a multinomial distribution with f i T [y, z] = 1 b i T [x] ? ? N i [x]?N j N k ? N i [x]N j N k ?y,z b j T [y]b k T [z] (18) After training, we take the average counts of sampled annotated rules and combinations of latent annotations as priors to parse raw sentences. 4 Experiments 1 Our goal is to understand parsing efficacy using sampling and latent annotations for low-resource languages, so we perform experiments on five languages with varying amount of training data. We compare our results to a number of previously established baselines. First, for all languages, we use both a standard unsmoothed PCFG and the Bikel parser, trained on the training corpus. Additionally, we compare to state-of-the-art results for both English and Chinese, which have an existing body of work in PCFGs using a Bayesian framework. For Chinese, we compare to Huang & Harper (2009), using their results that only use the Chinese Treebank (CTB). For English, we compare to Liang et al. (2009). Prior results for parsing the constituency version of the Italian data are available from Alicante et al. (2012), but as they make use of a different version of the treebank including extra sentences, and additionally use the extensive functional tags present in the corpus, we do not directly compare our results to theirs. 2 4.1 Data English (ENG) and Chinese (CHI) are the two main languages used for this work; they are commonly used in parser evaluation and have previous examples of statistical parsers using a Bayesian framework. And since we primarily are interested in parsing low-resource languages, we include results for Kinyarwanda (KIN) and Malagasy (MLG) as examples of languages without substantial existing treebanks. Finally, as a middleground language, we use Italian (ITL). For English, we use the Wall-Street Journal section of the Penn Treebank (WSJ) (Marcus et al., 1993). The data split is sections 02-21 for training, section 22 for development, and section 23 for testing. For Chinese, the Chinese Treebank (CTB5) (Xue et al., 2005) was used. The data split is files 81-899 for training, files 4180 for development, and files 1-40/900-931 for testing. The ITL data is from the Turin University Treebank (TUT) (Bosco et al., 2000) and consists of 2,860 Italian sentences from a variety of domains. It was split into training, development, and test sets with a 70/15/15 percentage split. The KIN texts are transcripts of testimonies by survivors of the Rwandan genocide provided by the Kigali Genocide Memorial Center, along with a few BBC news articles. The MLG texts are articles from the websites Lakroa and La Gazette and Malagasy Global Voices. Both datasets are described in Garrette and Baldridge (2013). The KIN and MLG data is very small compared to ENG and CHI: the KIN dataset con1 Code available at github.com/jmielens/gibbs-pcfg-2014, along with instructions for replicating experiments when possible 2 As part of a standardized pre-processing step, we strip functional tags, which makes a direct comparison to their results inappropriate. 294 tains 677 sentences, while the MLG dataset has only 113. Also, we simulated a small training set for ENG data by using only section 02 of the WSJ for training. 4.2 Experimental Setup As a preprocessing step, all trees are converted into Chomsky Normal-Form such that all non-terminal productions are binary and all unary chains are removed. Additional standard normalization is performed. Functional tags (e.g. the SBJ part of NP-SBJ), empty nodes (traces), and indices are removed. Our binarization is simple: given a parent, select the rightmost child as the head and add a stand-in node that contains the remainder of the original children; the process then recurses. This simple technique uses no explicit headfinding rules, which eases cross-linguistic applicability. From this normalized data, we train latent PCFGs with K=1,2,4,8,16,32 (where K=1 is equivalent to the plain PCFG described in section 2). 4.3 Practical refinements Unknown word handling. We use a similar unknown word handling procedure to Liang et al. (2009). From our raw corpus we extract features associated with every word, these features include surrounding context words as well as substring suffix/prefix features. Using these features we produce fifty clusters using k-means. Then, as a pre-parsing step, we replace all words occurring less than five times with their cluster label this simulates unknown words for training. Finally, during evaluation, any word not seen in training was also replaced with its corresponding cluster label. This final step is simple because there are no ?unknown unknowns? in our corpus, as the clustering has been performed over the entire corpus prior to training. This approach is similar to methods for unsupervised POStag induction that also utilize clusters in this manner (Dasgupta & Ng, 2007). We compare this unknown word handling method to one in which the clustering and a classifier is trained not on the corpus under consideration, but rather on a separate corpus of unrelated data. This comparison was made to understand the effects of including the evaluation set in the training data (without labels) versus training on out-of-domain texts. This is a more realistic measurement of out-of-the-box performance of a trained model. Jump-starting sampling. In the basic setup, training high K-value models takes a prohibitively long time, so we also consider a jump-start technique that allows larger annotation values (such as K=16) to be run in less time. We train these high-K value models first on a highly reduced training set (5% of the full training set) for a large number of iterations, and then use the found ? values as the starting point for training on the full training set for a small number of iterations. Although many of the estimated parameters on the reduced set will be zero, the prior allows us to eventually System K=1 K=2 K=4 K=8 K=16 Unsmoothed PCFG 40.2 ? ? ? ? Bikel Parser 57.9 ? ? ? ? Liang et al. 07 60.5 71.1 77.2 79.2 78.2 Berkeley Parser 60.8 74.4 78.4 79.1 78.7 Gibbs PCFG 61.0 71.3 76.6 78.7 78.0 Table 1: F1 scores for small English training data experiments. ?K? is the number of latent annotations ? K=1 represents a vanilla, unannotated PCFG. recover this information in the full set. This allows us to train on the full training set, which is desirable relative to training on a reduced set, while still allowing the model sufficient iterations to burn in. The fact that we are likely starting in a fairly good position within the search space (due to estimating ? from the corpus) also likely helps enable these lower iteration counts. 5 Results We start with Tables 1 and 2, which show performance when training on section 02 of the WSJ (pretending English is a ?low-resource? language). The results show that the basic Gibbs PCFG (where K=1), with an Fscore of 61.0, substantially outperforms not only an unsmoothed PCFG (the simplest baseline), but also the Bikel parser (Bikel, 2004b) trained on the same amount of data. Table 1 also shows further large gains are obtained from using latent annotations?from 60.5 for K=1 to 78.7 for K=8. The Gibbs PCFG also compares quite favorably to the PCFG-LA of Liang et al. (2009)?slightly better for K=1 and K=2 and slightly worse for K=4 and K=8. Table 2 shows that the Gibbs PCFG is able to produce results with a smaller amount of variance relative to the Berkeley Parser, even at low training sizes. This trend is repeated in Table 3, which shows that the Gibbs PCFG also produces less variance when training on different single sections of the WSJ relative to the Berkeley Parser, although it again produces slightly lower F1 scores. We also use the small English corpus to determine the effects of weighting the prior when sampling annotations, varying ? between 0.1 and 10.0. Though performance is not sensitive to varying ? for larger corpora, Figure 1 shows it can make a substantial difference for smaller corpora (with an optimal value was obtained with an ? value of 5 for this small training set). This seems to indicate that the lower counts associated with the smaller training sets should be compensated for by weighting those counts more heavily when processing the evaluation set, as we had anticipated. System WSJ Sec. 02 KIN MLG Berkeley Parser 78.3 ? 0.93 60.6 ? 1.1 52.2 ? 2.0 Gibbs PCFG 76.7 ? 0.63 67.2 ? 0.92 57.5 ? 1.1 Table 2: F1 scores with standard deviation over ten runs of small training data, K=4. 295 System F1 / StDev Berkeley Parser 77.5 ? 2.1 Gibbs PCFG 77.0 ? 1.4 Table 3: F1 scores with standard deviations over twenty runs, training on individual WSJ sections (02-21). Figure 1: Accuracy by varying ? levels for small English data. To evaluate the effectiveness of the jump-start technique, we ran the full ENG data set with K=4 to compare the results from the full training setup to jumpstarting. For this, we performed 100 training iterations on the reduced training set (WSJ section 02) and then used the resulting ? values to seed training on the full training set. Those training runs varied between three and nine iterations, and the results are shown in Figure 2. The full ENG K=4 F-score is 86.2, so these results represent a slight step back. Nonetheless, the technique is still valuable in that it allows for inferring latent annotations for higher K-values than would typically be available to us in a reasonable timeframe. Table 4 shows the results for the main experiments. Sampling a vanilla PCFG (K=1) produces results that are not state-of-the-art, but still good overall and always better than an unsmoothed PCFG. The benefits of the latent annotations are further shown in the increase Condition ENG CHI ITA KIN MLG Unsmoothed PCFG 69.9 66.8 62.1 45.9 49.2 Liang et al. 07 87.1 ? ? ? ? Huang & Harper09 ? 84.1 ? ? ? Bikel Parser 86.9 81.1 74.5 55.7 49.5 Berkeley Parser 90.1 83.4 71.6 61.4 51.8 Gibbs PCFG,K=1 79.3 75.4 66.3 58.5 55.1 Gibbs PCFG,K=2 82.6 79.8 69.3 65.0 57.0 Gibbs PCFG,K=4 86.0 82.3 71.9 67.2 57.8 Gibbs PCFG,K=16 87.2 83.2 72.4 68.1 58.2 Gibbs PCFG,K=32 87.4 83.4 71.0 66.8 55.3 Table 4: F1 scores for experiments on sampled PCFGs. Note that Wang and Blunsom (2013) obtain an ENG Fscore of 77.9% using collapsed VB for K=2. Though they do not give exact numbers, their Fig. 7 indicates an F-score of about 87% for K=16. Figure 2: F-Score for K=4, varying full-set training iterations (with and without 100x jump start). of F1 score in all languages, as compared to the vanilla PCFG. Experiments were run up to K=32 primarily due to time constraint. Although previous literature results report increases up to the equivalent of K=64, it may be the case that higher K values with no merge step more easily lead to overfitting in our model ? reducing the effectiveness of those high values, as shown by the overall poorer performance on several languages at K=32 when compared to K=16 as well as the general levelling-off seen at the high K values. For English and Chinese, the previous Bayesian framework parsers outperform our own, but only by around two points. Additionally, our parsing of Chinese improves on the Bikel parser (trained on our training data) despite the fact that the Bikel parser makes use of language specific optimizations. Our parser needs no changes to switch languages. The Gibbs PCFG with K=16 is superior to the strong Bikel and Berkeley Parser benchmarks for both KIN and MLG, a promising result for future work on parsing low-resource languages in general. Note also that our parser exhibits less variance than Berkeley Parser especially for KIN and MLG, which supports the fact that the variance of Berkeley Parser is higher for models with few subcategories (Petrov et al., 2006). Examples of the improvement across latent annotations for a given tree are shown in Figure 3. The details of the noun phrase ?no major bond offerings? were the same for each tree, and are thus abstracted here. The low K-value tree (K=2) is shown in 3a, and primarily suffers from issues related to the prepositional phrase, ?in Europe friday?. In particular, the low K-value tree incorrectly groups ?Europe friday? as a noun phrase object of ?in?. The higher K-value tree (K=8) is shown in 3b. This tree manages to correctly analyze the prepositional phrase, accurately separately the temporal locative ?Friday? from the actual prepositional phrase of ?in Europe?. However, the high K-value tree makes a 296 Figure 3: Examples of tree progression in the Gibbs PCFG with a) K=2, b) K=8, and c) gold tree. different mistake that the low K-value tree did not; it groups ?no major bond offerings in Europe Friday? as a noun phrase, when it should be three separate phrases (two noun phrases and a prepositional phrase). This error may be related to the additional latent annotations. With more available noun phrase subtypes, it may be the case that a more unusual noun phrase could be permitted that would have been too low probability with only a few subtypes. To determine whether the substantial range in F1 scores across languages are primarily the result of the much larger training corpora available for certain languages, two extreme training set reduction experiments were conducted. The training sets for all languages were reduced to a total of either 100 or 500 sentences. This process was repeated 10 times in a crossvalidation setup, where 10 separate sets of sentences were selected for each language. The results of these experiments are shown in Table 5. We conclude that while data availability is a major factor in the higher performance of English and Chinese in our original experiments, it is not the only issue. Clearly, either the linguistic facts of particular languages or perhaps choices of formalism and annotation conventions in the corpora make some of the languages more difficult to parse than others. The primary questions is why Gibbs-PCFG is able to achieve higher relative performance on the KIN/MLG datasets when compared to the other parsers, and why this advantage does not necessarily transfer to the extreme small-scale versions of the ENG/CHI/ITL data. Preliminary investigation into the properties of the corpora have revealed a number of potential answers. For instance, the POS tagsets for KIN/MLG are substantially reduced compared to the other corpora, and there are differences in the branching factor of the native versions of the corpora as well: a typical maximum branching factor for a tree in ENG/CHI/ITL is around 4-5, while for KIN/MLG it is almost always 2 (natively binary). Branching factors above 5 essentially never occur in KIN/MLG, while they are not rare in ENG/CHI/ITL. The question of exactly why the Gibbs-PCFG seems to perform well on these corpora remains an open question, but these differences could provide a starting point Condition In-Domain Out-of-Domain Full English (K=4) 86.0 83.3 Small English (K=4) 76.6 75.7 Kinyarwanda (K=4) 67.2 65.1 Malagasy (K=4) 57.8 55.4 Table 6: Effect of differing regimes for handling unknown words. for future analysis. In addition to the actual F1 scores, the relative uniformity of the standard deviation results indicates that the individual parsers are not that much different in terms of their ability to provide consistent results at these small data extremes, as opposed to the slightly higher training levels where the Gibbs-PCFG generated smaller variances. Considering the effects of unknown word handling, Table 6 shows that using the evaluation set when creating the unknown word classifier does improve overall parsing accuracy when compared to an unknown word handler that is trained on out-of-domain texts. This shows that results reported in previous work somewhat overstate the accuracy of these parsers when used in the wild?which matters greatly in the low-resource setting. 6 Conclusion Our experiments demonstrate that sampling vanilla PCFGs, as well as PCFGs with latent annotations, is feasible with the use of a Gibbs sampler technique and produces results that are in line with previous parsers on controlled test sets. Our results also show that our methods are effective on a wide variety of languages?including two low-resource languages? with no language-specific model modifications needed. Additionally, although not a uniform winner, the Gibbs-PCFG shows a propensity for performing well on naturally small corpora (here, KIN/MLG). The exact reason for this remains slightly unclear, but the fact that a similar advantage is not found for extremely small versions of large corpora indicates that our approach may be particularly well-suited for application in real low-resource environments as opposed to a sim297 Parser Size ENG CHI ITL KIN MLG Bikel 100 54.7 ? 2.2 51.4 ? 3.0 51 ? 2.4 47.1 ? 2.3 44.4 ? 2.0 Berkeley 100 55.2 ? 2.6 53.9 ? 2.9 50 ? 2.8 47.8 ? 2.1 44.5 ? 2.3 Gibbs-PCFG 100 54.5 ? 2.0 51.7 ? 2.4 49.5 ? 3.6 50.3 ? 2.3 45.8 ? 1.8 Bikel 500 56.2 ? 2.0 54.1 ? 2.7 54.2 ? 2.4 ? ? Berkeley 500 58.9 ? 2.2 56.4 ? 2.7 52.5 ? 2.7 ? ? Gibbs-PCFG 500 58.1 ? 2.0 55.7 ? 2.3 51.1 ? 3.2 ? ? Table 5: 100/500 sentence training set results, including st.dev over 10 runs. KIN/MLG did not have enough data to run the 500 sentence version. ulated environment. Having established this procedure and its relative tolerance for low amounts of data, we would like to extend the model to make use of partial bracketing information instead of complete trees, perhaps in the form of Fragmentary Unlabeled Dependency Grammar annotations (Schneider et al., 2013). This would allow the sampling procedure to potentially operate using corpora with lighter annotations than full trees, making initial annotation effort not quite as heavy and potentially increasing the amount of available data for low-resource languages. Additionally, using the expert partial annotations to help restrict the sample space could provide good gains in terms of training time. Acknowledgments Supported by the U.S. Army Research Office under grant number W911NF-10-1-0533. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the U.S. Army Research Office. References Anita Alicante, Cristina Bosco, Anna Corazza, and Alberto Lavelli. 2012. A treebank-based study on the influence of Italian word order on parsing performance. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of LREC?12, Istanbul, Turkey. European Language Resources Association (ELRA). Emily M. Bender, Dan Flickinger, and Stephan Oepen. 2002. The Grammar Matrix: An Open-Source Starter-Kit for the Rapid Development of CrossLinguistically Consistent Broad-Coverage Precision Grammars. In John Carroll, Nelleke Oostdijk, and Richard Sutcliffe, editors, Proceedings of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics, pages 8?14, Taipei, Taiwan. Dan Bikel. 2004a. On The Parameter Space of Generative Lexicalized Statistical Parsing Models. Ph.D. thesis, University of Pennsylvania. Daniel M Bikel. 2004b. Intricacies of Collins? parsing model. Computational Linguistics, 30(4):479?511. Ezra Black, Fred Jelinek, John Lafferty, David M Magerman, Robert Mercer, and Salim Roukos. 1992. Towards history-based grammars: Using richer models for probabilistic parsing. In Proceedings of the workshop on Speech and Natural Language, pages 134?139. Association for Computational Linguistics. Taylor L Booth and Richard A Thompson. 1973. Applying probability measures to abstract languages. Computers, IEEE Transactions on, 100(5):442?450. Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo, and Leonardo Lesmo. 2000. Building a Treebank for Italian: a Data-driven Annotation Schema. In In Proceedings of the Second International Conference on Language Resources and Evaluation LREC-2000 (pp. 99, pages 99?105. Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. Department of Computer Science, Univ. Eugene Charniak. 1996. Tree-bank grammars. In Proceedings of the National Conference on Artificial Intelligence, pages 1031?1036. Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132?139. Association for Computational Linguistics. Noam Chomsky. 1956. Three models for the description of language. Information Theory, IRE Transactions on, 2(3):113?124. Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 223?231. Association for Computational Linguistics. Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2013. Experiments with spectral learning of latent-variable PCFGs. In Proceedings of NAACL-HLT, pages 148?157. 298 Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 184?191. Association for Computational Linguistics. Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 16?23. Association for Computational Linguistics. Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational linguistics, 29(4):589?637. Jenny Rose Finkel, Christopher D Manning, and Andrew Y Ng. 2006. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 618?626. Association for Computational Linguistics. Dan Garrette and Jason Baldridge. 2013. Learning a Part-of-Speech Tagger from Two Hours of Annotation. In Proceedings of NAACL, Atlanta, Georgia. Dan Garrette, Jason Mielens, and Jason Baldridge. 2013. Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages. In Proceedings of the 51th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics. Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721?741. Joshua T Goodman. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University Cambridge, Massachusetts. Zhongqiang Huang and Mary Harper. 2009. SelfTraining PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 832?841. Association for Computational Linguistics. Rebecca Hwa, Philip Resnik, and Amy Weinberg. Breaking the Resource Bottleneck for Multilingual Parsing. In The Proceedings of the Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data. Conference on Language Resources and Evaluation. Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov Chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139?146. Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613?632. Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423?430. Association for Computational Linguistics. Jonas Kuhn. 2004a. Applying computational linguistic techniques in a documentary project for Qanjobal (Mayan, Guatemala). In In Proceedings of LREC 2004. Citeseer. Jonas Kuhn. 2004b. Experiments in parallel-text based grammar induction. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 470. Association for Computational Linguistics. Karim Lary and Steve J Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algrithm. Computer, Speech and Language, 4:35?56. Percy Liang, Michael I Jordan, and Dan Klein. 2009. Probabilistic grammars and hierarchical Dirichlet processes. The handbook of applied Bayesian analysis. David M Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 276?283. Association for Computational Linguistics. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313?330. Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 75?82. Association for Computational Linguistics. Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings of the 30th annual meeting on Association for Computational Linguistics, pages 128?135. Association for Computational Linguistics. Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In HLT-NAACL, pages 404?411. Slav Petrov and Dan Klein. 2008. Sparse multi-scale grammars for discriminative latent variable parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 867?876. Association for Computational Linguistics. 299 Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433? 440. Association for Computational Linguistics. Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models. In ACL, pages 1077?1086. Nathan Schneider, Brendan O?Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A Smith, Chris Dyer, and Jason Baldridge. 2013. A framework for (under) specifying dependency syntax without overloading annotators. arXiv preprint arXiv:1306.2091. Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume 1, pages 440?448. Association for Computational Linguistics. Matthew A Taddy. 2011. On estimation and selection for topic models. arXiv preprint arXiv:1109.4518. Pengyu Wang and Phil Blunsom. 2013. Collapsed Variational Bayesian Inference for PCFGs. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 173? 182, Sofia, Bulgaria, August. Association for Computational Linguistics. Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Nat. Lang. Eng., 11(2):207?238, June. 300 "}
{"date": 2010, "text": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944?952, MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics Automatic Evaluation of Translation Quality for Distant Language Pairs Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada NTT Communication Science Laboratories, NTT Corporation 2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan {isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jp Abstract Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate ?A because B? as ?B because A.? Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al, 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson?s correlation coefficient and Spearman?s rank correlation ? with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following Fujii et al (2008) In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means 944 (R0) he was interested in world history because he read the book into an English sentence such as (H0) he read the book because he was interested in world history in which the cause and the effect are swapped. Why does this happen? The former half of (J0) means ?He read the book,? and the latter half means ?(he) was interested in world history.? The middle word ?node? between them corresponds to ?because.? Therefore, SMT systems output sentences like (H0). On the other hand, Rule-based Machine Translation (RBMT) systems correctly give (R0). In order to find (R0), SMT systems have to search a very large space because we cannot restrict its search space with a small distortion limit. Most SMT systems thus fail to find (R0). Consequently, the global word order is essential for translation between distant language pairs, and wrong word order can easily lead to misunderstanding or incomprehensibility. Perhaps, some readers do not understand why we emphasize word order from this example alone. A few more examples will clarify what happens when SMT is applied to Japanese-to-English translation. Even the most famous SMT service available on the web failed to translate the following very simple sentence at the time of writing this paper. Japanese: meari wa jon wo koroshita. Reference: Mary killed John. SMT output: John killed Mary. Since it cannot translate such a simple sentence, it obviously cannot translate more complex sentences correctly. Japanese: bobu ga katta hon wo jon wa yonda. Reference: John read a book that Bob bought. SMT output: Bob read the book John bought. Another example is: Japanese: bobu wa meari ni yubiwa wo kau tameni, jon no mise ni itta. Reference: Bob went to John?s store to buy a ring for Mary. SMT output: Bob Mary to buy the ring, John went to the store. In this way, this SMT service usually gives incomprehensible or misleading translations, and thus people prefer RBMT services. Other SMT systems also tend to make similar word order mistakes, and special care should be paid to the translation between distant language pairs such as Japanese and English. Even Japanese people cannot solve this word order problem easily: It is well known that Japanese people are not good at speaking English. From this point of view, conventional automatic evaluation metrics of translation quality disregard word order mistakes too much. Single-reference BLEU is defined by a geometrical mean of n-gram precisions pn and is modified by Brevity Penalty (BP) min(1, exp(1? r/h)), where r is the length of the reference and h is the length of the hypothesis. BLEU = BP? (p1p2p3p4) 1/4. Its range is [0, 1]. The BLEU score of (H0) with reference (R0) is 1.0?(11/11?9/10?6/9?4/8)1/4 = 0.740. Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order. Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs. Similarly, other popular scores such as NIST, PER, and TER (Snover et al, 2006) also give relatively good scores to this translation. NIST also considers only local word orders (n-grams). PER (Position-Independent Word Error Rate) was designed to disregard word order completely. TER (Snover et al, 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis. There are two popular rank correlation coefficients: Spearman?s ? and Kendall?s ? (Kendall, 1975). In Isozaki et al (2010), we used Kendall?s ? to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics. 945 It is not clear how well ? works as an automatic evaluation metric of translation quality. Moreover, Spearman?s ? might work better than Kendall?s ? . As we discuss later, ? considers only the direction of the rank change, whereas ? considers the distance of the change. The first objective of this paper is to examine which is the better metric for distant language pairs. The second objective is to find improvements of these rank correlation-metrics. Spearman?s ? is based on Pearson?s correlation coefficients. Suppose we have two lists of numbers x = [0.1, 0.4, 0.2, 0.6], y = [0.9, 0.6, 0.2, 0.7]. To obtain Pearson?s coefficients between x and y, we use the raw values in these lists. If we substitute their ranks for their raw values, we get x? = [1, 3, 2, 4] and y? = [4, 2, 1, 3]. Then, Spearman?s ? between x and y is given by Pearson?s coefficients between x? and y?. This ? can be rewritten as follows when there is no tie: ? = 1? ? i d 2 i n+1C3 . Here, di indicates the difference in the ranks of the i-th element. Rank distances are squared in this formula. Because of this square, we expect that ? decreases drastically when there is an element that significantly changes in rank. But we are also afraid that ? may be too severe for alternative good translations. Since Pearson?s correlation metric assumes linearity, nonlinear monotonic functions can change its score. On the other hand, Spearman?s ? and Kendall?s ? uses ranks instead of raw evaluation scores, and simple application of monotonic functions cannot change them (use of other operations such as averaging sentence scores can change them). 2 Methodology 2.1 Word alignment for rank correlations We have to determine word ranks to obtain rank correlation coefficients. Suppose we have: (R1) John hit Bob yesterday (H1) Bob hit John yesterday The 1st word ?John? in R1 becomes the 3rd word in H1. The 2nd word ?hit? in R1 becomes the 2nd word in H1. The 3rd word ?Bob? in R1 becomes the 1st word in H1. The 4th word ?yesterday? in R1 becomes the 4th word in H1. Thus, we get H1?s word order list [3, 2, 1, 4]. The number of all pairs of integers in this list is 4C2 = 6. It has three increasing pairs: (3,4), (2,4), and (1,4). Since Kendall?s ? is given by: ? = 2? the number of increasing pairs the number of all pairs ? 1, H1?s ? is 2? 3/6? 1 = 0.0. In this case, we can obtain Spearman?s ? as follows: ?John? moved by d1 = 2 words, ?hit? moved by d2 = 0 words, ?Bob? moved by d3 = 2 words, and ?yesterday? moved by d4 = 0 words. Therefore, H1?s ? is 1? (22 + 02 + 22 + 02)/5C3 = 0.2. Thus, ? considers only the direction of the movement, whereas ? considers the distance of the movement. Both ? and ? have the same range [?1, 1]. The main objective of this paper is to clarify which rank correlation is closer to human evaluation scores. We have to consider the limitation of the rank correlation metrics. They are defined only when there is one-to-one correspondence. However, a reference sentence and a hypothesis sentence may have different numbers of words. They may have two or more occurrences of the same word in one sentence. Sometimes, a word in the reference does not appear in the hypothesis, or a word in the hypothesis does not appear in the reference. Therefore, we cannot calculate ? and ? following the above definitions in general. Here, we determine the correspondence of words between hypotheses and references as follows. First, we find one-to-one corresponding words. That is, we find words that appear in both sentences and only once in each sentence. Suppose we have: (R2) the boy read the book (H2) the book was read by the boy By removing non-aligned words by one-to-one correspondence, we get: 946 (R3) boy read book (H3) book read boy Thus, we lost ?the.? We relax this one-to-one correspondence constraint by using one-to-one corresponding bigrams. (R2) and (H2) share ?the boy? and ?the book,? and we can align these instances of ?the? correctly. (R4) the1 boy2 read3 the4 book5 (H4) the4 book5 read3 the1 boy2 Now, we have five aligned words, and H4?s word order is represented by [4, 5, 3, 1, 2]. In returning to H0 and R0, we find that each of these sentences has eleven words. Almost all words are aligned by one-to-one correspondence but ?he? is not aligned because it appears twice in each sentence. By considering one-to-one corresponding bigrams (?he was? and ?he read?), ?he? is aligned as follows. (R5) he1 was2 interested3 in4 world5 history6 because7 he8 read9 the10 book11 (H5) he8 read9 the10 book11 because7 he1 was2 interested3 in4 world5 history6 H5?s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6]. The number of increasing pairs is: 4C2 = 6 pairs in [8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5, 6]. Then we obtain ? = 2 ? (6 + 15)/11C2 ? 1 = ?0.236. On the other hand, ? i d 2 i = 5 2 ? 6 + 22 + 72 ? 4 = 350, and we obtain ? = 1 ? 350/12C3 = ?0.591. Therefore, both Spearman?s ? and Kendall?s ? give very bad scores to the misleading translation H0. This fact implies they are much better metrics than BLEU, which gave a good score to it. ? is much lower than ? as we expected. In general, we can use higher-order n-grams for this alignment, but here we use only unigrams and bigrams for simplicity. This algnment algorithm is given in Figure 1. Since some hypothesis words do not have corresponding reference words, the output integer list worder is sometimes shorter than the evaluated sentence. Therefore, we should not use worder[i] ? i as di directly. We have to renumber the list by rank as we did in Section 1. Read a hypothesis sentence h = h1h2 . . . hm and its reference sentence r = r1r2 . . . rn. Initialize worder with an empty list. For each word hi in h: ? If hi appears only once each in h and r, append j s.t. rj = hi to worder. ? Otherwise, if the bigram hihi+1 appears only once each in h and r, append j s.t. rjrj+1 = hihi+1 to worder. ? Otherwise, if the bigram hi?1hi appears only once each in h and r, append j s.t. rj?1rj = hi?1hi to worder. Return worder. Figure 1: Word alignment algorithm for rank correlation 2.2 Word order metrics and meta-evaluation metrics These rank correlation metrics sometimes have negative values. In order to make them just like other automatic evaluation metrics, we normalize them as follows. ? Normalized Kendall?s ? : NKT = (? + 1)/2. ? Normalized Spearman?s ?: NSR = (?+ 1)/2. Accordingly, NKT is 0.382 and NSR is 0.205. These metrics are defined only when the number of aligned words is two or more. We define both NKT and NSR as zero when the number is one or less. Consequently, these normalized metrics have the same range [0, 1]. In order to avoid confusion, we use these abbreviations (NKT and NSR) when we use rank correlations as word order metrics, because these correlation metrics are also used in the machine translation community for meta-evaluation. For metaevaluation, we use Spearman?s ? and Pearson?s correlation coefficient and call them ?Spearman? and ?Pearson,? respectively. 2.3 Overestimation problem Since we measure the rank correlation of only corresponding words, these metrics will overestimate the correlation. For instance, a hypothesis sentence might have only two corresponding words among 947 0 0.2 0.4 0.6 0.8 1.0 0 0.2 0.4 0.6 0.8 1.0 BP (brevity penalty) no rm al iz ed av er ag e ad eq ua cy ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ?? ?? ? ? ? ?? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ?? ? ? ? ?? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ?? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ?? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0 0.2 0.4 0.6 0.8 1.0 0 0.2 0.4 0.6 0.8 1.0 P (precision) no rm al iz ed av er ag e ad eq ua cy ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ?? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ?? ? ??? ? ??? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ?? ? ? ? ?? ? ? ??? ? ? ?? ? ?? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ?? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ?? ? ? ? ?? ? ? ? ? ? Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right). (Each ? corresponds to one sentence generated by one MT system) dozens of words. In this case, these two words determine the score of the whole sentence. If the two words appear in their order in the reference, the whole sentence obtains the best score, NSR = NKT = 1.0, in spite of the fact that only two words matched. Solving this overestimation problem is the second objective of this paper. BLEU uses ?Brevity Penalty (BP)? (Section 1) to reduce the scores of too-short sentences. We can combine the above word order metrics with BP, e.g., NKT? BP and NSR? BP. However, we cannot very much expect from this solution because BP scores do not correlate with human judgments well. The left graph of Figure 2 shows a scatter plot of BP and ?normalized average adequacy.? This graph has 15 (systems) ? 100 (sentences) dots. Each dot (?) corresponds to one sentence from one translation system. In the NTCIR-7 data, three human judges gave five-point scores (1, 2, 3, 4, 5) for ?adequacy? and ?fluency? of each translated sentence. Although each system translated 1,381 sentences, only 100 sentences were evaluated by the judges. For each translated sentence, we averaged three judges? adequacy scores and normalized this average x by (x?1)/4. This is our ?normalized average adequacy,? and the dots appears only at multiples of 1/3? 1/4. This graph shows that BP has very little correlation with adequacy, and we cannot expect BP to improve the meta-evaluation performance very much. Perhaps, BP?s poor performance was caused by the fact that most MT systems output almost the same number of words, and if the number exceeds the length of the reference, BP=1.0 holds. Therefore, we have to consider other modifiers for this overestimation problem. We can use other common metrics such as precision, recall, and Fmeasure to reduce the overestimation of NSR and NKT. ? Precision: P = c/|h|, where c is the number of corresponding words and |h| is the number of words in the hypothesis sentence h. ? Recall: R = c/r, where |r| is the number of words in the reference sentence r. ? F-measure: F? = (1 + ?2)PR/(?2P + R), where ? is a parameter. In (R2)&(H2)?s case, precision is 5/7 = 0.714 and recall is 5/5 = 1.000. Which metric should we use? Our preliminary experiments with NTCIR-7 data showed that precision correlated best with adequacy among these three metrics (P , R, and F?=1). In addition, BLEU is essentially made for precision. Therefore, precision seems the most promising modifier. The right graph of Figure 2 shows a scatter plot of precision and normalized average adequacy. The graph shows that precision has more correlation with adequacy than BP. We can observe that sentences with very small P values usually obtain very low adequacy scores but those with mediocre P values often obtain good adequacy scores. 948 If we multiply P directly by NSR or NKT, those sentences with mediocre P values will lose too much of their scores. The use of ? x will mitigate this problem. Since ? P is closer to 1.0 than P itself, multiplication of ? P instead of P itself will save these sentences. If we apply ? x twice ( ?? P = 4 ? P ), it will further save them. Therefore, we expect? ? P and? 4 ? P to work better than ?P . Now, we propose two new metrics: NSRP? and NKTP?, where ? is a parameter (0 ? ? ? 1). 3 Experiments 3.1 Meta-evaluation with NTCIR-7 data In order to compare automatic translation evaluation methods, we use submissions to the NTCIR-7 Patent Translation (PATMT) task (Fujii et al, 2008). Fourteen MT systems participated in the JapaneseEnglish intrinsic evaluation. There were two RuleBased MT (RMBT) systems and one Examplebased MT (EBMT) system. All other systems were Statistical MT (SMT) systems. The task organizers provided a baseline SMT system. These 15 systems translated 1,381 Japanese sentences into English. The organizers evaluated these translations by using BLEU and human judgments. In the human judgements, three experts independently evaluated 100 selected sentences in terms of ?adequacy? and ?fluency.? For automatic evaluation, we used a single reference sentence for each of these 100 manually evaluated sentences. Echizen-ya et al (2009) used multireference data, but their data is not publicly available yet. For this meta-evaluation, we measured the corpus-level correlation between the human evaluation scores and the automatic evaluation scores. We simply averaged scores of 100 sentences for the proposed metrics. For existing metrics such as BLEU, we followed their definitions for corpus-level evaluation instead of simple averages of sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al, 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. edu/?snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al, 2007) that covers only European language pairs. Callison-Burch et al (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences. This data has different language pairs: Spanish, French, German ? English. We exclude CzechEnglish because there were so few systems (See the footnote of p. 146 in their paper). 4 Results 4.1 Meta-evaluation with NTCIR-7 data Table 1 shows the main results of this paper. The left part has corpus-level meta-evaluation with adequacy. Error metrics, WER, PER, and TER, have negative correlation coefficients, but we did not show their minus signs here. Both NSR-based metrics and NKT-based metrics perform better than conventional metrics for this NTCIR PATMT JE translation data. As we expected, ?BP and ?P (1/1) performed badly. Spearman of BP itself is zero. NKT performed slightly better than NSR. Perhaps, NSR penalized alternative good translations too much. However, one of the NSR-based metrics, NSRP 1/4, gave the best Spearman score of 0.947, and the difference between NSRP? and NKTP? was small. Modification with P led to this improvement. NKT gave the best Pearson score of 0.922. However, Pearson measures linearity and we can change its score through a nonlinear monotonic function without changing Spearman very much. For instance, (NSRP 1/4)1.5 also has Spearman of 0.947 but its Pearson is 0.931, which is better than NKT?s 0.922. Thus, we think Spearman is a better metaevaluation metric than Pearson. 949 Table 1: NTCIR-7 Meta-evaluation: correlation with human judgments (Spm = Spearman, Prs = Pearson) human judge Adequacy Fluency eval\\ meta-eval Spm Prs Spm Prs P 0.615 0.704 0.672 0.876 R 0.436 0.669 0.461 0.854 F?=1 0.525 0.692 0.543 0.871 BP 0.000 0.515 -0.007 0.742 NSR 0.904 0.906 0.869 0.910 NSRP 1/8 0.937 0.905 0.890 0.934 NSRP 1/4 0.947 0.900 0.901 0.944 NSRP 1/2 0.937 0.890 0.926 0.949 NSRP 1/1 0.883 0.872 0.883 0.939 NSR ? BP 0.851 0.874 0.769 0.910 NKT 0.940 0.922 0.887 0.931 NKTP 1/8 0.940 0.913 0.908 0.944 NKTP 1/4 0.940 0.904 0.908 0.949 NKTP 1/2 0.929 0.890 0.897 0.949 NKTP 1/1 0.897 0.869 0.879 0.936 NKT ? BP 0.829 0.878 0.726 0.918 ROUGE-L 0.903 0.874 0.889 0.932 ROUGE-S(4) 0.593 0.757 0.640 0.869 IMPACT 0.797 0.813 0.751 0.932 WER 0.894 0.822 0.836 0.926 TER 0.854 0.806 0.372 0.856 PER 0.375 0.642 0.393 0.842 METEOR(TERp) 0.490 0.708 0.508 0.878 GTM(-e 12) 0.618 0.723 0.601 0.850 NIST 0.343 0.661 0.372 0.856 BLEU 0.515 0.653 0.500 0.795 The right part of Table 1 shows correlation with fluency, but adequacy is more important, because our motivation is to provide a metric that is useful to reduce incomprehensible or misunderstanding outputs of MT systems. Again, the correlation-based metrics gave better scores than conventional metrics, and BP performed badly. NSR-based metrics proved to be as good as NKT-based metrics. Meta-evaluation scores of the de facto standard BLEU is much lower than those of other metrics. Echizen-ya et al (2009) reported that IMPACT performed very well for sentence-level evaluation of NTCIR-7 PATMT JE data. This corpus-level result also shows that IMPACT works better than BLEU, but ROUGE-L, WER, and our methods give better scores than IMPACT. Table 2: WMT-07 meta-evaluation: Each source language has two columns: the left one is News Corpus and the right one is Europarl. Spearman?s ? with human ?rank? source French Spanish German NSR 0.775 0.837 0.523 0.766 0.700 0.593 NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685 NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714 NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714 NKT 0.845 0.857 0.607 0.838 0.700 0.630 NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714 NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714 NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714 BLEU 0.786 0.679 0.750 0.595 0.400 0.821 WER 0.607 0.857 0.750 0.429 0.000 0.500 ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857 ROUGES 0.883 0.679 0.786 0.690 0.400 0.929 4.2 Meta-evaluation with WMT-07 data Callison-Burch et al (2007) have performed different human evaluation methods for different language pairs and different corpora. Their Table 5 shows inter-annotator agreements for the human evaluation methods. According to their table, the ?sentence ranking? (or ?rank?) method obtained better agreement than ?adequacy.? Therefore, we show Spearman?s ? for ?rank.? We used the scores given in their Tables 9, 10, and 11. (The ?constituent? methods obtained the best inter-annotator agreement, but these methods focus on local translation quality and have nothing to do with global word order, which we are discussing here.) Table 2 shows that our metrics designed for distant language pairs are comparable to conventional methods even for similar language pairs, but ROUGE-L and ROUGE-S performed better than ours for French News Corpus and German Europarl. BLEU scores in this table agree with those in Table 17 of Callison-Burch et al (2007) within rounding errors. After some experiments, we noticed that the use ofR instead of P often gives better scores for WMT07, but it degrades NTCIR-7 scores. We can extend our metric by F? , weighted harmonic mean of P and R, or any other interpolation, but the introduction of new parameters into our metric makes it difficult 950 to control. Improvement without new parameters is beyond the scope of this paper. 5 Discussion It has come to our attention that Birch et al (2010) has independently proposed an automatic evaluation method based on Kendall?s ? . First, they started with Kendall?s ? distance, which can be written as ?1?NKT? in our terminology, and then subtracted it from one. Thus, their metric is nothing but NKT. Then, they proposed application of the square root to get better Pearson by improving ?the sensitivity to small reorderings.? Since they used ?Kendall?s ?? and ?Kendall?s ? distance? interchangeably, it is not clear what they mean by ? ? Kendall?s ? ,? but perhaps they mean 1 ? ? 1?NKT because ? NKT is more insensitive to small reorderings. Table 3 shows the performance of these metrics for NTCIR-7 data. Pearson?s correlation coefficient with adequacy was improved by 1 ? ? 1? NKT, but other scores were degraded in this experiment. The difference between our method and Birch et al. (2010)?s method comes from the fact that we used Japanese-English translation data and Spearman?s correlation for meta-evaluation, whereas they used Chinese-English translation data and only Pearson?s correlation for meta-evaluation. Chinese word order is different from English, but Chinese is a Subject-Verb-Object (SVO) language and thus is much closer to English word order than Japanese, a typical SOV language. We preferred NSR because it penalizes global word order mistakes much more than does NKT, and as discussed above, global word order mistakes often lead to incomprehensibility and misunderstanding. On the other hand, they also tried Hamming distance, and summarized their experiments as follows: However, the Hamming distance seems to be more informative than Kendall?s tau for small amounts of reordering. This sentence and the introduction of the square root to NKT imply that Chinese word order is close to that of English, and they have to measure subtle word order mistakes. Table 3: NTCIR-7 meta-evaluation: Effects of square root (b(x) = 1? ? 1? x) NKT ? NKT b(NKT) Spearman w/ adequacy 0.940 0.940 0.922 Pearson w/ adequacy 0.922 0.817 0.941 Spearman w/ fluency 0.887 0.865 0.858 Pearson w/ fluency 0.931 0.917 0.833 In spite of these differences, the two groups independently recognized the usefulness of rank correlations for automatic evaluation of translation quality for distant language pairs. In their WMT-2010 paper (Birch and Osborne, 2010), they multiplied NKT with the brevity penalty and interpolated it with BLEU for the WMT-2010 shared task. This fact implies that incomprehensible or misleading word order mistakes are rare in translation among European languages. 6 Conclusions When Statistical Machine Translation is applied to distant language pairs such as Japanese and English, word order becomes an important problem. SMT systems often fail to find an appropriate translation because of a large search space. Therefore, they often output misleading or incomprehensible sentences such as ?A because B? vs. ?B because A.? To penalize such inadequate translations, we presented an automatic evaluation method based on rank correlation. There were two questions for this approach. First, which correlation coefficient should we use: Spearman?s ? or Kendall?s ?? Second, how should we solve the overestimation problem caused by the nature of one-to-one correspondence? We answered these questions through our experiments using the NTCIR-7 PATMT JE translation data. For the first question, ? was slightly better than ?, but ? was improved by precision. For the second question, it turned out that BLEU?s Brevity Penalty was counter-productive. A precision-based penalty gave a better solution. With this precisionbased penalty, both ? and ? worked well and they outperformed conventional methods for NTCIR-7 data. For similar language pairs, our method was comparable to conventional evaluation methods. Fu951 ture work includes extension of the method so that it can outperform conventional methods even for similar language pairs. References Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgements. In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and Summarization, pages 65?72. Alexandra Birch and Miles Osborne. 2010. LRscore for evaluating lexical and reordering quality in MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 327? 332. Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, 24(1):15?26. Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluatiing the role of Bleu in machine translation research. In Proc. of the Conference of the European Chapter of the Association for Computational Linguistics, pages 249?256. Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Chrstof Monz, and Josh Schroeder. 2007. (Meta-)Evaluation of machine translation. In Proc. of the Workshop on Machine Translation (WMT), pages 136?158. Etienne Denoual and Yves Lepage. 2005. BLEU in characters: towards automatic MT evaluation in languages without word delimiters. In Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing, pages 81?86. Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum. In Proceedings of MT Summit XII Workshop on Patent Translation, pages 151?158. Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata, Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, and Noriko Kando. 2009. Metaevaluation of automatic evaluation methods for machine translation using patent translation data in ntcir7. In Proceedings of the 3rd Workshop on Patent Translation, pages 9?16. Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Working Notes of the NTCIR Workshop Meeting (NTCIR), pages 389?400. Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010. Head Finalization: A simple reordering rule for SOV languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 250?257. Maurice G. Kendall. 1975. Rank Correlation Methods. Charles Griffin. Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proc. of the North American Chapter of the Association of Computational Linguistics (NAACL), pages 71?78. Dan Melamed, Ryan Green, and Joseph P. Turian. 2007. Precision and recall of machine translation. In Proc. of NAACL-HLT, pages 61?63. Kishore Papineni, Salim Roukos, Todd Ward, John Henderson, and Florence Reeder. 2002a. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish Results. In Proc. of the International Conference on Human Language Technology Research (HLT), pages 132?136. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002b. BLEU: a method for automatic evaluation of machine translation. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 311?318. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas. 952 "}
{"date": 2002, "text": "An Integrated Architecture for Shallow and Deep Processing Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller, Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu, Markus Becker and Hans-Ulrich Krieger DFKI GmbH Stuhlsatzenhausweg 3 Saarbru?cken, Germany whiteboard@dfki.de Abstract We present an architecture for the integration of shallow and deep NLP components which is aimed at flexible combination of different language technologies for a range of practical current and future applications. In particular, we describe the integration of a high-level HPSG parsing system with different high-performance shallow components, ranging from named entity recognition to chunk parsing and shallow clause recognition. The NLP components enrich a representation of natural language text with layers of new XML meta-information using a single shared data structure, called the text chart. We describe details of the integration methods, and show how information extraction and language checking applications for realworld German text benefit from a deep grammatical analysis. 1 Introduction Over the last ten years or so, the trend in applicationoriented natural language processing (e.g., in the area of term, information, and answer extraction) has been to argue that for many purposes, shallow natural language processing (SNLP) of texts can provide sufficient information for highly accurate and useful tasks to be carried out. Since the emergence of shallow techniques and the proof of their utility, the focus has been to exploit these technologies to the maximum, often ignoring certain complex issues, e.g. those which are typically well handled by deep NLP systems. Up to now, deep natural language processing (DNLP) has not played a significant role in the area of industrial NLP applications, since this technology often suffers from insufficient robustness and throughput, when confronted with large quantities of unrestricted text. Current information extractions (IE) systems therefore do not attempt an exhaustive DNLP analysis of all aspects of a text, but rather try to analyse or ?understand? only those text passages that contain relevant information, thereby warranting speed and robustness wrt. unrestricted NL text. What exactly counts as relevant is explicitly defined by means of highly detailed domain-specific lexical entries and/or rules, which perform the required mappings from NL utterances to corresponding domain knowledge. However, this ?fine-tuning? wrt. a particular application appears to be the major obstacle when adapting a given shallow IE system to another domain or when dealing with the extraction of complex ?scenario-based? relational structures. In fact, (Appelt and Israel, 1997) have shown that the current IE technology seems to have an upper performance level of less than 60% in such cases. It seems reasonable to assume that if a more accurate analysis of structural linguistic relationships could be provided (e.g., grammatical functions, referential relationships), this barrier might be overcome. Actually, the growing market needs in the wide area of intelligent information management systems seem to request such a break-through. In this paper we will argue that the quality of curComputational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448. Proceedings of the 40th Annual Meeting of the Association for rent SNLP-based applications can be improved by integrating DNLP on demand in a focussed manner, and we will present a system that combines the finegrained anaysis provided by HPSG parsing with a high-performance SNLP system into a generic and flexible NLP architecture. 1.1 Integration Scenarios Owing to the fact that deep and shallow technologies are complementary in nature, integration is a nontrivial task: while SNLP shows its strength in the areas of efficiency and robustness, these aspects are problematic for DNLP systems. On the other hand, DNLP can deliver highly precise and fine-grained linguistic analyses. The challenge for integration is to combine these two paradigms according to their virtues. Probably the most straightforward way to integrate the two is an architecture in which shallow and deep components run in parallel, using the results of DNLP, whenever available. While this kind of approach is certainly feasible for a real-time application such as Verbmobil, it is not ideal for processing large quantities of text: due to the difference in processing speed, shallow and deep NLP soon run out of sync. To compensate, one can imagine two possible remedies: either to optimize for precision, or for speed. The drawback of the former strategy is that the overall speed will equal the speed of the slowest component, whereas in case of the latter, DNLP will almost always time out, such that overall precision will hardly be distinguishable from a shallowonly system. What is thus called for is an integrated, flexible architecture where components can play at their strengths. Partial analyses from SNLP can be used to identify relevant candidates for the focussed use of DNLP, based on task or domain-specific criteria. Furthermore, such an integrated approach opens up the possibility to address the issue of robustness by using shallow analyses (e.g., term recognition) to increase the coverage of the deep parser, thereby avoiding a duplication of efforts. Likewise, integration at the phrasal level can be used to guide the deep parser towards the most likely syntactic analysis, leading, as it is hoped, to a considerable speedup. shallow NLP components NLP deep components internal repr. layer multi chart annot. XML external repr. generic OOP component interface WHAM application specification input and result Figure 1: The WHITEBOARD architecture. 2 Architecture The WHITEBOARD architecture defines a platform that integrates the different NLP components by enriching an input document through XML annotations. XML is used as a uniform way of representing and keeping all results of the various processing components and to support a transparent software infrastructure for LT-based applications. It is known that interesting linguistic information ?especially when considering DNLP? cannot efficiently be represented within the basic XML markup framework (?typed parentheses structure?), e.g., linguistic phenomena like coreferences, ambiguous readings, and discontinuous constituents. The WHITEBOARD architecture employs a distributed multi-level representation of different annotations. Instead of translating all complex structures into one XML document, they are stored in different annotation layers (possibly non-XML, e.g. feature structures). Hyperlinks and ?span? information together support efficient access between layers. Linguistic information of common interest (e.g. constituent structure extracted from HPSG feature structures) is available in XML format with hyperlinks to full feature structure representations externally stored in corresponding data files. Fig. 1 gives an overview of the architecture of the WHITEBOARD Annotation Machine (WHAM). Applications feed the WHAM with input texts and a specification describing the components and configuration options requested. The core WHAM engine has an XML markup storage (external ?offline? representation), and an internal ?online? multi-level annotation chart (index-sequential access). Following the trichotomy of NLP data representation models in (Cunningham et al, 1997), the XML markup contains additive information, while the multi-level chart contains positional and abstraction-based information, e.g., feature structures representing NLP entities in a uniform, linguistically motivated form. Applications and the integrated components access the WHAM results through an object-oriented programming (OOP) interface which is designed as general as possible in order to abstract from component-specific details (but preserving shallow and deep paradigms). The interfaces of the actually integrated components form subclasses of the generic interface. New components can be integrated by implementing this interface and specifying DTDs and/or transformation rules for the chart. The OOP interface consists of iterators that walk through the different annotation levels (e.g., token spans, sentences), reference and seek operators that allow to switch to corresponding annotations on a different level (e.g., give all tokens of the current sentence, or move to next named entity starting from a given token position), and accessor methods that return the linguistic information contained in the chart. Similarily, general methods support navigating the type system and feature structures of the DNLP components. The resulting output of the WHAM can be accessed via the OOP interface or as XML markup. The WHAM interface operations are not only used to implement NLP component-based applications, but also for the integration of deep and shallow processing components itself. 2.1 Components 2.1.1 Shallow NL component Shallow analysis is performed by SPPC, a rulebased system which consists of a cascade of weighted finite?state components responsible for performing subsequent steps of the linguistic analysis, including: fine-grained tokenization, lexicomorphological analysis, part-of-speech filtering, named entity (NE) recognition, sentence boundary detection, chunk and subclause recognition, see (Piskorski and Neumann, 2000; Neumann and Piskorski, 2002) for details. SPPC is capable of processing vast amounts of textual data robustly and efficiently (ca. 30,000 words per second in standard PC environment). We will briefly describe the SPPC components which are currently integrated with the deep components. Each token identified by a tokenizer as a potential word form is morphologically analyzed. For each token, its lexical information (list of valid readings including stem, part-of-speech and inflection information) is computed using a fullform lexicon of about 700,000 entries that has been compiled out from a stem lexicon of about 120,000 lemmas. After morphological processing, POS disambiguation rules are applied which compute a preferred reading for each token, while the deep components can back off to all readings. NE recognition is based on simple pattern matching techniques. Proper names (organizations, persons, locations), temporal expressions and quantities can be recognized with an average precision of almost 96% and recall of 85%. Furthermore, a NE?specific reference resolution is performed through the use of a dynamic lexicon which stores abbreviated variants of previously recognized named entities. Finally, the system splits the text into sentences by applying only few, but highly accurate contextual rules for filtering implausible punctuation signs. These rules benefit directly from NE recognition which already performs restricted punctuation disambiguation. 2.1.2 Deep NL component The HPSG Grammar is based on a large?scale grammar for German (Mu?ller, 1999), which was further developed in the VERBMOBIL project for translation of spoken language (Mu?ller and Kasper, 2000). After VERBMOBIL the grammar was adapted to the requirements of the LKB/PET system (Copestake, 1999), and to written text, i.e., extended with constructions like free relative clauses that were irrelevant in the VERBMOBIL scenario. The grammar consists of a rich hierarchy of 5,069 lexical and phrasal types. The core grammar contains 23 rule schemata, 7 special verb movement rules, and 17 domain specific rules. All rule schemata are unary or binary branching. The lexicon contains 38,549 stem entries, from which more than 70% were semi-automatically acquired from the annotated NEGRA corpus (Brants et al, 1999). The grammar parses full sentences, but also other kinds of maximal projections. In cases where no full analysis of the input can be provided, analyses of fragments are handed over to subsequent modules. Such fragments consist of maximal projections or single words. The HPSG analysis system currently integrated in the WHITEBOARD system is PET (Callmeier, 2000). Initially, PET was built to experiment with different techniques and strategies to process unification-based grammars. The resulting system provides efficient implementations of the best known techniques for unification and parsing. As an experimental system, the original design lacked open interfaces for flexible integration with external components. For instance, in the beginning of the WHITEBOARD project the system only accepted fullform lexica and string input. In collaboration with Ulrich Callmeier the system was extended. Instead of single word input, input items can now be complex, overlapping and ambiguous, i.e. essentially word graphs. We added dynamic creation of atomic type symbols, e.g., to be able to add arbitrary symbols to feature structures. With these enhancements, it is possible to build flexible interfaces to external components like morphology, tokenization, named entity recognition, etc. 3 Integration Morphology and POS The coupling between the morphology delivered by SPPC and the input needed for the German HPSG was easily established. The morphological classes of German are mapped onto HPSG types which expand to small feature structures representing the morphological information in a compact way. A mapping to the output of SPPC was automatically created by identifying the corresponding output classes. Currently, POS tagging is used in two ways. First, lexicon entries that are marked as preferred by the shallow component are assigned higher priority than the rest. Thus, the probability of finding the correct reading early should increase without excluding any reading. Second, if for an input item no entry is found in the HPSG lexicon, we automatically create a default entry, based on the part?of?speech of the preferred reading. This increases robustness, while avoiding increase in ambiguity. Named Entity Recognition Writing HPSG grammars for the whole range of NE expressions etc. is a tedious and not very promising task. They typically vary across text sorts and domains, and would require modularized subgrammars that can be easily exchanged without interfering with the general core. This can only be realized by using a type interface where a class of named entities is encoded by a general HPSG type which expands to a feature structure used in parsing. We exploit such a type interface for coupling shallow and deep processing. The classes of named entities delivered by shallow processing are mapped to HPSG types. However, some finetuning is required whenever deep and shallow processing differ in the amount of input material they assign to a named entity. An alternative strategy is used for complex syntactic phrases containing NEs, e.g., PPs describing time spans etc. It is based on ideas from Explanation?based Learning (EBL, see (Tadepalli and Natarajan, 1996)) for natural language analysis, where analysis trees are retrieved on the basis of the surface string. In our case, the part-of-speech sequence of NEs recognised by shallow analysis is used to retrieve pre-built feature structures. These structures are produced by extracting NEs from a corpus and processing them directly by the deep component. If a correct analysis is delivered, the lexical parts of the analysis, which are specific for the input item, are deleted. We obtain a sceletal analysis which is underspecified with respect to the concrete input items. The part-of-speech sequence of the original input forms the access key for this structure. In the application phase, the underspecified feature structure is retrieved and the empty slots for the input items are filled on the basis of the concrete input. The advantage of this approach lies in the more elaborate semantics of the resulting feature structures for DNLP, while avoiding the necessity of adding each and every single name to the HPSG lexicon. Instead, good coverage and high precision can be achieved using prototypical entries. Lexical Semantics When first applying the original VERBMOBIL HPSG grammar to business news articles, the result was that 78.49% of the missing lexical items were nouns (ignoring NEs). In the integrated system, unknown nouns and NEs can be recognized by SPPC, which determines morphosyntactic information. It is essential for the deep system to associate nouns with their semantic sorts both for semantics construction, and for providing semantically based selectional restrictions to help constraining the search space during deep parsing. GermaNet (Hamp and Feldweg, 1997) is a large lexical database, where words are associated with POS information and semantic sorts, which are organized in a fine-grained hierarchy. The HPSG lexicon, on the other hand, is comparatively small and has a more coarse-grained semantic classification. To provide the missing sort information when recovering unknown noun entries via SPPC, a mapping from the GermaNet semantic classification to the HPSG semantic classification (Siegel et al, 2001) is applied which has been automatically acquired. The training material for this learning process are those words that are both annotated with semantic sorts in the HPSG lexicon and with synsets of GermaNet. The learning algorithm computes a mapping relevance measure for associating semantic concepts in GermaNet with semantic sorts in the HPSG lexicon. For evaluation, we examined a corpus of 4664 nouns extracted from business news that were not contained in the HPSG lexicon. 2312 of these were known in GermaNet, where they are assigned 2811 senses. With the learned mapping, the GermaNet senses were automatically mapped to HPSG semantic sorts. The evaluation of the mapping accuracy yields promising results: In 76.52% of the cases the computed sort with the highest relevance probability was correct. In the remaining 20.70% of the cases, the correct sort was among the first three sorts. 3.1 Integration on Phrasal Level In the previous paragraphs we described strategies for integration of shallow and deep processing where the focus is on improving DNLP in the domain of lexical and sub-phrasal coverage. We can conceive of more advanced strategies for the integration of shallow and deep analysis at the length cover- complete LP LR 0CB \u0000 2CB age match \u0000 40 100 80.4 93.4 92.9 92.1 98.9 \u0001 40 99.8 78.6 92.4 92.2 90.7 98.5 Training: 16,000 NEGRA sentences Testing: 1,058 NEGRA sentences Figure 2: Stochastic topological parsing: results level of phrasal syntax by guiding the deep syntactic parser towards a partial pre-partitioning of complex sentences provided by shallow analysis systems. This strategy can reduce the search space, and enhance parsing efficiency of DNLP. Stochastic Topological Parsing The traditional syntactic model of topological fields divides basic clauses into distinct fields: so-called pre-, middleand post-fields, delimited by verbal or sentential markers. This topological model of German clause structure is underspecified or partial as to non-sentential constituent boundaries, but provides a linguistically well-motivated, and theory-neutral macrostructure for complex sentences. Due to its linguistic underpinning the topological model provides a pre-partitioning of complex sentences that is (i) highly compatible with deep syntactic structures and (ii) maximally effective to increase parsing efficiency. At the same time (iii) partiality regarding the constituency of non-sentential material ensures the important aspects of robustness, coverage, and processing efficiency. In (Becker and Frank, 2002) we present a corpusdriven stochastic topological parser for German, based on a topological restructuring of the NEGRA corpus (Brants et al, 1999). For topological treebank conversion we build on methods and results in (Frank, 2001). The stochastic topological parser follows the probabilistic model of non-lexicalised PCFGs (Charniak, 1996). Due to abstraction from constituency decisions at the sub-sentential level, and the essentially POS-driven nature of topological structure, this rather simple probabilistic model yields surprisingly high figures of accuracy and coverage (see Fig.2 and (Becker and Frank, 2002) for more detail), while context-free parsing guarantees efficient processing. The next step is to elaborate a (partial) mapping of shallow topological and deep syntactic structures that is maximally effective for preference-guiTopological Structure: CL-V2 VF-TOPIC LK-FIN MF RK-t NN VVFIN ADV NN PREP NN VVFIN [ \u0002\u0004\u0003\u0006\u0005\u0006\u0007\t\b [ \u0007 \u0005 \u0006\u000e\t\u000f\u0011\u0010\u0012\u0002 Peter] [ \u0003\u0014\u0013\u0015\u0005\u0011 \u0011\u0010\u0017\u0016 i?t] [ \u0018\u0019 gerne Wu?rstchen mit Kartoffelsalat] [ \u001a\u0011\u0013\u0015\u0005 ff -]] Peter eats happily sausages with potato salad Deep Syntactic Structure: [ \u0002fi\u000f [ \u0016fl\u000f Peter] [ \u0002\u0011ffi [ \u0007 i?t] [ \u0007\t\u000f gerne [ \u0007\u001f\u000f [ \u0016fl\u000f Wu?rstchen [ \u000ffi\u000f mit [ \u0016fl\u000f Kartoffelsalat]]] [ \u0007 \u0005\u0006ff -]]]]] Mapping: CL-V2 ! CP, VF-TOPIC ! XP, LK-FIN ! V, \" LK-FIN MF RK-t #\u001f! C?, \" MF RK-t #fi! VP, RK-t ! V-t Figure 3: Matching topological and deep syntactic structures ded deep syntactic analysis, and thus, efficiency improvements in deep syntactic processing. Such a mapping is illustrated for a verb-second clause in Fig.3, where matching constituents of topological and deep-syntactic phrase structure are indicated by circled nodes. With this mapping defined for all sentence types, we can proceed to the technical aspects of integration into the WHITEBOARD architecture and XML text chart, as well as preference-driven HPSG analysis in the PET system. 4 Experiments An evaluation has been started using the NEGRA corpus, which contains about 20,000 newspaper sentences. The main objectives are to evaluate the syntactic coverage of the German HPSG on newspaper text and the benefits of integrating deep and shallow analysis. The sentences of the corpus were used in their original form without stripping, e.g. parenthesized insertions. We extended the HPSG lexicon semiautomatically from about 10,000 to 35,000 stems, which roughly corresponds to 350,000 full forms. Then, we checked the lexical coverage of the deep system on the whole corpus, which resulted in 28.6% of the sentences being fully lexically analyzed. The corresponding experiment with the integrated system yielded an improved lexical coverage of 71.4%, due to the techniques described in section 3. This increase is not achieved by manual extension, but only through synergy between the deep and shallow components. To test the syntactic coverage, we processed the subset of the corpus that was fully covered lexically (5878 sentences) with deep analysis only. The results are shown in table 4 in the second column. In order to evaluate the integrated system we processed 20,568 sentences from the corpus without further extension of the HPSG lexicon (see table 4, third column). Deep Integrated # sentences 20,568 avg. sentence length 16.83 avg. lexical ambiguity 2.38 1.98 avg. # analyses 16.19 18.53 analysed sentences 2,569 4,546 lexical coverage 28.6% 71.4% overall coverage 12.5% 22.1% Figure 4: Evaluation of German HPSG About 10% of the sentences that were successfully parsed by deep analysis only could not be parsed by the integrated system, and the number of analyses per sentence dropped from 16.2% to 8.6%, which indicates a problem in the morphology interface of the integrated system. We expect better overall results once this problem is removed. 5 Applications Since typed feature structures (TFS) in Whiteboard serve as both a representation and an interchange format, we developed a Java package (JTFS) that implements the data structures, together with the necessary operations. These include a lazy-copying unifier, a subsumption and equivalence test, deep copying, iterators, etc. JTFS supports a dynamic construction of typed feature structures, which is important for information extraction. 5.1 Information Extraction Information extraction in Whiteboard benefits both from the integration of the shallow and deep analysis results and from their processing methods. We chose management succession as our application domain. Two sets of template filling rules are defined: pattern-based and unification-based rules. The pattern-based rules work directly on the output delivered by the shallow analysis, for example, (1) Nachfolger von 1 $fl%'&\u0011(*)*+ +-,\u0004./%1032 4 person out 1 5 . This rule matches expressions like Nachfolger von Helmut Kohl (successor of) which contains two string tokens Nachfolger and von followed by a person name, and fills the slot of person outwith the recognized person name Helmut Kohl. The patternbased grammar yields good results by recognition of local relationships as in (1). The unificationbased rules are applied to the deep analysis results. Given the fine-grained syntactic and semantic analysis of the HPSG grammar and its robustness (through SNLP integration), we decided to use the semantic representation (MRS, see (Copestake et al, 2001)) as additional input for IE. The reason is that MRSs express precise relationships between the chunks, in particular, in constructions involving (combinations of) free word order, long distance dependencies, control and raising, or passive, which are very difficult, if not impossible, to recognize for a pattern-based grammar. E.g., the short sentence (2) illustrates a combination of free word order, control, and passive. The subject of the passive verb wurde gebeten is located in the middle field and is at the same time the subject of the infinitive verb zu u?bernehmen. A deep (HPSG) analysis can recognize the dependencies quite easily, whereas a pattern based grammar cannot determine, e.g., for which verb Peter Miscke or Dietmar Hopp is the subject. (2) Peter Miscke following was Dietmar Hopp asked, the development sector to take over. Peter Entwicklungsabteilung Miscke zu zufolge u?bernehmen. wurde Dietmar Hopp gebeten, die ? According to Peter Miscke, Dietmar Hopp was asked to take over the development sector.? We employ typed feature structures (TFS) as our modelling language for the definition of scenario template types and template element types. Therefore, the template filling results from shallow and deep analysis can be uniformly encoded in TFS. As a side effect, we can easily adapt JTFS unification for the template merging task, by interperting the partially filled templates from deep and shallow analysis as constraints. E.g., to extract the relevant information from the above sentence, the following unification-based rule can be applied: 67 7 7 7 8 PERSON IN \b DIVISION 9 MRS 6 8 PRED ?u?bernehmen? AGENT \b THEME 9 :; :=< < < < ; 5.2 Language checking Another area where DNLP can support existing shallow-only tools is grammar and controlled language checking. Due to the scarce distribution of true errors (Becker et al, to appear), there is a high a priori probability for false alarms. As the number of false alarms decides on user-acceptance, precision is of utmost importance and cannot easily be traded for recall. Current controlled language checking systems for German, such as MULTILINT (http://www.iai.uni-sb.de/en/multien.html) or FLAG (http://flag.dfki.de), build exclusively on SNLP: while checking of local errors (e.g. NP-internal agreement, prepositional case) can be performed quite reliably by such a system, error types involving non-local dependencies, or access to grammatical functions are much harder to detect. The use of DNLP in this area is confronted with several systematic problems: first, formal grammars are not always available, e.g., in the case of controlled languages; second, erroneous sentences lie outside the language defined by the competence grammar, and third, due to the sparse distribution of errors, a DNLP system will spend most of the time parsing perfectly wellformed sentences. Using an integrated approach, a shallow checker can be used to cheaply identify initial error candidates, while false alarms can be eliminated based on the richer annotations provided by the deep parser. 6 Discussion In this paper we reported on an implemented system called WHITEBOARD which integrates different shallow components with a HPSG?based deep system. The integration is realized through the metaphor of textual annotation. To best of our knowledge, this is the first implemented system which integrates high-performance shallow processing with an advanced deep HPSG?based analysis system. There exists only very little other work that considers integration of shallow and deep NLP using an XML?based architecture, most notably (Grover and Lascarides, 2001). However, their integration efforts are largly limited to the level of POS tag information. Acknowledgements This work was supported by a research grant from the German Federal Ministry of Education, Science, Research and Technology (BMBF) to the DFKI project WHITEBOARD, FKZ: 01 IW 002. Special thanks to Ulrich Callmeier for his technical support concerning the integration of PET. References D. Appelt and D. Israel. 1997. Building information extraction systems. Tutorial during the 5th ANLP, Washington. M. Becker and A. Frank. 2002. A Stochastic Topological Parser of German. In Proceedings of COLING 2002, Teipei, Taiwan. M. Becker, A. Bredenkamp, B. Crysmann, and J. Klein. to appear. Annotation of error types for german newsgroup corpus. In Anne Abeille?, editor, Treebanks: Building and Using Syntactically Annotated Corpora. Kluwer, Dordrecht. T. Brants, W. Skut, and H. Uszkoreit. 1999. Syntactic Annotation of a German newspaper corpus. In Proceedings of the ATALA Treebank Workshop, pages 69? 76, Paris, France. U. Callmeier. 2000. PET ? A platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering, 6 (1) (Special Issue on Efficient Processing with HPSG):99 ? 108. E. Charniak. 1996. Tree-bank Grammars. In AAAI-96. Proceedings of the 13th AAAI, pages 1031?1036. MIT Press. A. Copestake, A. Lascarides, and D. Flickinger. 2001. An algebra for semantic construction in constraintbased grammars. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL 2001), Toulouse, France. A. Copestake. 1999. The (new) LKB system. ftp://www-csli.stanford.edu/> aac/newdoc.pdf. H. Cunningham, K. Humphreys, R. Gaizauskas, and Y. Wilks. 1997. Software Infrastructure for Natural Language Processing. In Proceedings of the Fifth ANLP, March. A. Frank. 2001. Treebank Conversion. Converting the NEGRA Corpus to an LTAG Grammar. In Proceedings of the EUROLAN Workshop on Multi-layer Corpus-based Analysis, pages 29?43, Iasi, Romania. C. Grover and A. Lascarides. 2001. XML-based data preparation for robust deep parsing. In Proceedings of the 39th ACL, pages 252?259, Toulouse, France. B. Hamp and H. Feldweg. 1997. Germanet - a lexicalsemantic net for german. In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, Madrid. S. Mu?ller and W. Kasper. 2000. HPSG analysis of German. In W. Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation, Artificial Intelligence, pages 238?253. Springer-Verlag, Berlin Heidelberg New York. S. Mu?ller. 1999. Deutsche Syntax deklarativ. HeadDriven Phrase Structure Grammar fu?r das Deutsche. Max Niemeyer Verlag, Tu?bingen. G. Neumann and J. Piskorski. 2002. A shallow text processing core engine. Computational Intelligence, to appear. J. Piskorski and G. Neumann. 2000. An intelligent text extraction and navigation system. In Proceedings of the RIAO-2000. Paris, April. M. Siegel, F. Xu, and G. Neumann. 2001. Customizing germanet for the use in deep linguistic processing. In Proceedings of the NAACL 2001 Workshop WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburgh,USA, July. P. Tadepalli and B. Natarajan. 1996. A formal framework for speedup learning from problems and solutions. Journal of AI Research, 4:445 ? 475. "}
{"date": 2010, "text": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288?1297, Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics Phylogenetic Grammar Induction Taylor Berg-Kirkpatrick and Dan Klein Computer Science Division University of California, Berkeley {tberg, klein}@cs.berkeley.edu Abstract We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%. 1 Introduction Learning multiple languages together should be easier than learning them separately. For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages? parses of the same bitext (Kuhn, 2004; Burkett and Klein, 2008; Kuzman et al, 2009; Smith and Eisner, 2009; Snyder et al, 2009a). Moreover, Snyder et al (2009b) in the context of unsupervised part-of-speech induction (and Bouchard-Co?te? et al (2007) in the context of phonology) show that extending beyond two languages can provide increasing benefit. However, multitexts are only available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter level, using a phylogeny-structured prior to tie together the various individual languages? learning problems. Our joint, hierarchical prior couples model parameters for different languages in a way that respects knowledge about how the languages evolved. Aspects of this work are closely related to Cohen and Smith (2009) and Bouchard-Co?te? et al (2007). Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. In their work, structurally constrained covariance in a logistic normal prior is used to couple parameters between the two languages. Our work, though also different in technical approach, differs most centrally in the extension to multiple languages and the use of a phylogeny. Bouchard-Co?te? et al (2007) considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure as well as the use of log-linear parameterization of local model components. Our work differs from theirs primarily in the task (syntax vs. phonology) and the variables governed by the phylogeny: in our model it is the grammar parameters that drift (in the prior) rather than individual word forms (in the likelihood model). Specifically, we consider dependency induction in the DMV model of Klein and Manning (2004). Our data is a collection of standard dependency data sets in eight languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. Our focus is not the DMV model itself, which is well-studied, but rather the prior which couples the various languages? parameters. While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. In our experiments, joint multilingual learning substantially outperforms independent monolingual learning. Using a limited phylogeny that 1288 only couples languages within linguistic families reduces error by 5.6% over the monolingual baseline. Using a flat, global phylogeny gives a greater reduction, almost 10%. Finally, a more articulated phylogeny that captures both inter- and intrafamily effects gives an even larger average relative error reduction of 21.1%. 2 Model We define our model over two kinds of random variables: dependency trees and parameters. For each language ? in a set L, our model will generate a collection t? of dependency trees ti?. We assume that these dependency trees are generated by the DMV model of Klein and Manning (2004), which we write as ti? ? DMV(??). Here, ?? is a vector of the various model parameters for language ?. The prior is what couples the ?? parameter vectors across languages; it is the focus of this work. We first consider the likelihood model before moving on to the prior. 2.1 Dependency Model with Valence A dependency parse is a directed tree t over tokens in a sentence s. Each edge of the tree specifies a directed dependency from a head token to a dependent, or argument token. The DMV is a generative model for trees t, which has been widely used for dependency parse induction. The observed data likelihood, used for parameter estimation, is the marginal probability of generating the observed sentences s, which are simply the leaves of the trees t. Generation in the DMV model involves two types of local conditional probabilities: CONTINUE distributions that capture valence and ATTACH distributions that capture argument selection. First, the Bernoulli CONTINUE probability distributions P CONTINUE(c|h, dir, adj; ??) model the fertility of a particular head type h. The outcome c ? {stop, continue} is conditioned on the head type h, direction dir, and adjacency adj. If a head type?s continue probability is low, tokens of this type will tend to generate few arguments. Second, the ATTACH multinomial probability distributions P ATTACH(a|h, dir; ??) capture attachment preferences of heads, where a and h are both token types. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold part-of-speech labels as tokens. Thus, the basic observed ?word? types are English Dutch SwedishDanish Spanish Portuguese Slovene Chinese Global IndoEuropean Germanic West Germanic North Germanic IberoRomance Italic Balto-Slavic Slavic SinoTibetan Sinitic Figure 1: An example of a linguistically-plausible phylogenetic tree over the languages in our training data. Leaves correspond to (observed) modern languages, while internal nodes represent (unobserved) ancestral languages. actually word classes. 2.1.1 Log-Linear Parameterization The DMV?s local conditional distributions were originally given as simple multinomial distributions with one parameter per outcome. However, they can be re-parameterized to give the following log-linear form (Eisner, 2002; Bouchard-Co?te? et al., 2007; Berg-Kirkpatrick et al, 2010): P CONTINUE(c|h, dir, adj; ??) = exp ? ??T f CONTINUE(c, h, dir, adj) ? P c? exp ? ??T f CONTINUE(c?, h, dir, adj) ? P ATTACH(a|h, dir; ??) = exp ? ??T f ATTACH(a, h, dir) ? P a? exp ? ??T f ATTACH(a?, h, dir) ? The parameters are weights ?? with one weight vector per language. In the case where the vector of feature functions f has an indicator for each possible conjunction of outcome and conditions, the original multinomial distributions are recovered. We refer to these full indicator features as the set of SPECIFIC features. 2.2 Phylogenetic Prior The focus of this work is coupling each of the parameters ?? in a phylogeny-structured prior. Consider a phylogeny like the one shown in Figure 1, where each modern language ? in L is a leaf. We would like to say that the leaves? parameter vectors arise from a process which slowly 1289 drifts along each branch. A convenient choice is to posit additional parameter variables ??+ at internal nodes ?+ ? L+, a set of ancestral languages, and to assume that the conditional distribution P (??|?par(?)) at each branch in the phylogeny is a Gaussian centered on ?par(?), where par(?) is the parent of ? in the phylogeny and ? ranges over L ? L+. The variance structure of the Gaussian would then determine how much drift (and in what directions) is expected. Concretely, we assume that each drift distribution is an isotropic Gaussian with mean ?par(?) and scalar variance ?2. The root is centered at zero. We have thus defined a joint distribution P (?|?2) where ? = (?? : ? ? L?L+). ?2 is a hyperparameter for this prior which could itself be re-parameterized to depend on branch length or be learned; we simply set it to a plausible constant value. Two primary challenges remain. First, inference under arbitrary priors can become complex. However, in the simple case of our diagonal covariance Gaussians, the gradient of the observed data likelihood can be computed directly using the DMV?s expected counts and maximum-likelihood estimation can be accomplished by applying standard gradient optimization methods. Second, while the choice of diagonal covariance is efficient, it causes components of ? that correspond to features occurring in only one language to be marginally independent of the parameters of all other languages. In other words, only features which fire in more than one language are coupled by the prior. In the next section, we therefore increase the overlap between languages? features by using coarse projections of parts-of-speech. 2.3 Projected Features With diagonal covariance in the Gaussian drift terms, each parameter evolves independently of the others. Therefore, our prior will be most informative when features activate in multiple languages. In phonology, it is useful to map phonemes to the International Phonetic Alphabet (IPA) in order to have a language-independent parameterization. We introduce a similarly neutral representation here by projecting languagespecific parts-of-speech to a coarse, shared inventory. Indeed, we assume that each language has a distinct tagset, and so the basic configurational features will be language specific. For example, when SPECIFIC: Activate for only one conjunction of outcome and conditions: 1(c = ?, h = ?, dir = ?, adj = ?) SHARED: Activate for heads from multiple languages using cross-lingual POS projection pi(?): 1(c = ?, pi(h) = ?, dir = ?, adj = ?) CONTINUE distribution feature templates. SPECIFIC: Activate for only one conjunction of outcome and conditions: 1(a = ?, h = ?, dir = ?) SHARED: Activate for heads and arguments from multiple languages using cross-lingual POS projection pi(?): 1(pi(a) = ?, pi(h) = ?, dir = ?) 1(pi(a) = ?, h = ?, dir = ?) 1(a = ?, pi(h) = ?, dir = ?) ATTACH distribution feature templates. Table 1: Feature templates for CONTINUE and ATTACH conditional distributions. an English VBZ takes a left argument headed by a NNS, a feature will activate specific to VBZ-NNSLEFT. That feature will be used in the log-linear attachment probability for English. However, because that feature does not show up in any other language, it is not usefully controlled by the prior. Therefore, we also include coarser features which activate on more abstract, cross-linguistic configurations. In the same example, a feature will fire indicating a coarse, direction-free NOUN-VERB attachment. This feature will now occur in multiple languages and will contribute to each of those languages? attachment models. Although such crosslingual features will have different weight parameters in each language, those weights will covary, being correlated by the prior. The coarse features are defined via a projection ? from language-specific part-of-speech labels to coarser, cross-lingual word classes, and hence we refer to them as SHARED features. For each corpus used in this paper, we use the tagging annotation guidelines to manually define a fixed mapping from the corpus tagset to the following coarse tagset: noun, verb, adjective, adverb, conjunction, preposition, determiner, interjection, numeral, and pronoun. Parts-of-speech for which this coarse mapping is ambiguous or impossible are not mapped, and do not have corresponding SHARED features. We summarize the feature templates for the CONTINUE and ATTACH conditional distributions in Table 1. Variants of all feature templates that ignore direction and/or adjacency are included. In practice, we found it beneficial for all language1290 independent features to ignore direction. Again, only the coarse features occur in multiple languages, so all phylogenetic influence is through those. Nonetheless, the effect of the phylogeny turns out to be quite strong. 2.4 Learning We now turn to learning with the phylogenetic prior. Since the prior couples parameters across languages, this learning problem requires parameters for all languages be estimated jointly. We seek to find ? = (?? : ? ? L ? L+) which optimizes log P (?|s), where s aggregates the observed leaves of all the dependency trees in all the languages. This can be written as log P (?) + logP (s|?) ? log P (s) The third term is a constant and can be ignored. The first term can be written as logP (?) = ? ??L?L+ 1 2?2 ??? ? ?par(?)? 2 2 + C where C is a constant. The form of logP (?) immediately shows how parameters are penalized for being different across languages, more so for languages that are near each other in the phylogeny. The second term log P (s|?) = ? ??L log P (s?|??) is a sum of observed data likelihoods under the standard DMV models for each language, computable by dynamic programming (Klein and Manning, 2004). Together, this yields the following objective function: l(?) = ? ??L?L+ 1 2?2 ??? ? ?par(?)?22 + ? ??L logP (s?|??) which can be optimized using gradient methods or (MAP) EM. Here we used L-BFGS (Liu et al, 1989). This requires computation of the gradient of the observed data likelihood log P (s?|??) which is given by: ? logP (s?|??) = Et?|s? [ ? log P (s?, t?|??) ] = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? c,h,dir,adj ec,h,dir,adj(s?; ??) ? [ f CONTINUE(c, h, dir, adj) ? ? c? P CONTINUE(c?|h, dir, adj; ??)f CONTINUE(c?, h, dir, adj) ] ? a,h,dir ea,h,dir(s?; ??) ? [ f ATTACH(a, h, dir) ? ? a? P ATTACH(a?|h, dir; ??)f ATTACH(a?, h, dir) ] ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? The expected gradient of the log joint likelihood of sentences and parses is equal to the gradient of the log marginal likelihood of just sentences, or the observed data likelihood (Salakhutdinov et al, 2003). ea,h,dir(s?; ??) is the expected count of the number of times head h is attached to a in direction dir given the observed sentences s? and DMV parameters ??. ec,h,dir,adj(s?; ??) is defined similarly. Note that these are the same expected counts required to perform EM on the DMV, and are computable by dynamic programming. The computation time is dominated by the computation of each sentence?s posterior expected counts, which are independent given the parameters, so the time required per iteration is essentially the same whether training all languages jointly or independently. In practice, the total number of iterations was also similar. 3 Experimental Setup 3.1 Data We ran experiments with the following languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. For all languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al, 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al, 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of their linguistic classifications. English and Dutch are part of the West Ger1291 English Dutch SwedishDanish Spanish Portuguese Slovene Chinese West Germanic North Germanic IberoRomance Slavic Sinitic Global English Dutch SwedishDanish Spanish Portuguese Slovene Chinese Global (a) (b) (c) English Dutch SwedishDanish Spanish Portuguese Slovene Chinese West Germanic North Germanic IberoRomance Slavic Sinitic Figure 2: (a) Phylogeny for FAMILIES model. (b) Phylogeny for GLOBAL model. (c) Phylogeny for LINGUISTIC model. manic family of languages, whereas Danish and Swedish are part of the North Germanic family. Spanish and Portuguese are both part of the IberoRomance family. Slovene is part of the Slavic family. Finally, Chinese is in the Sinitic family, and is not an Indo-European language like the others. We interchangeably speak of a language family and the ancestral node corresponding to that family?s root language in a phylogeny. 3.2 Models Compared We evaluated three phylogenetic priors, each with a different phylogenetic structure. We compare with two monolingual baselines, as well as an allpairs multilingual model that does not have a phylogenetic interpretation, but which provides very similar capacity for parameter coupling. 3.2.1 Phylogenetic Models The first phylogenetic model uses the shallow phylogeny shown in Figure 2(a), in which only languages within the same family have a shared parent node. We refer to this structure as FAMILIES. Under this prior, the learning task decouples into independent subtasks for each family, but no regularities across families can be captured. The family-level model misses the constraints between distant languages. Figure 2(b) shows another simple configuration, wherein all languages share a common parent node in the prior, meaning that global regularities that are consistent across all languages can be captured. We refer to this structure as GLOBAL. While the global model couples the parameters for all eight languages, it does so without sensitivity to the articulated structure of their descent. Figure 2(c) shows a more nuanced prior structure, LINGUISTIC, which groups languages first by family and then under a global node. This structure allows global regularities as well as regularities within families to be learned. 3.2.2 Parameterization and ALLPAIRS Model Daume? III (2007) and Finkel and Manning (2009) consider a formally similar Gaussian hierarchy for domain adaptation. As pointed out in Finkel and Manning (2009), there is a simple equivalence between hierarchical regularization as described here and the addition of new tied features in a ?flat? model with zero-meaned Gaussian regularization on all parameters. In particular, instead of parameterizing the objective in Section 2.4 in terms of multiple sets of weights, one at each node in the phylogeny (the hierarchical parameterization, described in Section 2.4), it is equivalent to parameterize this same objective in terms of a single set of weights on a larger of group features (the flat parameterization). This larger group of features contains a duplicate set of the features discussed in Section 2.3 for each node in the phylogeny, each of which is active only on the languages that are its descendants. A linear transformation between parameterizations gives equivalence. See Finkel and Manning (2009) for details. In the flat parameterization, it seems equally reasonable to simply tie all pairs of languages by adding duplicate sets of features for each pair. This gives the ALLPAIRS setting, which we also compare to the tree-structured phylogenetic models above. 3.3 Baselines To evaluate the impact of multilingual constraint, we compared against two monolingual baselines. The first baseline is the standard DMV with only SPECIFIC features, which yields the standard multinomial DMV (weak baseline). To facilitate comparison to past work, we used no prior for this monolingual model. The second baseline is the DMV with added SHARED features. This model includes a simple isotropic Gaussian prior on pa1292 Monolingual Multilingual Phylogenetic Corpus Size Ba se lin e B as el in e w /S H A R ED A LL PA IR S FA M IL IE S B ES TP A IR G LO B A L LI N G U IS TI C West Germanic English 6008 47.1 51.3 48.5 51.3 51.3 (Ch) 51.2 62.3Dutch 6678 36.3 36.0 44.0 36.1 36.2 (Sw) 44.0 45.1 North Germanic Danish 1870 33.5 33.6 40.5 31.4 34.2 (Du) 39.6 41.6Swedish 3571 45.3 44.8 56.3 44.8 44.8 (Ch) 44.5 58.3 Ibero-Romance Spanish 712 28.0 40.5 58.7 63.4 63.8 (Da) 59.4 58.4Portuguese 2515 38.5 38.5 63.1 37.4 38.4 (Sw) 37.4 63.0 Slavic Slovene 627 38.5 39.7 49.0 ? 49.6 (En) 49.4 48.4 Sinitic Chinese 959 36.3 43.3 50.7 ? 49.7 (Sw) 50.1 49.6 Macro-Avg. Relative Error Reduction 17.1 5.6 8.5 9.9 21.1 Table 2: Directed dependency accuracy of monolingual and multilingual models, and relative error reduction over the monolingual baseline with SHARED features macro-averaged over languages. Multilingual models outperformed monolingual models in general, with larger gains from increasing numbers of languages. Additionally, more nuanced phylogenetic structures outperformed cruder ones. rameters. This second baseline is the more direct comparison to the multilingual experiments here (strong baseline). 3.4 Evaluation For each setting, we evaluated the directed dependency accuracy of the minimum Bayes risk (MBR) dependency parses produced by our models under maximum (posterior) likelihood parameter estimates. We computed accuracies separately for each language in each condition. In addition, for multilingual models, we computed the relative error reduction over the strong monolingual baseline, macro-averaged over languages. 3.5 Training Our implementation used the flat parameterization described in Section 3.2.2 for both the phylogenetic and ALLPAIRS models. We originally did this in order to facilitate comparison with the non-phylogenetic ALLPAIRS model, which has no equivalent hierarchical parameterization. In practice, optimizing with the hierarchical parameterization also seemed to underperform.1 1We noticed that the weights of features shared across languages had larger magnitude early in the optimization procedure when using the flat parameterization compared to using the hierarchical parameterization, perhaps indicating that cross-lingual influences had a larger effect on learning in its initial stages. All models were trained by directly optimizing the observed data likelihood using L-BFGS (Liu et al., 1989). Berg-Kirkpatrick et al (2010) suggest that directly optimizing the observed data likelihood may offer improvements over the more standard expectation-maximization (EM) optimization procedure for models such as the DMV, especially when the model is parameterized using features. We stopped training after 200 iterations in all cases. This fixed stopping criterion seemed to be adequate in all experiments, but presumably there is a potential gain to be had in fine tuning. To initialize, we used the harmonic initializer presented in Klein and Manning (2004). This type of initialization is deterministic, and thus we did not perform random restarts. We found that for all models ?2 = 0.2 gave reasonable results, and we used this setting in all experiments. For most models, we found that varying ?2 in a reasonable range did not substantially affect accuracy. For some models, the directed accuracy was less flat with respect to ?2. In these less-stable cases, there seemed to be an interaction between the variance and the choice between head conventions. For example, for some settings of ?2, but not others, the model would learn that determiners head noun phrases. In particular, we observed that even when direct accuracy did fluctuate, undirected accuracy remained more stable. 1293 4 Results Table 2 shows the overall results. In all cases, methods which coupled the languages in some way outperformed the independent baselines that considered each language independently. 4.1 Bilingual Models The weakest of the coupled models was FAMILIES, which had an average relative error reduction of 5.6% over the strong baseline. In this case, most of the average improvement came from a single family: Spanish and Portuguese. The limited improvement of the family-level prior compared to other phylogenies suggests that there are important multilingual interactions that do not happen within families. Table 2 also reports the maximum accuracy achieved for each language when it was paired with another language (same family or otherwise) and trained together with a single common parent. These results appear in the column headed by BESTPAIR, and show the best accuracy for the language on that row over all possible pairings with other languages. When pairs of languages were trained together in isolation, the largest benefit was seen for languages with small training corpora, not necessarily languages with common ancestry. In our setup, Spanish, Slovene, and Chinese have substantially smaller training corpora than the rest of the languages considered. Otherwise, the patterns are not particularly clear; combined with subsequent results, it seems that pairwise constraint is fairly limited. 4.2 Multilingual Models Models that coupled multiple languages performed better in general than models that only considered pairs of languages. The GLOBAL model, which couples all languages, if crudely, yielded an average relative error reduction of 9.9%. This improvement comes as the number of languages able to exert mutual constraint increases. For example, Dutch and Danish had large improvements, over and above any improvements these two languages gained when trained with a single additional language. Beyond the simplistic GLOBAL phylogeny, the more nuanced LINGUISTIC model gave large improvements for English, Swedish, and Portuguese. Indeed, the LINGUISTIC model is the only model we evaluated that gave improvements for all the languages we considered. It is reasonable to worry that the improvements from these multilingual models might be partially due to having more total training data in the multilingual setting. However, we found that halving the amount of data used to train the English, Dutch, and Swedish (the languages with the most training data) monolingual models did not substantially affect their performance, suggesting that for languages with several thousand sentences or more, the increase in statistical support due to additional monolingual data was not an important effect (the DMV is a relatively low-capacity model in any case). 4.3 Comparison of Phylogenies Recall the structures of the three phylogenies presented in Figure 2. These phylogenies differ in the correlations they can represent. The GLOBAL phylogeny captures only ?universals,? while FAMILIES captures only correlations between languages that are known to be similar. The LINGUISTIC model captures both of these effects simultaneously by using a two layer hierarchy. Notably, the improvement due to the LINGUISTIC model is more than the sum of the improvements due to the GLOBAL and FAMILIES models. 4.4 Phylogenetic vs. ALLPAIRS The phylogeny is capable of allowing appropriate influence to pass between languages at multiple levels. We compare these results to the ALLPAIRS model in order to see whether limitation to a tree structure is helpful. The ALLPAIRS model achieved an average relative error reduction of 17.1%, certainly outperforming both the simple phylogenetic models. However, the rich phylogeny of the LINGUISTIC model, which incorporates linguistic constraints, outperformed the freer ALLPAIRS model. A large portion of this improvement came from English, a language for which the LINGUISTIC model greatly outperformed all other models evaluated. We found that the improved English analyses produced by the LINGUISTIC model were more consistent with this model?s analyses of other languages. This consistency was not present for the English analyses produced by other models. We explore consistency in more detail in Section 5. 4.5 Comparison to Related Work The likelihood models for both the strong monolingual baseline and the various multilingual mod1294 els are the same, both expanding upon the standard DMV by adding coarse SHARED features. These coarse features, even in a monolingual setting, improved performance slightly over the weak baseline, perhaps by encouraging consistent treatment of the different finer-grained variants of partsof-speech (Berg-Kirkpatrick et al, 2010).2 The only difference between the multilingual systems and the strong baseline is whether or not crosslanguage influence is allowed through the prior. While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. When Cohen and Smith compared their best shared logistic-normal bilingual models to monolingual counter-parts for the languages they investigate (Chinese and English), they reported a relative error reduction of 5.3%. In comparison, with the LINGUISTIC model, we saw a much larger 16.9% relative error reduction over our strong baseline for these languages. Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. Our results indicate that the majority of our model?s power beyond that of the standard DMV is derived from multilingual, and in particular, more-than-bilingual, interaction. These are, to the best of our knowledge, the first results of this kind for grammar induction without bitext. 5 Analysis By examining the proposed parses we found that the LINGUISTIC and ALLPAIRS models produced analyses that were more consistent across languages than those of the other models. We also observed that the most common errors can be summarized succinctly by looking at attachment counts between coarse parts-of-speech. Figure 3 shows matrix representations of dependency 2Coarse features that only tie nouns and verbs are explored in Berg-Kirkpatrick et al (2010). We found that these were very effective for English and Chinese, but gave worse performance for other languages. counts. The area of a square is proportional to the number of order-collapsed dependencies where the column label is the head and the row label is the argument in the parses from each system. For ease of comprehension, we use the cross-lingual projections and only show counts for selected interesting classes. Comparing Figure 3(c), which shows dependency counts proposed by the LINGUISTIC model, to Figure 3(a), which shows the same for the strong monolingual baseline, suggests that the analyses proposed by the LINGUISTIC model are more consistent across languages than are the analyses proposed by the monolingual model. For example, the monolingual learners are divided as to whether determiners or nouns head noun phrases. There is also confusion about which labels head whole sentences. Dutch has the problem that verbs modify pronouns more often than pronouns modify verbs, and pronouns are predicted to head sentences as often as verbs are. Spanish has some confusion about conjunctions, hypothesizing that verbs often attach to conjunctions, and conjunctions frequently head sentences. More subtly, the monolingual analyses are inconsistent in the way they head prepositional phrases. In the monolingual Portuguese hypotheses, prepositions modify nouns more often than nouns modify prepositions. In English, nouns modify prepositions, and prepositions modify verbs. Both the Dutch and Spanish models are ambivalent about the attachment of prepositions. As has often been observed in other contexts (Liang et al, 2008), promoting agreement can improve accuracy in unsupervised learning. Not only are the analyses proposed by the LINGUISTIC model more consistent, they are also more in accordance with the gold analyses. Under the LINGUISTIC model, Dutch now attaches pronouns to verbs, and thus looks more like English, its sister in the phylogenetic tree. The LINGUISTIC model has also chosen consistent analyses for prepositional phrases and noun phrases, calling prepositions and nouns the heads of each, respectively. The problem of conjunctions heading Spanish sentences has also been corrected. Figure 3(b) shows dependency counts for the GLOBAL multilingual model. Unsurprisingly, the analyses proposed under global constraint appear somewhat more consistent than those proposed under no multi-lingual constraint (now three lan1295 Figure 3: Dependency counts in proposed parses. Row label modifies column label. (a) Monolingual baseline with SHARED features. (b) GLOBAL model. (c) LINGUISTIC model. (d) Dependency counts in hand-labeled parses. Analyses proposed by monolingual baseline show significant inconsistencies across languages. Analyses proposed by LINGUISTIC model are more consistent across languages than those proposed by either the monolingual baseline or the GLOBAL model. guages agree that prepositional phrases are headed by prepositions), but not as consistent as those proposed by the LINGUISTIC model. Finally, Figure 3(d) shows dependency counts in the hand-labeled dependency parses. It appears that even the very consistent LINGUISTIC parses do not capture the non-determinism of prepositional phrase attachment to both nouns and verbs. 6 Conclusion Even without translated texts, multilingual constraints expressed in the form of a phylogenetic prior on parameters can give substantial gains in grammar induction accuracy over treating languages in isolation. Additionally, articulated phylogenies that are sensitive to evolutionary structure can outperform not only limited flatter priors but also unconstrained all-pairs interactions. 7 Acknowledgements This project is funded in part by the NSF under grant 0915265 and DARPA under grant N10AP20007. 1296 References T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In North American Chapter of the Association for Computational Linguistics. D. M. Bikel and D. Chiang. 2000. Two statistical parsing models applied to the Chinese treebank. In Second Chinese Language Processing Workshop. A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths. 2007. A probabilistic approach to diachronic phonology. In Empirical Methods in Natural Language Processing. S. Buchholz and E. Marsi. 2006. Computational Natural Language Learning-X shared task on multilingual dependency parsing. In Conference on Computational Natural Language Learning. D. Burkett and D. Klein. 2008. Two languages are better than one (for syntactic parsing). In Empirical Methods in Natural Language Processing. S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In North American Chapter of the Association for Computational Linguistics. M. Collins. 1999. Head-driven statistical models for natural language parsing. In Ph.D. thesis, University of Pennsylvania, Philadelphia. H. Daume? III. 2007. Frustratingly easy domain adaptation. In Association for Computational Linguistics. J. Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Association for Computational Linguistics. J. R. Finkel and C. D. Manning. 2009. Hierarchical bayesian domain adaptation. In North American Chapter of the Association for Computational Linguistics. D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Association for Computational Linguistics. J. Kuhn. 2004. Experiments in parallel-text based grammar induction. In Association for Computational Linguistics. G. Kuzman, J. Gillenwater, and B. Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Association for Computational Linguistics/International Joint Conference on Natural Language Processing. P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-based learning. In Advances in Neural Information Processing Systems. D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming. M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the penn treebank. Computational Linguistics. R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003. Optimization with EM and expectationconjugate-gradient. In International Conference on Machine Learning. D. A. Smith and J. Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Empirical Methods in Natural Language Processing. B. Snyder, T. Naseem, and R. Barzilay. 2009a. Unsupervised multilingual grammar induction. In Association for Computational Linguistics/International Joint Conference on Natural Language Processing. B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay. 2009b. Adding more languages improves unsupervised multilingual part-of-speech tagging: A Bayesian non-parametric approach. In North American Chapter of the Association for Computational Linguistics. N. Xue, F-D Chiou, and M. Palmer. 2002. Building a large-scale annotated Chinese corpus. In International Conference on Computational Linguistics. 1297 "}
{"date": 2000, "text": " Lexical transfer using a vector-space model  Eiichiro SUMITA ATR Spoken Language Translation Research Laboratories 2-2 Hikaridai, Seika, Soraku Kyoto 619-0288, Japan sumita@slt.atr.co.jp  Abstract Building a bilingual dictionary for transfer in a machine translation system is conventionally done by hand and is very time-consuming. In order to overcome this bottleneck, we propose a new mechanism for lexical transfer, which is simple and suitable for learning from bilingual corpora. It exploits a vector-space model developed in information retrieval research. We present a preliminary result from our computational experiment. Introduction Many machine translation systems have been developed and commercialized. When these systems are faced with unknown domains, however, their performance degrades. Although there are several reasons behind this poor performance, in this paper, we concentrate on one of the major problems, i.e., building a bilingual dictionary for transfer. A bilingual dictionary consists of rules that map a part of the representation of a source sentence to a target representation by taking grammatical differences (such as the word order between the source and target languages) into consideration. These rules usually use case-frames as their base and accompany syntactic and/or semantic constraints on mapping from a source word to a target word. For many machine translation systems, experienced experts on individual systems compile the bilingual dictionary, because this is a complicated and difficult task. In other words, this task is knowledge-intensive and labor-intensive, and therefore, time-consuming. Typically, the developer of a machine translation system has to spend several years building a general-purpose bilingual dictionary. Unfortunately, such a general-purpose dictionary is not almighty, in that (1) when faced with a new domain, unknown source words may emerge and/or some domain-specific usages of known words may appear and (2) the accuracy of the target word selection may be insufficient due to the handling of many target words simultaneously. Recently, to overcome these bottlenecks in knowledge building and/or tuning, the automation of lexicography has been studied by many researchers: (1) approaches using a decision tree: the ID3 learning algorithm is applied to obtain transfer rules from case-frame representations of simple sentences with a thesaurus for generalization (Akiba et. al., 1996 and Tanaka, 1995); (2) approaches using structural matching: to obtain transfer rules, several search methods have been proposed for maximal structural matching between trees obtained by parsing bilingual sentences (Kitamura and Matsumoto, 1996; Meyers et. al., 1998; and Kaji et. al.,1992). 1 Our proposal 1.1 Our problem and approach In this paper, we concentrate on lexical transfer, i.e., target word selection. In other words, the mapping of structures between source and target expressions is not dealt with here. We assume that this structural transfer can be solved on top of lexical transfer. We propose an approach that differs from the studies mentioned in the introduction section in that: I) It use not structural representations like case frames but vector-space representations. II) The weight of each element for constraining the ambiguity of target words is determined automatically by following the term frequency and  inverse document frequency in information retrieval research. III) A word alignment that does not rely on parsing is utilized. IV) Bilingual corpora are clustered in terms of target equivalence. 1.2 Background The background for the decisions made in our approach is as follows: A) We would like to reduce human interaction to prepare the data necessary for building lexical transfer rules. B) We do not expect that mature parsing systems for multi-languages and/or spoken languages will be available in the near future. C) We would like the determination of the importance of each feature in the target selection to be automated. D) We would like the problem caused by errors in the corpora and data sparseness to be reduced. 2 Vector-space model This section explains our trial for applying a vector-space model to lexical transfer starting from a basic idea. 2.1 Basic idea We can select an appropriate target word for a given source word by observing the environment including the context, world knowledge, and target words in the neighborhood. The most influential elements in the environment are of course the other words in the source sentence surrounding the concerned source word. Suppose that we have translation examples including the concerned source word and we know in advance which target word corresponds to the source word. By measuring the similarity between (1) an unknown sentence that includes the concerned source word and (2) known sentences that include the concerned source word, we can select the target word which is included in the most similar sentence. This is the same idea as example-based machine translation (Sato and Nagao, 1990 and Furuse et. al., 1994).  Group1: ?? (not sweet) source sentence 1: This beer is drier and full-bodied. target sentence 1: ??????????????????  source sentence 2: Would you like dry or sweet sherry? target sentence 2: ??????????????????????????  source sentence 3: A dry red wine would go well with it. target sentence 3: ?????????????????? Group2: ?? (not wet) source sentence 4: Your skin feels so dry. target sentence 4: ????????????  source sentence 5: You might want to use some cream to protect your skin against the dry air. target sentence 5: ?????????????????????????????? Table 1 Portions of English ?dry? into Japanese for an aligned corpus   Listed in Table 1 are samples of English-Japanese sentence pairs of our corpus including the source word ?dry.? The upper three samples of group 1 are translated with the target word ??? (not sweet)? and the lower two samples of group 2 are translated with the target word ??? (not wet).? The remaining portions of target sentences are hidden here because they do not relate to the discussion in the paper. The underlined words are some of the cues used to select the target words. They are distributed in the source sentence with several different grammatical relations such as subject, parallel adjective, modified noun, and so on, for the concerned word ?dry.? 2.2 Sentence vector We propose representing the sentence as a sentence vector, i.e., a vector that lists all of the words in the sentence. The sentence vector of the first sentence of Table 1 is as follows: <this, beer, is, dry, and, full-body>  Figure 1 System Configuration Figure 1 outlines our proposal. Suppose that we have the sentence vector of an input sentence I and the sentence vector of an example sentence E from a bilingual corpus. We measure the similarity by computing the cosine of the angle between I and E. We output the target word of the example sentence whose cosine is maximal.  2.3 Modification of sentence vector The na?ve implementation of a sentence vector that uses the occurrence of words themselves suffers from data sparseness and unawareness of relevance. 2.3.1 Semantic category incorporation To reduce the adverse influence of data sparseness, we count occurrences by not only the words themselves but also by the semantic categories of the words given by a thesaurus. For example, the ??? (not sweet)? sentences of Vector generator Bilingual corpus Corpus vector, {E} Thesaurus Input sentence Input vector, I Cosine calculation The most similar vector  Table 1 have the different cue words of ?beer,? ?sherry,? and ?wine,? and the cues are merged into a single semantic category alcohol in the sentence vectors. 2.3.2 Grouping sentences and weighting dimensions The previous subsection does not consider the relevance to the target selection of each element of the vectors; therefore, the selection may fail due to non-relevant elements. We exploit the term frequency and inverse document frequency in information retrieval research. Here, we regard a group of sentences that share the same target word as a document.? Vectors are made not sentence-wise but group-wise. The relevance of each dimension is the term frequency multiplied by the inverse document frequency. The term frequency is the frequency in the document (group). A repetitive occurrence may indicate the importance of the word. The inverse document frequency corresponds to the discriminative power of the target selection. It is usually calculated as a logarithm of N divided by df where N is the number of the documents (groups) and df is the frequency of documents (groups) that include the word.  Cluster 1: a piece of paper money, C(??) source sentence 1: May I have change for a ten dollar bill? target sentence 1: ?????????????????  source sentence 2: Could you change a fifty dollar bill? target sentence 2: ??????????????? Cluster 2: an account, C(??) source sentence 3: I've already paid the bill. target sentence 3: ???????????  source sentence 4: Isn't my bill too high? target sentence 4: ??????????????  source sentence 5: I'm checking out. May I have the bill, please? target sentence 5: ????????????????? Table 2 Samples of groups clustered by target equivalence  3 Pre-processing of corpus Before generating vectors, the given bilingual corpus is pre-processed in two ways (1) words are aligned in terms of translation; (2) sentences are clustered in terms of target equivalence to reduce problems caused by data sparseness. 3.1 Word alignment We need to have source words and target words aligned in parallel corpora. We use a word alignment program that does not rely on parsing (Sumita, 2000). This is not the focus of this paper, and therefore, we will only describe it briefly here. First, all possible alignments are hypothesized as a matrix filled with occurrence similarities between source words and target words. Second, using the occurrence similarities and other constraints, the most plausible alignment is selected from the matrix.  3.2 Clustering by target words We adopt a clustering method to avoid the sparseness that comes from variations in target words. The translation of a word can vary more than the meaning of the target word.  For example, the English word ?bill? has two main meanings: (1) a piece of paper money, and (2) an account. In Japanese, there is more than one word for each meaning. For (1), ??? and ?? ?? can correspond, and for (2), ???,? ?? ?,? and ???? can correspond. The most frequent target word can represent the cluster, e.g., ???? for (1) a piece of paper money; ???? for (2) an account. We assume that selecting a cluster is equal to selecting the target word. If we can merge such equivalent translation variations of target words into clusters, we can improve the accuracy of lexical transfer for two reasons: (1) doing so makes the mark larger by neglecting accidental differences among target words; (2) doing so collects scattered pieces of evidence and strengthens the effect. Furthermore, word alignment as an automated process is incomplete. We therefore need to filter out erroneous target words that come from alignment errors. Erroneous target words are considered to be low in frequency and are expected to be semantically dissimilar from correct target words based on correct alignment. Clustering example corpora can help filter out erroneous target words. By calculating the semantic similarity between the semantic codes of target words, we perform clustering according to the simple algorithm in subsection 3.2.2. 3.2.1 Semantic similarity Suppose each target word has semantic codes for all of its possible meanings. In our thesaurus, for example, the target word ??? has three decimal codes, 974 (label/tag), 829 (counter) and 975 (money) and the target word ???? has a single code 975 (money). We represent this as a code vector and define the similarity between the two target words by computing the cosine of the angle between their code vectors. 3.2.2 Clustering algorithm We adopt a simple procedure to cluster a set of n target words X = {X1, X2,?, Xn}. X is sorted in the descending order of the frequency of Xn in a sub-corpus including the concerned source word. We repeat (1) and (2) until the set X is empty. (1) We move the leftmost Xl from X to the new cluster C(Xl). (2) For all m (m>l) , we move Xm from X to C(Xl) if the cosine of Xl and Xm is larger than the threshold T. As a result, we obtain a set of clusters {C(Xl)} for each meaning as exemplified in Table 2. The threshold of semantic similarity T is determined empirically. T in the experiment was 1/2. 4 Experiment To demonstrate the feasibility of our proposal, we conducted a pilot experiment as explained in this section.  Number of sentence pairs (English-Japanese) 19,402 Number of source words (English) 156,128 Number of target words (Japanese) 178,247 Number of source content words (English) 58,633 Number of target content words (Japanese) 64,682 Number of source different content words (English) 4,643 Number of target different content words (Japanese) 6,686 Table 3 Corpus statistics   4.1 Experimental conditions For our sentence vectors and code vectors, we used hand-made thesauri of Japanese and English covering our corpus (for a travel arrangement task), whose hierarchy is based on that of the Japanese commercial thesaurus Kadokawa Ruigo Jiten (Ohno and Hamanishi, 1984). We used our English-Japanese phrase book (a collection of pairs of typical sentences and their translations) for foreign tourists. The statistics of the corpus are summarized in Table 3. We word-aligned the corpus before generating the sentence vectors. We focused on the transfer of content words such as nouns, verbs, and adjectives. We picked out six polysemous words for a preliminary evaluation: ?bill,? ?dry,? ?call? in English and ?? ,? ??? ,? ??? ? in Japanese. We confined ourselves to a selection between two major clusters of each source word using the method in subsection 3.2  #1&2 #1 baseline #correct vsm bill [noun] 47 30 64% 40 85% call [verb] 179 93 52% 118 66% dry [adjective] 6 3 50% 4 67% ? [noun] 19 13 68% 14 73% ?? [verb] 60 42 70% 49 82% ?? [adjective] 26 15 57% 16 62% Table 4 Accuracy of the baseline and the VSM systems  4.2 Selection accuracy We compared the accuracy of our proposal using the vector-space model  (vsm system) with that of a decision-by-majority model (baseline system). The results are shown in Table 4. Here, the accuracy of the baseline system is #1 (the number of target sentences of the most major cluster) divided by #1&2 (the number of target sentences of clusters 1 & 2). The accuracy of the vsm system is #correct (the number of vsm answers that match the target sentence) divided by #1&2.  #all #1&2 Coverage bill [noun] 63 47 74% call [verb] 226 179 79% dry [adjective] 8 6 75% ? [noun] 22 19 86% ?? [verb] 77 60 78% ?? [adjective] 38 26 68% Table 5 Coverage of the top two clusters  Judging was done mechanically by assuming that the aligned data was 100% correct.1 Our vsm system achieved an accuracy from about 60% to about 80% and outperformed the baseline system by about 5% to about 20%.  1  This does not necessarily hold, therefore, performance degrades in a certain degree. 4.3 Coverage of major clusters One reason why we clustered the example database was to filter out noise, i.e., wrongly aligned words. We skimmed the clusters and we saw that many instances of noise were filtered out. At the same time, however, a portion of correctly aligned data was unfortunately discarded. We think that such discarding is not  fatal because the coverage of clusters 1&2 was relatively high, around 70% or 80% as shown in Table 5. Here, the coverage is #1&2 (the number of data not filtered) divided by #all (the number of data before discarding). 5 Discussion 5.1 Accuracy An experiment was done for a restricted problem, i.e., select the appropriate one cluster (target word) from two major clusters (target words), and the result was encouraging for the automation of the lexicography for transfer. We plan to improve the accuracy obtained so far by exploring elementary techniques: (1) Adding new features including extra linguistic information such as the role of the speaker of the sentence (Yamada et al, 2000) (also, the topic that sentences are referring to) may be effective; and (2) Considering the physical distance from the concerned input word, which may improve the accuracy. A kind of window function might also be useful; (3) Improving the word alignment, which may contribute to the overall accuracy. 5.2 Data sparseness In our proposal, deficiencies in the na?ve implementation of vsm are compensated in several ways by using a thesaurus, grouping, and clustering, as explained in subsections 2.3 and 3.2. 5.3 Future work We showed only the translation of content words. Next, we will explore the translation of function words, the word order, and full sentences. Our proposal depends on a handcrafted thesaurus. If we manage to do without craftsmanship, we will achieve broader applicability. Therefore, automatic thesaurus construction is an important research goal for the future. Conclusion In order to overcome a bottleneck in building a bilingual dictionary, we proposed a simple mechanism for lexical transfer using a vector space. A preliminary computational experiment showed that our basic proposal is promising. Further development, however, is required: to use a window function or to use a better alignment program; to compare other statistical methods such as decision trees, maximal entropy, and so on. Furthermore, an important future work is to create a full translation mechanism based on this lexical transfer. Acknowledgements Our thanks go to Kadokawa-Shoten for providing us with the Ruigo-Shin-Jiten. References Akiba, O., Ishii, M., ALMUALLIM, H., and Kaneda, S. (1996) A Revision Learner to Acquire English Verb Selection Rules, Journal of NLP, 3/3, pp. 53-68, (in Japanese). Furuse, O., Sumita, E. and Iida, H. (1994) Transfer-Driven Machine Translation Utilizing Empirical Knowledge, Transactions of IPSJ, 35/3, pp. 414-425, (in Japanese). Kaji, H., Kida, Y. and Morimoto, Y. (1992) Learning translation templates from bilingual text, Proc. of Coling-92, pp. 672-678. Kitamura, M. and Matsumoto, Y. (1996) Automatic Acquisition of Translation Rules from Parallel Corpora, Transactions of IPSJ, 37/6, pp. 1030-1040, (in Japanese). Meyers, A., Yangarber, R., Grishman, R., Macleod, C., and Sandoval, A. (1998) Deriving Transfer rules from dominance-preserving alignments, Coling-ACL98, pp. 843-847. Ohno, S. and Hamanishi, M. (1984) Ruigo-Shin-Jiten, Kadokawa, p. 932, (in Japanese). Sato, S. and Nagao, M. (1990) Toward memory-based translation, Coling-90, pp. 247-252. Sumita, E. (2000) Word alignment using matrix PRICAI-00, 2000, (to appear). Tanaka H. (1995) Statistical Learning of ?Case Frame Tree? for Translating English Verbs, Journal of NLP, 2/3, pp. 49-72, (in Japanese). Yamada, S., Sumita, E. and Kashioka, H. (2000) Translation using Information on Dialogue Participants, ANLP-00, pp. 37-43. "}
{"date": 1994, "text": "GRADED UNIF ICAT ION:  A FRAMEWORK FOR INTERACTIVE  PROCESSING Alber t  K im * Depar tment  of  Computer  and  In fo rmat ion  Sciences Un ivers i ty  of  Pennsy lvan ia Ph i lade lph ia ,  Pennsy lvan ia ,  USA email :  a lk im?unagi ,  cis. upenn, edu Abst rac t An extension to classical unification, called graded unification is presented. It is capable of combining contradictory information. An interactive processing paradigm and parser based on this new operator are also presented. In t roduct ion Improved understanding of the nature of knowledge used in human language processing suggests the feasibility of interactive models in computational linguistics (CL). Recent psycholinguistic work such as (Stowe, 1989; Trueswell et al, 1994) has documented rapid employment of semantic information to guide human syntactic processing. In addition, corpus-based stochastic modelling of lexical patterns (see Weischedel et al, 1993) may provide information about word sense frequency of the kind advocated since (Ford et al, 1982). Incremental employment of such knowledge to resolve syntactic ambiguity is a natural step towards improved cognitive accuracy and efficiency in CL models. This exercise will, however, pose difficulties for the classical ('hard') constraint-based paradigm. As illustrated by the Trueswell et al (1994) results, this view of constraints is too rigid to handle the kinds of effects at hand. These experiments used pairs of locally ambiguous reduced relative clauses such as: 1) the man recognized by the spy took off down the street 2) the van recognized by the spy took off down the street The verb recognized is ambiguously either a past participial form or a past tense form. Eye tracking showed that subjects resolved the ambiguity rapidly (before reading the by-phrase) in 2) but not in 1) 1. The conclusion they draw is that subjects use knowledge about thematic roles to guide syntactic decisions. Since van, which is inanimate, makes a good Theme but a poor Agent for recognized, the past participial analysis in 2) is reinforced and the main clause (past tense) suppressed. Being animate, man performs either thematic role well, allowing the main clause reading to remain *I thank Christy Doran, Jason Eisner, Jeff Reynar, and John Trueswell for valuable comments. I am grateful to Ewan Klein and the Centre for Cognitive Science, Edinburgh, where most of this work was conducted, and also acknowledge the support of DARPA grant N00014-90-J-1863. 1In fact, ambiguity effects were often completely eliminated in examples like 2), with reading times matching those for the unambiguous case: 3) the man/van that was recognized by the spy ... plausible until the disambiguating by-phrase is encountered. At this point, readers of 1) displayed confusion. Semantic onstraints do appear to be at work here. However, the effects observed by Trueswell et al are graded. Verb-complement combinations occupy a continuous spectrum of \"thematic fit\", which influences reading times. This likely stems from the variance of verbs with respect o the thematic roles they allow (e.g., Agent, Instrument, Patient, etc.) and the syntactic positions of these. The upshot of such observations i  that classical unification (see Shieber, 1986), which has served well as the combinatory mechanism in classical constraint-based parsers, is too brittle to withstand this onslaught of uncertainty. This paper presents an extension to classical unification, called graded unification. Graded unification combines two feature structures, and returns a strength which reflects the compatibility of the information encoded by the two structures. Thus, two structures which could not unify via classical unification may unify via graded unification, and all combinatory decisions made during processing are endowed with a level of goodness. The operator is similar in spirit to the operators of fuzzy logic (see Kapcprzyk, 1992), which attempts to provide a calculus for reasoning in uncertain domains. Another related approach is the \"Unification Space\" model of Kempen & Vosse (1989), which unifies through a process of simulated annealing, and also uses a notion of unification strength. A parser has been implemented which combines constituents via graded unification and whose decisions are influenced by unification strengths. The result is a paradigm of incremental processing, which maintains a feature-based system of knowledge representation. System Descr ipt ion Though the employment of graded unification engenders a new processing style, the system's architecture parallels that of a conventional unification-based parser. Feature Structures: Prioritized Features The feature structures which encode the grammar in this system are conventional feature structures augmented by the association of priorities with each atomic-valued feature. Prioritizing features allows them to vary in terms of influence over the strength of unification. The priority of an atomic-valued feature fi in a feature structure X will be denoted by Pr i ( f i ,  X) . The effect of feature prioritization is clarified in the following sections. 313 Graded Unification Given two feature structures, the graded unification mechanism (Ua) computes two results, a unifying structure and a unification strength. S t ructura l  Uni f icat ion Graded unification builds structure xactly as classical unification except in the case of atomic unification, where it deviates crucially. Atoms in this framework are weighted isjunctive values. The weight associated with a disjunct is viewed as the confidence with which the processor believes that disjunct o be the 'correct' value. Figures l(a) and l(b) depict atoms (where l(a) is \"truly atomic\" because it contains only one disjunct). (a) (b) (?) Figure h Examples of Atoms Atomic unification creates a mixture of its two argument atoms as follows. When two atoms are unified, the set union of their disjuncts is collected in the result. For each disjunct in the result, the associated weight becomes the average of the weights associated with that disjunct in the two argument atoms. Figure l(c) shows an example unification of two atoms. The result is an atom which is 'believed' to be SG (singular), but could possibly be PL (plural). Unification Strength The unification strength (denoted t3aStrength)  is a weighted average of atomic unification strengths, defined in terms of two sums, the actual compatibil ity and the perfect compatibility. If A and B are non-atomic feature structures to be unified, then the following holds: I l aS t rength(A ,  B )  = ActualCornpatibility(A,B) Per \\] ectC ornpatibility( A,B ) \" The actual  compat ib i l i ty  is the sum: Pri(f i ,A)+Pri( l i ,B) , UGStrength(v ia ,V iB) ~.  if fi shared by A and B ? Pv i ( f i ,  A)  if f i  occurs only in A Pr i ( f i ,  B )  if fi occurs only in B where i indexes all atomic-valued features in A or B, and v;a and ViB are the values of f i  in A and B respectively. The perfect  compat ib i l i ty  is computed by a formula identical to this except hat UaSt rength  is set to 1. If A and B are atomic, then I IGStreng lh(A,  B)  is the total weight of disjuncts shared by A and B: t J cS t rength(A ,B)  = ~-~i M in (w iA ,  WiB) where i indexes all disjuncts di shared by A and B,  and wia and wiB are the weights of di in A and B respectively. By taking atomic unification strengths into account, the actual compatibility provides a raw measure of the extent to which two feature structures agree. By ignoring unification strengths (assuming a value of 1.o), the perfect compatibility is an idealization of the actual compatibility; it is what the actual compatibility would be if the two structures were able to unify via classical unification. Thus, unification strength is always a value between 0 and 1. The Parser: Activated Chart Edges The parser is a modified unification-based chart parser. Chart edges are assigned activation levels, which represent the 'goodness' of (or confidence in) their associated analyses. Each new edge is activated according to the strength of the unification which licenses its creation and the activations of its constituent edges. Constraining Graded Unification Without some strict limit on its operation, graded unification will overgenerate wildly. Two mechanisms exist to constrain graded unification. First, if a particular unification completes with strength below a specified unification threshold, it fails. Second, if a new edge is constructed with activation below a specified activation threshold, it is not allowed to enter the chart, and is suspended. Parsing Strategy  The chart is initialized to contain one inactive edge for each lexical entry of each word in the input. Lexical edges are currently assigned an initial activation of 1.o. The chart can then be expanded in two ways: 1. An active edge may be extended by unifying its first unseen constituent with the LrlS of an inactive edge. 2. A new active edge may be created by unifying the LHS of a rule with the first unseen constituent of some active edge in the chart (top down rule invocation). E~EI IA  ~ s o/c~>~ ,r~e.2 I I G ~  \\[ c\" - -  o ,o Figure 2: Extension of an Active Edge by an Inactive Edge Figure 2 depicts the extension of the active EDGE1 with the inactive EDGE2. The characters represent feature structures, and the ovular nodes on the right end of each edge represent activation level. The parser tries to unify C', the mother node of EDGE2, with C, the first needed constituent of EDGE1. If this unification succeeds, the parser builds the extended edge, EDGE3 (where C Ua C' produces C\"). The activation of the new edge is a function of the strength of the unification and the current activations of EDGE1 and EDGE2: activ3 = wl ? t J cSTRENGTH(C,  C') + w~ ? activl 9- w 3 . activ2 (The weights wi sum to 1.) EDGE3 enters the chart only if its activation exceeds the activation threshold. Rule invocation is depicted in figure 3. The first needed constituent in EDGE1 is unified with the LHS of aULE1. EDGE2 is created to begin searching for C. The new edge's activation is again a function of unification strength and other activations: activ 3 --- w l  ? UGSTRENGTH(C,  C') 9- w2 ? activl + w 3 . activ2 314 E~E~ I A - -  B o / C ~ RULEI \\[_IGOr-------------'/ \\[ C ' - -  D E ~ EDGE2 ~ ' J ~ \"  ~ o D E Figure 3: Top Down Rule Invocation The activation levels of grammar rule edges, like those for lexical edges, are currently pegged to 1.o. A Framework for Interact ive Processing The system described above provides a flexible framework for the interactive use of non-syntactic knowledge. An imacy  and  Themat ic  Ro les Knowledge about animacy and its important function in the filling of thematic roles can be modelled as a binary feature, ANIMATE. A (active voice) verb can strongly 'want' an animate Agent by specifying that its subject be \\[ANIMATE Jr\\] and assigning ahigh priority to the feature ANIMATE. Thus, any parse combining this verb with an inanimate subject will suffer in terms of unification strength. A noun can be strongly animate by having a high weight associated with the positive value of ANIMATE. Animacy has been encoded in a toy grammar. However, principled settings for the priority of this feature are left to future work. S ta t i s t i ca l  In fo rmat ion  f rom Corpora Corpus-based part-of-speech (POS) statistics can also be naturally incorporated into the current model. It is proposed here that a Viterbi decoder could be used to generate the likelihoods of the n best POS tags for a given word in the input string. Lexical chart edges would then be initially activated to levels proportional to the predicted likelihoods of their associated tags. Since these activations will be propagated to larger edges, parses involving predicted word senses would consequently be given a head start in a race of activations. Attractively, this strategy allows a fuller use of statistical information than one which uses the information simply to deterministically choose the n best tags, which are then treated as equally likely. In teract ion  o f  D iverse  In fo rmat ion A crucial feature of this framework is its potential for modelling the interaction between sources of information like the two above when they disagree. Sentences 1} and 2) again provide illustration. In such sentences, knowledge about word sense frequency supports the wrong analysis, and semantic onstraints must be employed to achieve the correct (human) performance. Intuitively, the raw frequency (without considering context) of the past tense form of recognized is higher than that of the past participial. POS taggers, despite considering local context, consistently mis-tag the verb in reduced relatives. The absence of a disambiguating relativizer (e.g., that) is one obvious ource of difficulty here. But even the ostensibly disambiguating preposition by, is itself ambiguous, since it might introduce a manner or locative phrase consistent with the main clause analysis. 2 Modelling human performance in such contexts requires allowing thematic information to compete against and defeat word frequency information. The current model allows such competition, as follows. POS information may incorrectly predict the main clause analysis, boosting the lexical edge associated with the past tense, and thereby boosting the main clause parse. However, the unification combining the past tense form of recognized with an inanimate subject (van) will be weak, due to the constraints encoded in the verb's lexical entry. Since the activations of constituent edges depend on the strengths of the unifications used to build them, the main clause parse Will lose activation. The parse combining the past participial with an inanimate subject (Theme) will suffer no losses, allowing it to overtake the incorrect parse. Conclusions and Future Work Assigning feature priorities and activation thresholds in this model will certainly be a considerable task. It is hoped that principled and automated methods can be found for assigning values to these variables. One promising idea is to glean information about patterns of subcategorization a d thematic roles from annotated corpora. Annotation of such information has been suggested as a future direction for the Treebank project (Marcus el al., 1993). It should be noted that learning such information will require more training data (hence larger corpora) than learning to tag part of speech. In addition, psycholinguistic studies such as the large norming study 3 of MacDonald and Pearlmutter (described in Trueswell et al, 1994) may prove useful in encoding thematic information in small lexicons. References Ford~ M., J. Bresnan, &: B.. Kaplan (1982). A Competence Based Theory of Syntact ic Closure. In Bresnan, J. (Ed.), The Mental Representation of Grammatica l  l:telations (pp. 727-796). MIT Press, Cambridge, MA. Kempen, O. and T. Vosse (1989). Incremental  Syntact ic  Tree Formation in Human Sentence Processing: a Cognit ive Architecture Based on Activation Decay and Simulated Annealing. Connection Science, 1(3), 273-290. Kapcprzyk, J. (1992). Fuzzy Sets and Fuzzy Logic. In Shapiro, S. (gd.) The Encyclopedia of Artificial Intelligence. John Wiley 8z Sons., New York. Marcus, M., B. Santorini, and M Markiewicz (1993). Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2), 1993. Shieber, S. (1986). An Introduction to Unification-Based Approaches to Grammar. CSLI Lecture Notes, Chicago University Press, Chicago. Stowe, L. (1989). Thematic Structures and Sentence Comprehension. In Carlsonp G. and M. Tanenhaus (Eds.) Linguistic Structure in Language Processing Kluwer Academic Publishers. Trueswell, J., M. T~nnenh&us, S. Garnsey (1994). Semantic Influences on Parsing: Use of Thematic Role Information in Syntactic Ambiguity B.esolutlon. Journal of Memory and Language, 33, In Press. Weischedel, R., B.. Schwartz, J. Palmucci, M. Meteer, and L. P~amshaw (1993). Coping with Ambiguity and Unknown Words through Probabilistic Models. Computational Linguistics, 19(2), 359-382. =In fact, the utility of byis neutralized in the case of POS tagging, since prepositions are uniformly tagged (e.g., using the tag IN in the Penn Treebank; see Marcus et al, 1993). 3These studies attempt o establish thematic patterns by asking large numbers of subjects to answer questions like \"How typical is it for a van to be recognized by someone?\" with a rating between 1 and 7. 315 "}
{"date": 2014, "text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1073?1083, Baltimore, Maryland, USA, June 23-25 2014. c ?2014 Association for Computational Linguistics Weak semantic context helps phonetic learning in a model of infant language acquisition Stella Frank sfrank@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Naomi H. Feldman nhf@umd.edu Department of Linguistics University of Maryland College Park, MD, 20742, USA Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Abstract Learning phonetic categories is one of the first steps to learning a language, yet is hard to do using only distributional phonetic information. Semantics could potentially be useful, since words with different meanings have distinct phonetics, but it is unclear how many word meanings are known to infants learning phonetic categories. We show that attending to a weaker source of semantics, in the form of a distribution over topics in the current context, can lead to improvements in phonetic category learning. In our model, an extension of a previous model of joint word-form and phonetic category inference, the probability of word-forms is topic-dependent, enabling the model to find significantly better phonetic vowel categories and word-forms than a model with no semantic knowledge. 1 Introduction Infants begin learning the phonetic categories of their native language in their first year (Kuhl et al, 1992; Polka and Werker, 1994; Werker and Tees, 1984). In theory, semantic information could offer a valuable cue for phoneme induction 1 by helping infants distinguish between minimal pairs, as linguists do (Trubetzkoy, 1939). However, due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories (see Swingley, 2009 for a review), most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information (de Boer and Kuhl, 2003; Dillon et al, 2013; 1 The models in this paper do not distinguish between phonetic and phonemic categories, since they do not capture phonological processes (and there are also none present in our synthetic data). We thus use the terms interchangeably. Feldman et al, 2013a; McMurray et al, 2009; Vallabha et al, 2007). Models without any semantic information are likely to underestimate infants? ability to learn phonetic categories. Infants learn language in the wild, and quickly attune to the fact that words have (possibly unknown) meanings. The extent of infants? semantic knowledge is not yet known, but existing evidence shows that six-month-olds can associate some words with their referents (Bergelson and Swingley, 2012; Tincoff and Jusczyk, 1999, 2012), leverage non-acoustic contexts such as objects or articulations to distinguish similar sounds (Teinonen et al, 2008; Yeung and Werker, 2009), and map meaning (in the form of objects or images) to new word-forms in some laboratory settings (Friedrich and Friederici, 2011; Gogate and Bahrick, 2001; Shukla et al, 2011). These findings indicate that young infants are sensitive to co-occurrences between linguistic stimuli and at least some aspects of the world. In this paper we explore the potential contribution of semantic information to phonetic learning by formalizing a model in which learners attend to the word-level context in which phones appear (as in the lexical-phonetic learning model of Feldman et al, 2013a) and also to the situations in which word-forms are used. The modeled situations consist of combinations of categories of salient activities or objects, similar to the activity contexts explored by Roy et al (2012), e.g.,?getting dressed? or ?eating breakfast?. We assume that child learners are able to infer a representation of the situational context from their non-linguistic environment. However, in our simulations we approximate the environmental information by running a topic model (Blei et al, 2003) over a corpus of childdirected speech to infer a topic distribution for each situation. These topic distributions are then used as input to our model to represent situational contexts. The situational information in our model is simi1073 lar to that assumed by theories of cross-situational word learning (Frank et al, 2009; Smith and Yu, 2008; Yu and Smith, 2007), but our model does not require learners to map individual words to their referents. Even in the absence of word-meaning mappings, situational information is potentially useful because similar-sounding words uttered in similar situations are more likely to be tokens of the same lexeme (containing the same phones) than similarsounding words uttered in different situations. In simulations of vowel learning, inspired by Vallabha et al (2007) and Feldman et al (2013a), we show a clear improvement over previous models in both phonetic and lexical (word-form) categorization when situational context is used as an additional source of information. This improvement is especially noticeable when the word-level context is providing less information, arguably the more realistic setting. These results demonstrate that relying on situational co-occurrence can improve phonetic learning, even if learners do not yet know the meanings of individual words. 2 Background and overview of models Infants attend to distributional characteristics of their input (Maye et al, 2002, 2008), leading to the hypothesis that phonetic categories could be acquired on the basis of bottom-up distributional learning alone (de Boer and Kuhl, 2003; Vallabha et al, 2007; McMurray et al, 2009). However, this would require sound categories to be well separated, which often is not the case?for example, see Figure 1, which shows the English vowel space that is the focus of this paper. Recent work has investigated whether infants could overcome such distributional ambiguity by incorporating top-down information, in particular, the fact that phones appear within words. At six months, infants begin to recognize word-forms such as their name and other frequently occurring words (Mandel et al, 1995; Jusczyk and Hohne, 1997), without necessarily linking a meaning to these forms. This ?protolexicon? can help differentiate phonetic categories by adding word contexts in which certain sound categories appear (Swingley, 2009; Feldman et al, 2013b). To explore this idea further, Feldman et al (2013a) implemented the Lexical-Distributional (LD) model, which jointly learns a set of phonetic vowel categories and a set of word-forms containing those categories. Simulations showed that the use of lexical context greatly 500100015002000250030003500 F2 200 400 600 800 1000 1200 F1 oa uw aw oo uh ah er ehae ihei iy Figure 1: The English vowel space (generated from Hillenbrand et al (1995), see Section 6.2), plotted using the first two formants. improved phonetic learning. Our own Topic-Lexical-Distributional (TLD) model extends the LD model to include an additional type of context: the situations in which words appear. To motivate this extension and clarify the differences between the models, we now provide a high-level overview of both models; details are given in Sections 3 and 4. 2.1 Overview of LD model Both the LD and TLD models are computationallevel models of phonetic (specifically, vowel) categorization where phones (vowels) are presented to the model in the context of words. 2 The task is to infer a set of phonetic categories and a set of lexical items on the basis of the data observed for each word token x i . In the original LD model, the observations for token x i are its frame f i , which consists of a list of consonants and slots for vowels, and the list of vowel tokensw i . (The TLD model includes additional observations, described below.) A single vowel token, w ij , is a two dimensional vector representing the first two formants (peaks in the frequency spectrum, ordered from lowest to highest). For example, a token of the word kitty would have the frame f i = k t , containing two consonant phones, /k/ and /t/, with two vowel phone slots in between, and two vowel formant vectors, 2 For a related model that also tackles the word segmentation problem, see Elsner et al (2013). In a model of phonological learning, Fourtassi and Dupoux (submitted) show that semantic context information similar to that used here remains useful despite segmentation errors. 1074 wi0 = [464, 2294] and w i1 = [412, 2760]. 3 Given the data, the model must assign each vowel token to a vowel category, w ij = c. Both the LD and the TLD models do this using intermediate lexemes, `, which contain vowel category assignments, v `j = c, as well as a frame f ` . If a word token is assigned to a lexeme, x i = `, the vowels within the word are assigned to that lexeme?s vowel categories, w ij = v `j = c. 4 The word and lexeme frames must match, f i = f ` . Lexical information helps with phonetic categorization because it can disambiguate highly overlapping categories, such as the ae and eh categories in Figure 1. A purely distributional learner who observes a cluster of data points in the ae-eh region is likely to assume all these points belong to a single category because the distributions of the categories are so similar. However, a learner who attends to lexical context will notice a difference: contexts that only occur with ae will be observed in one part of the ae-eh region, while contexts that only occur with eh will be observed in a different (though partially overlapping) space. The learner then has evidence of two different categories occurring in different sets of lexemes. Simulations with the LD model show that using lexical information to constrain phonetic learning can greatly improve categorization accuracy (Feldman et al, 2013a), but it can also introduce errors. When two word tokens contain the same consonant frame but different vowels (i.e., minimal pairs), the model is more likely to categorize those two vowels together. Thus, the model has trouble distinguishing minimal pairs. Although young children also have trouble with minimal pairs (Stager and Werker, 1997; Thiessen, 2007), the LD model may overestimate the degree of the problem. We hypothesize that if a learner is able to associate words with the contexts of their use (as children likely are), this could provide a weak source of information for disambiguating minimal pairs even without knowing their exact meanings. That is, if the learner hears kV 1 t and kV 2 t in different situational contexts, they are likely to be different lexical items (and V 1 and V 2 different phones), despite the lexical similarity between them. 3 In simulations we also experiment with frames in which consonants are not represented perfectly. 4 The notation is overloaded: w ij refers both to the vowel formants and the vowel category assignments, and x i refers to both the token identity and its assignment to a lexeme. 2.2 Overview of TLD model To demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model. Each situation h is associated with a mixture of topics ? h , which is assumed to be observed. Thus, for the ith token in situation h, denoted x hi , the observed data will be its frame f hi , vowels w hi , and topic vector ? h . From an acquisition perspective, the observed topic distribution represents the child?s knowledge of the context of the interaction: she can distinguish bathtime from dinnertime, and is able to recognize that some topics appear in certain contexts (e.g. animals on walks, vegetables at dinnertime) and not in others (few vegetables appear at bathtime). We assume that the child would learn these topics from observing the world around her and the co-occurrences of entities and activities in the world. Within any given situation, there might be a mixture of different (actual or possible) topics that are salient to the child. We assume further that as the child learns the language, she will begin to associate specific words with each topic as well. Thus, in the TLD model, the words used in a situation are topic-dependent, implying meaning, but without pinpointing specific referents. Although the model observes the distribution of topics in each situation (corresponding to the child observing her non-linguistic environment), it must learn to associate each (phonetically and lexically ambiguous) word token with a particular topic from that distribution. The occurrence of similar-sounding words in different situations with mostly non-overlapping topics will provide evidence that those words belong to different topics and that they are therefore different lexemes. Conversely, potential minimal pairs that occur in situations with similar topic distributions are more likely to belong to the same topic and thus the same lexeme. Although we assume that children infer topic distributions from the non-linguistic environment, we will use transcripts from CHILDES to create the word/phone learning input for our model. These transcripts are not annotated with environmental context, but Roy et al (2012) found that topics learned from similar transcript data using a topic model were strongly correlated with immediate activities and contexts. We therefore obtain the topic distributions used as input to the TLD model by 1075 training an LDA topic model (Blei et al, 2003) on a superset of the child-directed transcript data we use for lexical-phonetic learning, dividing the transcripts into small sections (the ?documents? in LDA) that serve as our distinct situations h. As noted above, the learned document-topic distributions ? are treated as observed variables in the TLD model to represent the situational context. The topic-word distributions learned by LDA are discarded, since these are based on the (correct and unambiguous) words in the transcript, whereas the TLD model is presented with phonetically ambiguous versions of these word tokens and must learn to disambiguate them and associate them with topics. 3 Lexical-Distributional Model In this section we describe more formally the generative process for the LD model (Feldman et al, 2013a), a joint Bayesian model over phonetic categories and a lexicon, before describing the TLD extension in the following section. The set of phonetic categories and the lexicon are both modeled using non-parametric Dirichlet Process priors, which return a potentially infinite number of categories or lexemes. A DP is parametrized as DP (?,H), where ? is a real-valued hyperparameter andH is a base distribution.H may be continuous, as when it generates phonetic categories in formant space, or discrete, as when it generates lexemes as a list of phonetic categories. A draw from a DP, G ? DP (?,H), returns a distribution over a set of draws from H , i.e., a discrete distribution over a set of categories or lexemes generated by H . In the mixture model setting, the category assignments are then generated from G, with the datapoints themselves generated by the corresponding components fromH . IfH is infinite, the support of the DP is likewise infinite. During inference, we marginalize over G. 3.1 Phonetic Categories: IGMM Following previous models of vowel learning (de Boer and Kuhl, 2003; Vallabha et al, 2007; McMurray et al, 2009; Dillon et al, 2013) we assume that vowel tokens are drawn from a Gaussian mixture model. The Infinite Gaussian Mixture Model (IGMM) (Rasmussen, 2000) includes a DP prior, as described above, in which the base distribution H C generates multivariate Gaussians drawn from a Normal Inverse-Wishart prior. 5 Each observation, a formant vector w ij , is drawn from the Gaussian corresponding to its category assignment c ij : ? c ,? c ? H C = NIW(? 0 ,? 0 , ? 0 ) (1) G C ? DP (? c , H C ) (2) c ij ? G C (3) w ij |c ij = c ? N(? c ,? c ) (4) The above model generates a category assignment c ij for each vowel token w ij . This is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level. 3.2 Lexicon In the LD model, vowel phones appear within words drawn from the lexicon. Each such lexeme is represented as a frame plus a list of vowel categories v ` . Lexeme assignments for each token are drawn from a DP with a lexicon-generating base distribution H L . The category for each vowel token in the word is determined by the lexeme; the formant values are drawn from the corresponding Gaussian as in the IGMM: G L ? DP (? l , H L ) (5) x i = ` ? G L (6) w ij |v `j = c ? N(? c ,? c ) (7) H L generates lexemes by first drawing the number of phones from a geometric distribution and the number of consonant phones from a binomial distribution. The consonants are then generated from a DP with a uniform base distribution (but note they are fixed at inference time, i.e., are observed categorically), while the vowel phones v ` are generated by the IGMM DP above, v `j ? G C . Note that two draws from H L may result in identical lexemes; these are nonetheless considered to be separate (homophone) lexemes. 4 Topic-Lexical-Distributional Model The TLD model retains the IGMM vowel phone component, but extends the lexicon of the LD model by adding topic-specific lexicons, which capture the notion that lexeme probabilities are topicdependent. Specifically, the TLD model replaces 5 This compound distribution is equivalent to ? c ? IW(? 0 , ? 0 ), ? c |? c ? N(? 0 , ? c ? 0 ) 1076 the Dirichlet Process lexicon with a Hierarchical Dirichlet Process (HDP; Teh (2006)). In the HDP lexicon, a top-level global lexicon is generated as in the LD model. Topic-specific lexicons are then drawn from the global lexicon, containing a subset of the global lexicon (but since the size of the global lexicon is unbounded, so are the topic-specific lexicons). These topic-specific lexicons are used to generate the tokens in a similar manner to the LD model. There are a fixed number of lower level topic-lexicons; these are matched to the number of topics in the LDA model used to infer the topic distributions (see Section 6.4). More formally, the global lexicon is generated as a top-level DP: G L ? DP (? l , H L ) (see Section 3.2; remember H L includes draws from the IGMM over vowel categories). G L is in turn used as the base distribution in the topic-level DPs, G k ? DP (? k , G L ). In the Chinese Restaurant Franchise metaphor often used to describe HDPs, G L is a global menu of dishes (lexemes). The topicspecific lexicons are restaurants, each with its own distribution over dishes; this distribution is defined by seating customers (word tokens) at tables, each of which serves a single dish from the menu: all tokens x at the same table t are assigned to the same lexeme ` t . Inference (Section 5) is defined in terms of tables rather than lexemes; if multiple tables draw the same dish from G L , tokens at these tables share a lexeme. In the TLD model, tokens appear within situations, each of which has a distribution over topics ? h . Each token x hi has a co-indexed topic assignment variable, z hi , drawn from ? h , designating the topic-lexicon from which the table for x hi is to be drawn. The formant values for w hij are drawn in the same way as in the LD model, given the lexeme assignment at x hi . This results in the following model, shown in Figure 2: G L ? DP (? l , H L ) (8) G k ? DP (? k , G L ) (9) z hi ?Mult(? h ) (10) x hi = t|z hi = k ? G k (11) w hij |x hi = t, v ` t j = c ? N(? c ,? c ) (12) 5 Inference: Gibbs Sampling We use Gibbs sampling to infer three sets of variables in the TLD model: assignments to vowel categories in the lexemes, assignments of tokens to ? 0 , ? 0 ,? 0 , ? 0 H C G C ? c ? c ,? c ? ? H L G L ? l G k ? k K z hi x hi f hi w hij |w hi | |x h | D ? h Figure 2: TLD model, depicting, from left to right, the IGMM component, the LD lexicon component, the topic-specific lexicons, and finally the token x hi , appearing in document h, with observed vowel formants w hij and frame f hi . The lexeme assignment x hi and the topic assignment z hi are inferred, the latter using the observed documenttopic distribution ? h . Note that f i is deterministic given the lexeme assignment. Squared nodes depict hyperparameters. ? is the set of hyperparameters used by H L when generating lexical items (see Section 3.2). topics, and assignments of tokens to tables (from which the assignment to lexemes can be read off). 5.1 Sampling lexeme vowel categories Each vowel in the lexicon must be assigned to a category in the IGMM. The posterior probability of a category assignment is composed of the DP prior over categories and the likelihood of the observed vowels belonging to that category. We use w `j to denote the set of vowel formants at position j in words that have been assigned to lexeme `. Then, P (v `j = c|w,x, ` \\` ) ? P (v `j = c|` \\` )p(w `j |v `j = c,w \\`j ) (13) The first (DP prior) factor is defined as: P (v `j = c|v \\`j ) = { n c P c n c +? c if c exists ? c P c n c +? c if c new (14) where n c is the number of other vowels in the lexicon, v \\lj , assigned to category c. Note that there is always positive probability of creating a new category. The likelihood of the vowels is calculated by marginalizing over all possible means and variances of the Gaussian category parameters, given 1077 the NIW prior. For a single point (if |w `j | = 1), this predictive posterior is in the form of a Student-t distribution; for the more general case see Feldman et al (2013a), Eq. B3. 5.2 Sampling table & topic assignments We jointly sample x and z, the variables assigning tokens to tables and topics. Resampling the table assignment includes the possibility of changing to a table with a different lexeme or drawing a new table with a previously seen or novel lexeme. The joint conditional probability of a table and topic assignment, given all other current token assignments, is: P (x hi = t, z hi = k|w hi , ? h , t \\hi , `,w \\hi ) = P (k|? h )P (t|k, ` t , t \\hi ) ? c?C p(w hi? |v ` t ? = c,w \\hi ) (15) The first factor, the prior probability of topic k in document h, is given by ? hk obtained from the LDA. The second factor is the prior probability of assigning word x i to table t with lexeme ` given topic k. It is given by the HDP, and depends on whether the table t exists in the HDP topic-lexicon for k and, likewise, whether any table in the topiclexicon has the lexeme `: P (t|k, `, t \\hi ) ? ? ? ? ? ? n kt n k +? k if t in k ? k n k +? k m ` m+? l if t new, ` known ? k n k +? k ? ` m+? l if t and ` new (16) Here n kt is the number of other tokens at table t, n k are the total number of tokens in topic k, m ` is the number of tables across all topics with the lexeme `, and m is the total number of tables. The third factor, the likelihood of the vowel formantsw hi in the categories given by the lexeme v l , is of the same form as the likelihood of vowel categories when resampling lexeme vowel assignments. However, here it is calculated over the set of vowels in the token assigned to each vowel category (i.e., the vowels at indices where v ` t ? = c). For a new lexeme, we approximate the likelihood using 100 samples drawn from the prior, each weighted by ?/100 (Neal, 2000). 5.3 Hyperparameters The three hyperparameters governing the HDP over the lexicon, ? l and ? k , and the DP over vowel categories, ? c , are estimated using a slice sampler. The remaining hyperparameters for the vowel category and lexeme priors are set to the same values used by Feldman et al (2013a). 6 Experiments 6.1 Corpus We test our model on situated child directed speech, taken from the C1 section of the Brent corpus in CHILDES (Brent and Siskind, 2001; MacWhinney, 2000). This corpus consists of transcripts of speech directed at infants between the ages of 9 and 15 months, captured in a naturalistic setting as parent and child went about their day. This ensures variability of situations. Utterances with unintelligible words or quotes are removed. We restrict the corpus to content words by retaining only words tagged as adj, n, part and v (adjectives, nouns, particles, and verbs). This is in line with evidence that infants distinguish content and function words on the basis of acoustic signals (Shi and Werker, 2003). Vowel categorization improves when attending only to more prosodically and phonologically salient tokens (Adriaans and Swingley, 2012), which generally appear within content, not function words. The final corpus consists of 13138 tokens and 1497 word types. 6.2 Hillenbrand Vowels The transcripts do not include phonetic information, so, following Feldman et al (2013a), we synthesize the formant values using data from Hillenbrand et al (1995). This dataset consists of a set of 1669 manually gathered formant values from 139 American English speakers (men, women and children) for 12 vowels. For each vowel category, we construct a Gaussian from the mean and covariance of the datapoints belonging to that category, using the first and second formant values measured at steady state. We also construct a second dataset using only datapoints from adult female speakers. Each word in the dataset is converted to a phonemic representation using the CMU pronunciation dictionary, which returns a sequence of Arpabet phoneme symbols. If there are multiple possible pronunciations, the first one is used. Each vowel phoneme in the word is then replaced by formant values drawn from the corresponding Hillenbrand Gaussian for that vowel. 1078 6.3 Merging Consonant Categories The Arpabet encoding used in the phonemic representation includes 24 consonants. We construct datasets both using the full set of consonants?the ?C24? dataset?and with less fine-grained consonant categories. Distinguishing all consonant categories assumes perfect learning of consonants prior to vowel categorization and is thus somewhat unrealistic (Polka and Werker, 1994), but provides an upper limit on the information that word-contexts can give. In the ?C15? dataset, the voicing distinction is collapsed, leaving 15 consonant categories. The collapsed categories are B/P, G/K, D/T, CH/JH, V/F, TH/DH, S/Z, SH/ZH, R/L while HH, M, NG, N, W, Y remain separate phonemes. This dataset mirrors the finding in Mani and Plunkett (2010) that 12 month old infants are not sensitive to voicing mispronunciations. The ?C6? dataset distinguishes between only 6 coarse consonant phonemes, corresponding to stops (B,P,G,K,D,T), affricates (CH,JH), fricatives (V, F, TH, DH, S, Z, SH, ZH, HH), nasals (M, NG, N), liquids (R, L), and semivowels/glides (W, Y). This dataset makes minimal assumptions about the category categories that infants could use in this learning setting. Decreasing the number of consonants increases the ambiguity in the corpus: bat not only shares a frame (b t) with boat and bite, but also, in the C15 dataset, with put, pad and bad (b/p d/t), and in the C6 dataset, with dog and kite, among many others (STOP STOP). Table 1 shows the percentage of types and tokens that are ambiguous in each dataset, that is, words in frames that match multiple wordtypes. Note that we always evaluate against the gold word identities, even when these are not distinguished in the model?s input. These datasets are intended to evaluate the degree of reliance on consonant information in the LD and TLD models, and to what extent the topics in the TLD model can replace this information. 6.4 Topics The input to the TLD model includes a distribution over topics for each situation, which we infer in advance from the full Brent corpus (not only the C1 subset) using LDA. Each transcript in the Brent corpus captures about 75 minutes of parent-child interaction, and thus multiple situations will be included in each file. The transcripts do not delimit Dataset C24 C15 C6 Input Types 1487 1426 1203 Frames 1259 1078 702 Ambig Types % 27.2 42.0 80.4 Ambig Tokens % 41.3 56.9 77.2 Table 1: Corpus statistics showing the increasing amount of ambiguity as consonant categories are merged. Input types are the number of word types with distinct input representations (as opposed to gold orthographic word types, of which there are 1497). Ambiguous types and tokens are those with frames that match multiple (orthographic) word types. situations, so we do this somewhat arbitrarily by splitting each transcript after 50 CDS utterances, resulting in 203 situations for the Brent C1 dataset. As well as function words, we also remove the five most frequent content words (be, go, get, want, come). On average, situations are only 59 words long, reflecting the relative lack of content words in CDS utterances. We infer 50 topics for this set of situations using the mallet toolkit (McCallum, 2002). Hyperparameters are inferred, which leads to a dominant topic that includes mainly light verbs (have, let, see, do). The other topics are less frequent but capture stronger semantic meaning (e.g. yummy, peach, cookie, daddy, bib in one topic, shoe, let, put, hat, pants in another). The word-topic assignments are used to calculate unsmoothed situation-topic distributions ? used by the TLD model. 6.5 Evaluation We evaluate against adult categories, i.e., the ?goldstandard?, since all learners of a language eventually converge on similar categories. (Since our model is not a model of the learning process, we do not compare the infant learning process to the learning algorithm.) We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure (VM; Rosenberg and Hirschberg, 2007). 6 VM is the harmonic mean of two components, similar to F-score, where the components (VC and VH) are measures of cross entropy between the gold and model categorization. 6 Other clustering measures, such as 1-1 matching and pairwise precision and recall (accuracy and completeness) showed the same trends, but VM has been demonstrated to be the most stable measure when comparing solutions with varying numbers of clusters (Christodoulopoulos et al, 2010). 1079 24 Cons 15 Cons 6 Cons 75 80 85 90 Dataset V M LD-all TLD-all LD-w TLD-w Figure 3: Vowel evaluation. ?all? refers to datasets with vowels synthesized from all speakers, ?w? to datasets with vowels synthesized from adult female speakers? vowels. The bars show a 95% Confidence Interval based on 5 runs. IGMM-all results in a VM score of 53.9 (CI=0.5); IGMM-w has a VM score of 65.0 (CI=0.2), not shown. For vowels, VM measures how well the inferred phonetic categorizations match the gold categories; for lexemes, it measures whether tokens have been assigned to the same lexemes both by the model and the gold standard. Words are evaluated against gold orthography, so homophones, e.g. hole and whole, are distinct gold words. 6.6 Results We compare all three models?TLD, LD, and IGMM?on the vowel categorization task, and TLD and LD on the lexical categorization task (since IGMM does not infer a lexicon). The datasets correspond to two sets of conditions: firstly, either using vowel categories synthesized from all speakers or only adult female speakers, and secondly, varying the coarseness of the observed consonant categories. Each condition (model, vowel speakers, consonant set) is run five times, using 1500 iterations of Gibbs sampling with hyperparameter sampling. Overall, we find that TLD outperforms the other models in both tasks, across all conditions. Vowel categorization results are shown in Figure 3. IGMM performs substantially worse than both TLD and LD, with scores more than 30 points lower than the best results for these models, clearly showing the value of the protolexicon and repli500100015002000250030003500 F2 200 400 600 800 1000 1200 F1 Figure 4: Vowels found by the TLD model; supervowels are indicated in red. The gold-standard vowels are shown in gold in the background but are mostly overlapped by the inferred categories. cating the results found by Feldman et al (2013a) on this dataset. Furthermore, TLD consistently outperforms the LD model, finding better phonetic categories, both for vowels generated from the combined categories of all speakers (?all?) and vowels generated from adult female speakers only (?w?), although the latter are clearly much easier for both models to learn. Both models perform less well when the consonant frames provide less information, but the TLD model performance degrades less than the LD performance. Both the TLD and the LD models find ?supervowel? categories, which cover multiple vowel categories and are used to merge minimal pairs into a single lexical item. Figure 4 shows example vowel categories inferred by the TLD model, including two supervowels. The TLD supervowels are used much less frequently than the supervowels found by the LD model, containing, on average, only twothirds as many tokens. Figure 5 shows that TLD also outperforms LD on the lexeme/word categorization task. Again performance decreases as the consonant categories become coarser, but the additional semantic information in the TLD model compensates for the lack of consonant information. In the individual components of VM, TLD and LD have similar VC (?recall?), but TLD has higher VH (?precision?), demonstrating that the semantic information given by the topics can separate potentially ambiguous words, as hypothesized. Overall, the contextual semantic information 1080 24 Cons 15 Cons 6 Cons 92 94 96 98 100 Dataset V M LD-all TLD-all LD-w TLD-w Figure 5: Lexeme evaluation. ?all? refers to datasets with vowels synthesized from all speakers, ?w? to datasets with vowels synthesized from adult female speakers? vowels. added in the TLD model leads to both better phonetic categorization and to a better protolexicon, especially when the input is noisier, using degraded consonants. Since infants are not likely to have perfect knowledge of phonetic categories at this stage, semantic information is a potentially rich source of information that could be drawn upon to offset noise from other domains. The form of the semantic information added in the TLD model is itself quite weak, so the improvements shown here are in line with what infant learners could achieve. 7 Conclusion Language acquisition is a complex task, in which many heterogeneous sources of information may be useful. In this paper, we investigated whether contextual semantic information could be of help when learning phonetic categories. We found that this contextual information can improve phonetic learning performance considerably, especially in situations where there is a high degree of phonetic ambiguity in the word-forms that learners hear. This suggests that previous models that have ignored semantic information may have underestimated the information that is available to infants. Our model illustrates one way in which language learners might harness the rich information that is present in the world without first needing to acquire a full inventory of word meanings. The contextual semantic information that the TLD model tracks is similar to that potentially used in other linguistic learning tasks. Theories of cross-situational word learning (Smith and Yu, 2008; Yu and Smith, 2007) assume that sensitivity to situational co-occurrences between words and non-linguistic contexts is a precursor to learning the meanings of individual words. Under this view, contextual semantics is available to infants well before they have acquired large numbers of semantic minimal pairs. However, recent experimental evidence indicates that learners do not always retain detailed information about the referents that are present in a scene when they hear a word (Medina et al, 2011; Trueswell et al, 2013). This evidence poses a direct challenge to theories of cross-situational word learning. Our account does not necessarily require learners to track co-occurrences between words and individual objects, but instead focuses on more abstract information about salient events and topics in the environment; it will be important to investigate to what extent infants encode this information and use it in phonetic learning. Regardless of the specific way in which infants encode semantic information, our method of adding this information by using LDA topics from transcript data was shown to be effective. This method is practical because it can approximate semantic information without relying on extensive manual annotation. The LD model extended the phonetic categorization task by adding word contexts; the TLD model presented here goes even further, adding larger situational contexts. Both forms of top-down information help the low-level task of classifying acoustic signals into phonetic categories, furthering a holistic view of language learning with interaction across multiple levels. Acknowledgments This work was supported by EPSRC grant EP/H050442/1 and a James S. McDonnell Foundation Scholar Award to the final author. References Frans Adriaans and Daniel Swingley. Distributional learning of vowel categories is supported by prosody in infant-directed speech. In Proceedings of the 34th Annual Conference of the Cognitive Science Society (CogSci), 2012. E. Bergelson and D. Swingley. At 6-9 months, human infants know the meanings of many 1081 common nouns. Proceedings of the National Academy of Sciences, 109(9):3253?3258, Feb 2012. David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. Hierarchical topic models and the nested Chinese restaurant process. In Advances in Neural Information Processing Systems 16, 2003. Michael R. Brent and Jeffrey M. Siskind. The role of exposure to isolated words in early vocabulary development. Cognition, 81(2):B33?B44, 2001. Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 575?584, Cambridge, MA, October 2010. Association for Computational Linguistics. Bart de Boer and Patricia K. Kuhl. Investigating the role of infant-directed speech with a computer model. Acoustics Research Letters Online, 4(4): 129, 2003. Brian Dillon, Ewan Dunbar, and William Idsardi. A single-stage approach to learning phonological categories: Insights from Inuktitut. Cognitive Science, 37(2):344?377, Mar 2013. Micha Elsner, Sharon Goldwater, Naomi Feldman, and Frank Wood. A cognitive model of early lexical acquisition with phonetic variability. In Proceedings of the 18th Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013. Naomi H. Feldman, Thomas L. Griffiths, Sharon Goldwater, and James L. Morgan. A role for the developing lexicon in phonetic category acquisition. Psychological Review, 2013a. Naomi H. Feldman, Emily B. Myers, Katherine S. White, Thomas L. Griffiths, and James L. Morgan. Word-level information influences phonetic learning in adults and infants. Cognition, 127(3): 427?438, 2013b. Abdellah Fourtassi and Emmanuel Dupoux. A rudimentary lexicon and semantics help bootstrap phoneme acquisition. Submitted. Michael C. Frank, Noah D. Goodman, and Joshua B. Tenenbaum. Using speakers? referential intentions to model early cross-situational word learning. Psychological Science, 20(5): 578?585, 2009. Manuela Friedrich and Angela D. Friederici. Word learning in 6-month-olds: Fast encoding?weak retention. Journal of Cognitive Neuroscience, 23 (11):3228?3240, Nov 2011. Lakshmi J. Gogate and Lorraine E. Bahrick. Intersensory redundancy and 7-month-old infants? memory for arbitrary syllable-object relations. Infancy, 2(2):219?231, Apr 2001. J. Hillenbrand, L. A. Getty, M. J. Clark, and K. Wheeler. Acoustic characteristics of American English vowels. Journal of the Acoustical Society of America, 97(5 Pt 1):3099?3111, May 1995. P. W. Jusczyk and Elizabeth A. Hohne. Infants? memory for spoken words. Science, 277(5334): 1984?1986, Sep 1997. Patricia K. Kuhl, Karen A. Williams, Francisco Lacerda, Kenneth N. Stevens, and Bjorn Lindblom. Linguistic experience alters phonetic perception in infants by 6 months of age. Science, 255(5044):606?608, 1992. Brian MacWhinney. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates, 2000. D. R. Mandel, P. W. Jusczyk, and D. B. Pisoni. Infants? recognition of the sound patterns of their own names. Psychological Science, 6(5):314? 317, Sep 1995. Nivedita Mani and Kim Plunkett. Twelve-montholds know their cups from their keps and tups. Infancy, 15(5):445470, Sep 2010. Jessica Maye, Daniel J. Weiss, and Richard N. Aslin. Statistical phonetic learning in infants: facilitation and feature generalization. Developmental Science, 11(1):122?134, Jan 2008. Jessica Maye, Janet F Werker, and LouAnn Gerken. Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82(3):B101?B111, Jan 2002. Andrew McCallum. MALLET: A machine learning for language toolkit, 2002. Bob McMurray, Richard N. Aslin, and Joseph C. Toscano. Statistical learning of phonetic categories: insights from a computational approach. Developmental Science, 12(3):369?378, May 2009. 1082 Tamara Nicol Medina, Jesse Snedeker, John C. Trueswell, and Lila R. Gleitman. How words can and cannot be learned by observation. Proceedings of the National Academy of Sciences, 108(22):9014?9019, 2011. Radford Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9: 249?265, 2000. Linda Polka and Janet F. Werker. Developmental changes in perception of nonnative vowel contrasts. Journal of Experimental Psychology: Human Perception and Performance, 20(2):421? 435, 1994. Carl Rasmussen. The infinite Gaussian mixture model. In Advances in Neural Information Processing Systems 13, 2000. Andrew Rosenberg and Julia Hirschberg. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing (EMNLP), 2007. Brandon C. Roy, Michael C. Frank, and Deb Roy. Relating activity contexts to early word learning in dense longitudinal data. In Proceedings of the 34th Annual Conference of the Cognitive Science Society (CogSci), 2012. Rushen Shi and Janet F. Werker. The basis of preference for lexical words in 6-month-old infants. Developmental Science, 6(5):484?488, 2003. M. Shukla, K. S. White, and R. N. Aslin. Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants. Proceedings of the National Academy of Sciences, 108 (15):6038?6043, Apr 2011. Linda B. Smith and Chen Yu. Infants rapidly learn word-referent mappings via cross-situational statistics. Cognition, 106(3):1558?1568, 2008. Christine L. Stager and Janet F. Werker. Infants listen for more phonetic detail in speech perception than in word-learning tasks. Nature, 388: 381?382, 1997. D. Swingley. Contributions of infant word learning to language development. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1536):3617?3632, Nov 2009. Yee Whye Teh. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), pages 985 ? 992, Sydney, 2006. Tuomas Teinonen, Richard N. Aslin, Paavo Alku, and Gergely Csibra. Visual speech contributes to phonetic learning in 6-month-old infants. Cognition, 108:850?855, 2008. Erik D. Thiessen. The effect of distributional information on children?s use of phonemic contrasts. Journal of Memory and Language, 56(1):16?34, Jan 2007. R. Tincoff and P. W. Jusczyk. Some beginnings of word comprehension in 6-month-olds. Psychological Science, 10(2):172?175, Mar 1999. Ruth Tincoff and Peter W. Jusczyk. Six-montholds comprehend words that refer to parts of the body. Infancy, 17(4):432444, Jul 2012. N. S. Trubetzkoy. Grundz?uge der Phonologie. Vandenhoeck und Ruprecht, G?ottingen, 1939. John C. Trueswell, Tamara Nicol Medina, Alon Hafri, and Lila R. Gleitman. Propose but verify: Fast mapping meets cross-situational word learning. Cognitive Psychology, 66:126?156, 2013. G. K. Vallabha, J. L. McClelland, F. Pons, J. F. Werker, and S. Amano. Unsupervised learning of vowel categories from infant-directed speech. Proceedings of the National Academy of Sciences, 104(33):13273?13278, Aug 2007. Janet F. Werker and Richard C. Tees. Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life. Infant Behavior and Development, 7:49?63, 1984. H. Henny Yeung and Janet F. Werker. Learning words? sounds before learning how words sound: 9-month-olds use distinct objects as cues to categorize speech information. Cognition, 113(2): 234?243, Nov 2009. Chen Yu and Linda B. Smith. Rapid word learning under uncertainty via cross-situational statistics. Psychological Science, 18(5):414?420, 2007. 1083 "}
{"date": 2008, "text": "Proceedings of ACL-08: HLT, pages 505?513, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics Randomized Language Models via Perfect Hash Functions David Talbot? School of Informatics University of Edinburgh 2 Buccleuch Place, Edinburgh, UK d.r.talbot@sms.ed.ac.uk Thorsten Brants Google Inc. 1600 Amphitheatre Parkway Mountain View, CA 94303, USA brants@google.com Abstract We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n-grams and their associated probabilities, backoff weights, or other parameters. The scheme can represent any standard n-gram model and is easily combined with existing model reduction techniques such as entropy-pruning. We demonstrate the space-savings of the scheme via machine translation experiments within a distributed language modeling framework. 1 Introduction Language models (LMs) are a core component in statistical machine translation, speech recognition, optical character recognition and many other areas. They distinguish plausible word sequences from a set of candidates. LMs are usually implemented as n-gram models parameterized for each distinct sequence of up to n words observed in the training corpus. Using higher-order models and larger amounts of training data can significantly improve performance in applications, however the size of the resulting LM can become prohibitive. With large monolingual corpora available in major languages, making use of all the available data is now a fundamental challenge in language modeling. Efficiency is paramount in applications such as machine translation which make huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers ?Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al, 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al, 2007), Golomb Coding (Church et al, 2007) and distributed language models that scale more readily (Brants et al, 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes parameters directly. It can be combined with any n-gram parameter estimation method and existing model reduction techniques such as entropy-based pruning. Parameters that are stored in the model are retrieved without error; however, false positives may occur whereby n-grams not in the model are incorrectly ?found? when requested. The false positive rate is determined by the space usage of the model. Our randomized language model is based on the Bloomier filter (Chazelle et al, 2004). We encode fingerprints (random hashes) of n-grams together with their associated probabilities using a perfect hash function generated at random (Majewski et al, 1996). Lookup is very efficient: the values of 3 cells in a large array are combined with the fingerprint of an n-gram. This paper focuses on machine translation. However, many of our findings should transfer to other applications of language modeling. 505 2 Scaling Language Models In statistical machine translation (SMT), LMs are used to score candidate translations in the target language. These are typically n-gram models that approximate the probability of a word sequence by assuming each token to be independent of all but n?1 preceding tokens. Parameters are estimated from monolingual corpora with parameters for each distinct word sequence of length l ? [n] observed in the corpus. Since the number of parameters grows somewhat exponentially with n and linearly with the size of the training corpus, the resulting models can be unwieldy even for relatively small corpora. 2.1 Scaling Strategies Various strategies have been proposed to scale LMs to larger corpora and higher-order dependencies. Model-based techniques seek to parameterize the model more efficiently (e.g. latent variable models, neural networks) or to reduce the model size directly by pruning uninformative parameters, e.g. (Stolcke, 1998), (Goodman and Gao, 2000). Representationbased techniques attempt to reduce space requirements by representing the model more efficiently or in a form that scales more readily, e.g. (Emami et al, 2007), (Brants et al, 2007), (Church et al, 2007). 2.2 Lossy Randomized Encodings A fundamental result in information theory (Carter et al, 1978) states that a random set of objects cannot be stored using constant space per object as the universe from which the objects are drawn grows in size: the space required to uniquely identify an object increases as the set of possible objects from which it must be distinguished grows. In language modeling the universe under consideration is the set of all possible n-grams of length n for given vocabulary. Although n-grams observed in natural language corpora are not randomly distributed within this universe no lossless data structure that we are aware of can circumvent this space-dependency on both the n-gram order and the vocabulary size. Hence as the training corpus and vocabulary grow, a model will require more space per parameter. However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). The space required in such a lossy encoding depends only on the range of values associated with the n-grams and the desired error rate, i.e. the probability with which two distinct n-grams are assigned the same fingerprint. 2.3 Previous Randomized LMs Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. While the approach results in significant space savings, working with corpus statistics, rather than n-gram probabilities directly, is computationally less efficient (particularly in a distributed setting) and introduces a dependency on the smoothing scheme used. It also makes it difficult to leverage existing model reduction strategies such as entropy-based pruning that are applied to final parameter estimates. In the next section we describe our randomized LM scheme based on perfect hash functions. This scheme can be used to encode any standard n-gram model which may first be processed using any conventional model reduction technique. 3 Perfect Hash-based Language Models Our randomized LM is based on the Bloomier filter (Chazelle et al, 2004). We assume the n-grams and their associated parameter values have been precomputed and stored on disk. We then encode the model in an array such that each n-gram?s value can be retrieved. Storage for this array is the model?s only significant space requirement once constructed.1 The model uses randomization to map n-grams to fingerprints and to generate a perfect hash function that associates n-grams with their values. The model can erroneously return a value for an n-gram that was never actually stored, but will always return the correct value for an n-gram that is in the model. We will describe the randomized algorithm used to encode n-gram parameters in the model, analyze the probability of a false positive, and explain how we construct and query the model in practice. 1Note that we do not store the n-grams explicitly and therefore that the model?s parameter set cannot easily be enumerated. 506 3.1 N -gram Fingerprints We wish to encode a set of n-gram/value pairs S = {(x1, v(x1)), (x2, v(x2)), . . . , (xN , v(xN ))} using an array A of size M and a perfect hash function. Each n-gram xi is drawn from some set of possible n-grams U and its associated value v(xi) from a corresponding set of possible values V . We do not store the n-grams and their probabilities directly but rather encode a fingerprint of each n-gram f(xi) together with its associated value v(xi) in such a way that the value can be retrieved when the model is queried with the n-gram xi. A fingerprint hash function f : U ? [0, B ? 1] maps n-grams to integers between 0 and B ? 1.2 The array A in which we encode n-gram/value pairs has addresses of size dlog2 Be hence B will determine the amount of space used per n-gram. There is a trade-off between space and error rate since the larger B is, the lower the probability of a false positive. This is analyzed in detail below. For now we assume only that B is at least as large as the range of values stored in the model, i.e. B ? |V|. 3.2 Composite Perfect Hash Functions The function used to associate n-grams with their values (Eq. (1)) combines a composite perfect hash function (Majewski et al, 1996) with the fingerprint function. An example is shown in Fig. 1. The composite hash function is made up of k independent hash functions h1, h2, . . . , hk where each hi : U ? [0,M ? 1] maps n-grams to locations in the array A. The lookup function is then defined as g : U ? [0, B ? 1] by3 g(xi) = f(xi)? ( k? i=1 A[hi(xi)] ) (1) where f(xi) is the fingerprint of n-gram xi and A[hi(xi)] is the value stored in location hi(xi) of the array A. Eq. (1) is evaluated to retrieve an n-gram?s parameter during decoding. To encode our model correctly we must ensure that g(xi) = v(xi) for all n-grams in our set S. Generating A to encode this 2The analysis assumes that all hash functions are random. 3We use ? to denote the exclusive bitwise OR operator. Figure 1: Encoding an n-gram?s value in the array. function for a given set of n-grams is a significant challenge described in the following sections. 3.3 Encoding n-grams in the model All addresses in A are initialized to zero. The procedure we use to ensure g(xi) = v(xi) for all xi ? S updates a single, unique location in A for each ngram xi. This location is chosen from among the k locations given by hj(xi), j ? [k]. Since the composite function g(xi) depends on the values stored at all k locations A[h1(xi)], A[h2(xi)], . . . , A[hk(xi)] in A, we must also ensure that once an n-gram xi has been encoded in the model, these k locations are not subsequently changed since this would invalidate the encoding; however, n-grams encoded later may reference earlier entries and therefore locations in A can effectively be ?shared? among parameters. In the following section we describe a randomized algorithm to find a suitable order in which to enter n-grams in the model and, for each n-gram xi, determine which of the k hash functions, say hj , can be used to update A without invalidating previous entries. Given this ordering of the n-grams and the choice of hash function hj for each xi ? S, it is clear that the following update rule will encode xi in the array A so that g(xi) will return v(xi) (cf. Eq.(1)) A[hj(xi)] = v(xi)? f(xi)? k? i=1?i6=j A[hi(xi)]. (2) 3.4 Finding an Ordered Matching We now describe an algorithm (Algorithm 1; (Majewski et al, 1996)) that selects one of the k hash 507 functions hj , j ? [k] for each n-gram xi ? S and an order in which to apply the update rule Eq. (2) so that g(xi) maps xi to v(xi) for all n-grams in S. This problem is equivalent to finding an ordered matching in a bipartite graph whose LHS nodes correspond to n-grams in S and RHS nodes correspond to locations in A. The graph initially contains edges from each n-gram to each of the k locations in A given by h1(xi), h2(xi), . . . , hk(xi) (see Fig. (2)). The algorithm uses the fact that any RHS node that has degree one (i.e. a single edge) can be safely matched with its associated LHS node since no remaining LHS nodes can be dependent on it. We first create the graph using the k hash functions hj , j ? [k] and store a list (degree one) of those RHS nodes (locations) with degree one. The algorithm proceeds by removing nodes from degree one in turn, pairing each RHS node with the unique LHS node to which it is connected. We then remove both nodes from the graph and push the pair (xi, hj(xi)) onto a stack (matched). We also remove any other edges from the matched LHS node and add any RHS nodes that now have degree one to degree one. The algorithm succeeds if, while there are still n-grams left to match, degree one is never empty. We then encode n-grams in the order given by the stack (i.e., first-in-last-out). Since we remove each location in A (RHS node) from the graph as it is matched to an n-gram (LHS node), each location will be associated with at most one n-gram for updating. Moreover, since we match an n-gram to a location only once the location has degree one, we are guaranteed that any other ngrams that depend on this location are already on the stack and will therefore only be encoded once we have updated this location. Hence dependencies in g are respected and g(xi) = v(xi) will remain true following the update in Eq. (2) for each xi ? S. 3.5 Choosing Random Hash Functions The algorithm described above is not guaranteed to succeed. Its success depends on the size of the array M , the number of n-grams stored |S| and the choice of random hash functions hj , j ? [k]. Clearly we require M ? |S|; in fact, an argument from Majewski et al (1996) implies that if M ? 1.23|S| and k = 3, the algorithm succeeds with high probabilFigure 2: The ordered matching algorithm: matched = [(a, 1), (b, 2), (d, 4), (c, 5)] ity. We use 2-universal hash functions (L. Carter and M. Wegman, 1979) defined for a range of size M via a prime P ? M and two random numbers 1 ? aj ? P and 0 ? bj ? P for j ? [k] as hj(x) ? ajx + bj mod P taken modulo M . We generate a set of k hash functions by sampling k pairs of random numbers (aj , bj), j ? [k]. If the algorithm does not find a matching with the current set of hash functions, we re-sample these parameters and re-start the algorithm. Since the probability of failure on a single attempt is low when M ? 1.23|S|, the probability of failing multiple times is very small. 3.6 Querying the Model and False Positives The construction we have described above ensures that for any n-gram xi ? S we have g(xi) = v(xi), i.e., we retrieve the correct value. To retrieve a value given an n-gram xi we simply compute the fingerprint f(xi), the hash functions hj(xi), j ? [k] and then return g(xi) using Eq. (1). Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for ngrams stored in the model. Hence we will not make errors for common n-grams that are typically in S. 508 Algorithm 1 Ordered Matching Input : Set of n-grams S; k hash functions hj , j ? [k]; number of available locations M . Output : Ordered matching matched or FAIL. matched ? [ ] for all i ? [0,M ? 1] do r2li ? ? end for for all xi ? S do l2ri ? ? for all j ? [k] do l2ri ? l2ri ? hj(xi) r2lhj(xi) ? r2lhj(xi) ? xi end for end for degree one ? {i ? [0,M ? 1] | |r2li| = 1} while |degree one| ? 1 do rhs ? POP degree one lhs ? POP r2lrhs PUSH (lhs, rhs) onto matched for all rhs? ? l2rlhs do POP r2lrhs? if |r2lrhs? | = 1 then degree one ? degree one ? rhs? end if end for end while if |matched| = |S| then return matched else return FAIL end if On the other hand, querying the model with an ngram that was not stored, i.e. with xi ? U \\ S we may erroneously return a value v ? V . Since the fingerprint f(xi) is assumed to be distributed uniformly at random (u.a.r.) in [0, B ? 1], g(xi) is also u.a.r. in [0, B?1] for xi ? U\\S . Hence with |V| values stored in the model, the probability that xi ? U \\ S is assigned a value in v ? V is Pr{g(xi) ? V|xi ? U \\ S} = |V|/B. We refer to this event as a false positive. If V is fixed, we can obtain a false positive rate \u000f by setting B as B ? |V|/\u000f. For example, if |V| is 128 then taking B = 1024 gives an error rate of \u000f = 128/1024 = 0.125 with each entry inA using dlog2 1024e = 10 bits. Clearly B must be at least |V| in order to distinguish each value. We refer to the additional bits allocated to each location (i.e. dlog2 Be ? log2 |V| or 3 in our example) as error bits in our experiments below. 3.7 Constructing the Full Model When encoding a large set of n-gram/value pairs S, Algorithm 1 will only be practical if the raw data and graph can be held in memory as the perfect hash function is generated. This makes it difficult to encode an extremely large set S into a single array A. The solution we adopt is to split S into t smaller sets S?i, i ? [t] that are arranged in lexicographic order.4 We can then encode each subset in a separate array A?i, i ? [t] in turn in memory. Querying each of these arrays for each n-gram requested would be inefficient and inflate the error rate since a false positive could occur on each individual array. Instead we store an index of the final n-gram encoded in each array and given a request for an n-gram?s value, perform a binary search for the appropriate array. 3.8 Sanity Checks Our models are consistent in the following sense (w1, w2, . . . , wn) ? S =? (w2, . . . , wn) ? S. Hence we can infer that an n-gram can not be present in the model, if the n? 1-gram consisting of the final n ? 1 words has already tested false. Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. Backoff smoothing algorithms typically request the longest n-gram supported by the model first, requesting shorter n-grams only if this is not found. In our case, however, if a query is issued for the 5-gram (w1, w2, w3, w4, w5) when only the unigram (w5) is present in the model, the probability of a false positive using such a backoff procedure would not be \u000f as stated above, but rather the probability that we fail to avoid an error on any of the four queries performed prior to requesting the unigram, i.e. 1?(1?\u000f)4 ? 4\u000f. We therefore query the model first with the unigram working up to the full n-gram requested by the decoder only if the preceding queries test positive. The probability of returning a false positive for any ngram requested by the decoder (but not in the model) will then be at most \u000f. 4In our system we use subsets of 5 million n-grams which can easily be encoded using less than 2GB of working space. 509 4 Experimental Set-up 4.1 Distributed LM Framework We deploy the randomized LM in a distributed framework which allows it to scale more easily by distributing it across multiple language model servers. We encode the model stored on each languagage model server using the randomized scheme. The proposed randomized LM can encode parameters estimated using any smoothing scheme (e.g. Kneser-Ney, Katz etc.). Here we choose to work with stupid backoff smoothing (Brants et al, 2007) since this is significantly more efficient to train and deploy in a distributed framework than a contextdependent smoothing scheme such as Kneser-Ney. Previous work (Brants et al, 2007) has shown it to be appropriate to large-scale language modeling. 4.2 LM Data Sets The language model is trained on four data sets: target: The English side of Arabic-English parallel data provided by LDC (132 million tokens). gigaword: The English Gigaword dataset provided by LDC (3.7 billion tokens). webnews: Data collected over several years, up to January 2006 (34 billion tokens). web: The Web 1T 5-gram Version 1 corpus provided by LDC (1 trillion tokens).5 An initial experiment will use the Web 1T 5-gram corpus only; all other experiments will use a loglinear combination of models trained on each corpus. The combined model is pre-compiled with weights trained on development data by our system. 4.3 Machine Translation The SMT system used is based on the framework proposed in (Och and Ney, 2004) where translation is treated as the following optimization problem e? = argmax e M? i=1 ?i?i(e, f). (3) Here f is the source sentence that we wish to translate, e is a translation in the target language, ?i, i ? [M ] are feature functions and ?i, i ? [M ] are weights. (Some features may not depend on f .) 5N -grams with count < 40 are not included in this data set. Full Set Entropy-Pruned # 1-grams 13,588,391 13,588,391 # 2-grams 314,843,401 184,541,402 # 3-grams 977,069,902 439,430,328 # 4-grams 1,313,818,354 407,613,274 # 5-grams 1,176,470,663 238,348,867 Total 3,795,790,711 1,283,522,262 Table 1: Num. of n-grams in the Web 1T 5-gram corpus. 5 Experiments This section describes three sets of experiments: first, we encode the Web 1T 5-gram corpus as a randomized language model and compare the resulting size with other representations; then we measure false positive rates when requesting n-grams for a held-out data set; finally we compare translation quality when using conventional (lossless) languages models and our randomized language model. Note that the standard practice of measuring perplexity is not meaningful here since (1) for efficient computation, the language model is not normalized; and (2) even if this were not the case, quantization and false positives would render it unnormalized. 5.1 Encoding the Web 1T 5-gram corpus We build a language model from the Web 1T 5-gram corpus. Parameters, corresponding to negative logarithms of relative frequencies, are quantized to 8-bits using a uniform quantizer. More sophisticated quantizers (e.g. (S. Lloyd, 1982)) may yield better results but are beyond the scope of this paper. Table 1 provides some statistics about the corpus. We first encode the full set of n-grams, and then a version that is reduced to approx. 1/3 of its original size using entropy pruning (Stolcke, 1998). Table 2 shows the total space and number of bytes required per n-gram to encode the model under different schemes: ?LDC gzip?d? is the size of the files as delivered by LDC; ?Trie? uses a compact trie representation (e.g., (Clarkson et al, 1997; Church et al., 2007)) with 3 byte word ids, 1 byte values, and 3 byte indices; ?Block encoding? is the encoding used in (Brants et al, 2007); and ?randomized? uses our novel randomized scheme with 12 error bits. The latter requires around 60% of the space of the next best representation and less than half of the com510 size (GB) bytes/n-gram Full Set LDC gzip?d 24.68 6.98 Trie 21.46 6.07 Block Encoding 18.00 5.14 Randomized 10.87 3.08 Entropy Pruned Trie 7.70 6.44 Block Encoding 6.20 5.08 Randomized 3.68 3.08 Table 2: Web 1T 5-gram language model sizes with different encodings. ?Randomized? uses 12 error bits. monly used trie encoding. Our method is the only one to use the same amount of space per parameter for both full and entropy-pruned models. 5.2 False Positive Rates All n-grams explicitly inserted into our randomized language model are retrieved without error; however, n-grams not stored may be incorrectly assigned a value resulting in a false positive. Section (3) analyzed the theoretical error rate; here, we measure error rates in practice when retrieving n-grams for approx. 11 million tokens of previously unseen text (news articles published after the training data had been collected). We measure this separately for all n-grams of order 2 to 5 from the same text. The language model is trained on the four data sources listed above and contains 24 billion ngrams. With 8-bit parameter values, the model requires 55.2/69.0/82.7 GB storage when using 8/12/16 error bits respectively (this corresponds to 2.46/3.08/3.69 bytes/n-gram). Using such a large language model results in a large fraction of known n-grams in new text. Table 3 shows, e.g., that almost half of all 5-grams from the new text were seen in the training data. Column (1) in Table 4 shows the number of false positives that occurred for this test data. Column (2) shows this as a fraction of the number of unseen n-grams in the data. This number should be close to 2?b where b is the number of error bits (i.e. 0.003906 for 8 bits and 0.000244 for 12 bits). The error rates for bigrams are close to their expected values. The numbers are much lower for higher n-gram orders due to the use of sanity checks (see Section 3.8). total seen unseen 2gms 11,093,093 98.98% 1.02% 3gms 10,652,693 91.08% 8.92% 4gms 10,212,293 68.39% 31.61% 5gms 9,781,777 45.51% 54.49% Table 3: Number of n-grams in test set and percentages of n-grams that were seen/unseen in the training data. (1) (2) (3) false pos. false posunseen false pos total 8 error bits 2gms 376 0.003339 0.000034 3gms 2839 0.002988 0.000267 4gms 6659 0.002063 0.000652 5gms 6356 0.001192 0.000650 total 16230 0.001687 0.000388 12 error bits 2gms 25 0.000222 0.000002 3gms 182 0.000192 0.000017 4gms 416 0.000129 0.000041 5gms 407 0.000076 0.000042 total 1030 0.000107 0.000025 Table 4: False positive rates with 8 and 12 error bits. The overall fraction of n-grams requested for which an error occurs is of most interest in applications. This is shown in Column (3) and is around a factor of 4 smaller than the values in Column (2). On average, we expect to see 1 error in around 2,500 requests when using 8 error bits, and 1 error in 40,000 requests with 12 error bits (see ?total? row). 5.3 Machine Translation We run an improved version of our 2006 NIST MT Evaluation entry for the Arabic-English ?Unlimited? data track.6 The language model is the same one as in the previous section. Table 5 shows baseline translation BLEU scores for a lossless (non-randomized) language model with parameter values quantized into 5 to 8 bits. We use MT04 data for system development, with MT05 data and MT06 (?NIST? subset) data for blind testing. As expected, results improve when using more bits. There seems to be little benefit in going beyond 6See http://www.nist.gov/speech/tests/mt/2006/doc/ 511 dev test test bits MT04 MT05 MT06 5 0.5237 0.5608 0.4636 6 0.5280 0.5671 0.4649 7 0.5299 0.5691 0.4672 8 0.5304 0.5697 0.4663 Table 5: Baseline BLEU scores with lossless n-gram model and different quantization levels (bits). 0.554 0.556 0.558 0.56 0.562 0.564 0.566 0.568 0.57 8  9  10  11  12  13  14  15  16 MT 05 B LE U Number of Error Bits 8 bit values 7 bit values 6 bit values 5 bit values Figure 3: BLEU scores on the MT05 data set. 8 bits. Overall, our baseline results compare favorably to those reported on the NIST MT06 web site. We now replace the language model with a randomized version. Fig. 3 shows BLEU scores for the MT05 evaluation set with parameter values quantized into 5 to 8 bits and 8 to 16 additional ?error? bits. Figure 4 shows a similar graph for MT06 data. We again see improvements as quantization uses more bits. There is a large drop in performance when reducing the number of error bits from 10 to 8, while increasing it beyond 12 bits offers almost no further gains with scores that are almost identical to the lossless model. Using 8-bit quantization and 12 error bits results in an overall requirement of (8+12)?1.23 = 24.6 bits = 3.08 bytes per n-gram. All runs use the sanity checks described in Section 3.8. Without sanity checks, scores drop, e.g. by 0.002 for 8-bit quantization and 12 error bits. Randomization and entropy pruning can be combined to achieve further space savings with minimal loss in quality as shown in Table (6). The BLEU score drops by between 0.0007 to 0.0018 while the 0.454 0.456 0.458 0.46 0.462 0.464 0.466 0.468 8  9  10  11  12  13  14  15  16 MT 06 (N IST) B LE U Number of Error Bits 8 bit values 7 bit values 6 bit values 5 bit values Figure 4: BLEU scores on MT06 data (?NIST? subset). size dev test test LM GB MT04 MT05 MT06 unpruned block 116 0.5304 0.5697 0.4663 unpruned rand 69 0.5299 0.5692 0.4659 pruned block 42 0.5294 0.5683 0.4665 pruned rand 27 0.5289 0.5679 0.4656 Table 6: Combining randomization and entropy pruning. All models use 8-bit values; ?rand? uses 12 error bits. model is reduced to approx. 1/4 of its original size. 6 Conclusions We have presented a novel randomized language model based on perfect hashing. It can associate arbitrary parameter types with n-grams. Values explicitly inserted into the model are retrieved without error; false positives may occur but are controlled by the number of bits used per n-gram. The amount of storage needed is independent of the size of the vocabulary and the n-gram order. Lookup is very efficient: the values of 3 cells in a large array are combined with the fingerprint of an n-gram. Experiments have shown that this randomized language model can be combined with entropy pruning to achieve further memory reductions; that error rates occurring in practice are much lower than those predicted by theoretical analysis due to the use of runtime sanity checks; and that the same translation quality as a lossless language model representation can be achieved when using 12 ?error? bits, resulting in approx. 3 bytes per n-gram (this includes one byte to store parameter values). 512 References B. Bloom. 1970. Space/time tradeoffs in hash coding with allowable errors. CACM, 13:422?426. Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLPCoNLL 2007, Prague. Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467?479. Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263?311. Larry Carter, Robert W. Floyd, John Gill, George Markowsky, and Mark N. Wegman. 1978. Exact and approximate membership testers. In STOC, pages 59? 65. L. Carter and M. Wegman. 1979. Universal classes of hash functions. Journal of Computer and System Science, 18:143?154. Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. 2004. The Bloomier Filter: an efficient data structure for static support lookup tables. In Proc. 15th ACM-SIAM Symposium on Discrete Algoritms, pages 30?39. Kenneth Church, Ted Hart, and Jianfeng Gao. 2007. Compressing trigram language models with golomb coding. In Proceedings of EMNLP-CoNLL 2007, Prague, Czech Republic, June. P. Clarkson and R. Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of EUROSPEECH, vol. 1, pages 2707?2710, Rhodes, Greece. Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen. 2007. Large-scale distributed language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2007, Hawaii, USA. J. Goodman and J. Gao. 2000. Language model size reduction by pruning and clustering. In ICSLP?00, Beijing, China. S. Lloyd. 1982. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129? 137. B.S. Majewski, N.C. Wormald, G. Havas, and Z.J. Czech. 1996. A family of perfect hashing methods. British Computer Journal, 39(6):547?554. Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417?449. Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270?274. D. Talbot andM. Osborne. 2007a. Randomised language modelling for statistical machine translation. In 45th Annual Meeting of the ACL 2007, Prague. D. Talbot and M. Osborne. 2007b. Smoothed Bloom filter language models: Tera-scale LMs on the cheap. In EMNLP/CoNLL 2007, Prague. 513 "}
